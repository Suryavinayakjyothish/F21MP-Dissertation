{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer,StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "## for explainer\n",
    "#from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#b = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "a = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "b = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "c =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "d =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "e =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#c = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week3.csv', low_memory=False , encoding='cp1252')\n",
    "#d = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#e =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "#g =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "#h =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10310733, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1795404, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape)\n",
    "#a.drop(a[a['attackType'] == '---'].index, axis = 0, inplace= True) \n",
    "b.drop(b[b['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "c.drop(c[c['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "d.drop(d[d['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "#e.drop(e[e['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_external = pd.concat([c,d,e], axis = 0)\n",
    "data_external.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to Increment attackID values\n",
    "data_external['attackID'] = data_external['attackID'].apply(lambda x: str(int(x) + 70) if x != '---' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(df):\n",
    "    if 'M' in df:\n",
    "        df = df.split('M')\n",
    "        df = df[0].strip()\n",
    "        df = float(df) * 1000000\n",
    "    elif 'B' in df:\n",
    "        df = df.split('B')\n",
    "        df = df[0].strip()\n",
    "        df =  float(df) * 1000000000\n",
    "    else: \n",
    "        df =float(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([a,b,data_external], axis = 0)\n",
    "data.reset_index(drop= True, inplace= True)\n",
    "data['Bytes'] = data['Bytes'].apply(lambda x: convert_bytes(x))\n",
    "data['attackType'] = data['attackType'].apply(lambda x:  'attack' if (x!= '---') else x )\n",
    "columns = ['Src Pt', 'Dst Pt','Tos','Flows','Packets', 'Bytes']\n",
    "for i in columns:\n",
    "    data[i] = pd.to_numeric(data[i]);\n",
    "del columns\n",
    "del a,b,c,d,e, data_external\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Hexadecimal value to Binary\n",
    "def hex_to_binary(hexdata):\n",
    "    scale = 16 ## equals to hexadecimal\n",
    "    num_of_bits = 9\n",
    "    return bin(int(hexdata, scale))[2:].zfill(num_of_bits);\n",
    "#Converts TCP flags to Binary\n",
    "def to_Binary(x):\n",
    "    l = 0\n",
    "    x = '...' + x\n",
    "    x = list(x)\n",
    "    for i in x:\n",
    "        if (i=='.'):\n",
    "            x[l]= '0'\n",
    "        else:\n",
    "            x[l] = '1'\n",
    "        l = l +1\n",
    "    return ''.join(x)\n",
    "#Converts the 'Flags' column to 9 indiviual columns (manual oneshot encoding)\n",
    "def flag_convert(df):  \n",
    "   # df['Flags'] = df['Flags'].apply(lambda x: (list(x)))\n",
    "   # temp = df['Flags'].apply(lambda x: toBinary(x))\n",
    "    hex_values = list(df[(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    flag_values = list(df[~(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    binary_values = {}\n",
    "    for i in hex_values:\n",
    "         binary_values[i] = (hex_to_binary(i))\n",
    "    for i in flag_values:\n",
    "         binary_values[i] = (to_Binary(i))\n",
    "    temp = df['Flags'].replace(binary_values)\n",
    "#temp = temp.apply(lambda x: pd.Series(x)) \n",
    "    temp = pd.DataFrame(temp.apply(list).tolist())\n",
    "#temp = pd.DataFrame(temp)\n",
    "#a = a.iloc[: , 1:]\n",
    "   # print(temp.head())\n",
    "    temp.columns = ['N','C','E','U' ,'A','P','R','S','F']\n",
    "    for i in temp.columns:\n",
    "        temp[i] = pd.to_numeric(temp[i]);\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a IP_pairs \n",
    "def make_pair(df):\n",
    "    ip_pair = df['Src IP Addr'] +'/' +df['Dst IP Addr']\n",
    "    source_ip = df['Src IP Addr'].unique().tolist()\n",
    "    destination_ip = df['Dst IP Addr'].unique().tolist()\n",
    "   # df = df.drop(columns = ['Src IP Addr', 'Dst IP Addr'])\n",
    "    df.insert(1, ' IP Pair', ip_pair)\n",
    "    return df\n",
    "\n",
    "def check_inverse(df):\n",
    "    list_pairs = df[' IP Pair'].unique()\n",
    "    tuple_pair = []\n",
    "    for i in list_pairs:\n",
    "        tuple_pair.append(tuple((i.split('/'))))\n",
    "    dic_store = {}\n",
    "    for i in tuple_pair:\n",
    "        if (i  not in dic_store.keys()) and (i[::-1] not in dic_store.keys()):\n",
    "            dic_store[i] = i[0] + '/' +i[1]\n",
    "    print(len(dic_store.keys()))\n",
    "    dic_final = {}\n",
    "    for i in dic_store.keys():\n",
    "        dic_final[i[0] + '/' +i[1]] = dic_store[i]\n",
    "        dic_final[i[1] + '/' +i[0]] = dic_store[i]\n",
    "    df[' IP Pair'] = df[' IP Pair'].map(dic_final)               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    columns = data.select_dtypes(include=numerics).columns\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    #df.astype({'attackType': 'str'})\n",
    "    df['attackType'] = label_encoder.fit_transform(df['attackType'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "#    onehot_encoder = OneHotEncoder()\n",
    "#    onehot_encoder.fit(df.attackType.to_numpy().reshape(-1, 1))\n",
    "#    proto = onehot_encoder.transform(df.attackType.to_numpy().reshape(-1, 1))\n",
    "#    proto = pd.DataFrame.sparse.from_spmatrix(proto)\n",
    "#    proto.astype('int32')\n",
    "#    proto.columns = label_encoder.classes_\n",
    "#    df = pd.concat([df, proto], axis = 1) \n",
    "    \n",
    "    \n",
    "    df['Proto'] = label_encoder.fit_transform(df['Proto'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    onehot_encoder1 = OneHotEncoder()\n",
    "    onehot_encoder1.fit(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = onehot_encoder1.transform(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = pd.DataFrame.sparse.from_spmatrix(proto)\n",
    "    proto.astype('int32')\n",
    "    proto.columns = label_encoder.classes_\n",
    "   # print(proto.head(1))\n",
    "    df = pd.concat([df, proto], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(columns = ['Date first seen', ' IP Pair', 'Flows', 'class', 'attackID','Flags',\n",
    "                              'attackDescription', 'Src IP Addr', 'Dst IP Addr','Proto'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplit IP address into features, 7 features\n",
    "def split_to_net(IP_address):\n",
    "    IP_list = IP_address.split(\".\")\n",
    "    needed_len = 7\n",
    "    needed_len = needed_len - len(IP_list)\n",
    "    for i in range(0,needed_len,1):\n",
    "        IP_list.append('0')\n",
    "    return IP_list\n",
    "#replace unknown IP address, and convert to columns\n",
    "def IP_split(df): \n",
    "    replace = {\"ATTACKER1\":\"0.0.0.0\",\n",
    "           \"ATTACKER2\":\"0.0.0.0\",\n",
    "           \"ATTACKER3\":\"0.0.0.0\",\n",
    "           \"EXT_SERVER\": \"0.0.0.0.1\",\n",
    "          \"OPENSTACK_NET\": \"0.0.0.0.0.1\",\n",
    "          \"DNS\": \"0.0.0.0.0.0.1\"}\n",
    "    df = df.replace({\"Src IP Addr\": replace, \"Dst IP Addr\": replace}, value=None)\n",
    "    temp_source = df[\"Src IP Addr\"].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "    temp_des = df['Dst IP Addr'].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "   # sourceIP = list(df[\"Src IP Addr\"].unique())\n",
    "   # destIP = list(df[\"Dst IP Addr\"].unique())\n",
    "   # sourceIP_values = {}\n",
    "   # desIP_values = {}\n",
    "   # for i in sourceIP:\n",
    "   #      sourceIP_values[i] = (split_to_net(i))\n",
    "   # for i in destIP:\n",
    "   #      desIP_values[i] = (split_to_net(i))\n",
    "    #print(sourceIP_values)\n",
    "   # print(desIP_values)\n",
    "#for Source IP\n",
    "    temp_source = temp_source.apply(lambda x: split_to_net(x) )\n",
    "    temp_source = pd.DataFrame(temp_source.apply(list).tolist())\n",
    "    temp_source.columns = ['sourceIP_feature 1','sourceIP_feature 2','sourceIP_feature 3','sourceIP_feature 4' ,\n",
    "                    'sourceEXT_SERVER','sourceOPENSTACK_NET','sourceDNS']\n",
    "    for i in temp_source.columns:\n",
    "        temp_source[i] = pd.to_numeric(temp_source[i]);\n",
    "    temp_source = temp_source.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_source], axis = 1)\n",
    "    #for Destination IP\n",
    "    temp_des = temp_des.apply(lambda x: split_to_net(x) )\n",
    "    temp_des = pd.DataFrame(temp_des.apply(list).tolist())\n",
    "    temp_des.columns = ['destIP_feature 1','destIP_feature 2','destIP_feature 3','destIP_feature 4' ,\n",
    "                    'destEXT_SERVER','destOPENSTACK_NET','destDNS']\n",
    "   # for i in temp_des.columns:\n",
    "       # temp_des[i] = pd.to_numeric(temp_des[i]);\n",
    "    temp_des = temp_des.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_des], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59362\n"
     ]
    }
   ],
   "source": [
    "data = make_pair(data)\n",
    "data = check_inverse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = IP_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Duration', 'Src Pt', 'Dst Pt', 'Packets', 'Bytes', 'Flows', 'Tos'], dtype='object')\n",
      "['---', 'attack']\n",
      "[0, 1]\n",
      "['GRE  ', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = normalize(data)\n",
    "data =  one_shot(data) \n",
    "#data = normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def unix_time(df):\n",
    "  #  df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x + ':00' if len(x) != 19 else x)\n",
    "   # df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x[0 : 5 : ] + x[7 : :] if len(x) != 19 else x[0 : 7 : ] + x[9 : :])\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: x.timestamp()*1000)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_profile(grouped):\n",
    "    grouped['---'] = unix_time(grouped['---'])\n",
    "    start_time = int(grouped['---'].head(1)['Date first seen'].values[0])\n",
    "    end_time = int(grouped['---'].tail(1)['Date first seen'].values[0])\n",
    "#date_bins = pd.IntervalIndex.from_tuples(\n",
    "#        [(i, i+3600000) for i in range(start_time, end_time, 3600000)],\n",
    "#        closed=\"left\")\n",
    "#date_labels = [f\"{i}\" for i in range(1, len(date_bins)+1, 1)]\n",
    "    normal_data = dict(tuple( grouped['---'].groupby( pd.cut(\n",
    "            grouped['---']['Date first seen'],\n",
    "               np.arange(start_time, end_time, 3*3600000)))))\n",
    "    del grouped['---']\n",
    "    num = []\n",
    "    for i in grouped_data.keys():\n",
    "          num.append(len(grouped_data[i]))\n",
    "    print(min(num))\n",
    "    num = max(num)\n",
    "    print(num)\n",
    "    print(len(grouped.keys()))\n",
    "    grouped = {**grouped, **normal_data}\n",
    "    print(len(grouped.keys()))\n",
    "    return grouped, num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data= dict(tuple(data.groupby(['attackID'])))\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_largeInstances(dic, length):\n",
    "    remove_ID = []\n",
    "    for i in dic.keys():\n",
    "        if (i != '---'):\n",
    "            if(len(dic[i]) >= length):\n",
    "                remove_ID.append(i)\n",
    "    print(len(remove_ID))\n",
    "    removed_attacks = {}\n",
    "    for i in remove_ID:\n",
    "        removed_attacks[i] = dic[i]\n",
    "        del dic[i]\n",
    "    return dic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "grouped_data = del_largeInstances(grouped_data, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "19732\n",
      "73\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "#grouped_data, num = normal_profile(grouped_data)\n",
    "for i in grouped_data.keys():\n",
    "    grouped_data[i] = flag_convert(grouped_data[i])\n",
    "   # grouped_data[i] =  drop_columns(grouped_data[i])\n",
    "grouped_data, num = normal_profile(grouped_data)\n",
    "for i in grouped_data.keys():\n",
    "   # grouped_data[i] = flag_convert(grouped_data[i])\n",
    "    grouped_data[i] =  drop_columns(grouped_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : False\n",
      "10 : False\n",
      "11 : False\n",
      "12 : False\n",
      "13 : False\n",
      "14 : False\n",
      "15 : False\n",
      "17 : False\n",
      "19 : False\n",
      "2 : False\n",
      "20 : False\n",
      "21 : False\n",
      "22 : False\n",
      "24 : False\n",
      "25 : False\n",
      "27 : False\n",
      "29 : False\n",
      "30 : False\n",
      "32 : False\n",
      "33 : False\n",
      "34 : False\n",
      "35 : False\n",
      "36 : False\n",
      "38 : False\n",
      "39 : False\n",
      "40 : False\n",
      "41 : False\n",
      "43 : False\n",
      "47 : False\n",
      "48 : False\n",
      "49 : False\n",
      "5 : False\n",
      "50 : False\n",
      "51 : False\n",
      "52 : False\n",
      "54 : False\n",
      "55 : False\n",
      "56 : False\n",
      "57 : False\n",
      "58 : False\n",
      "61 : False\n",
      "62 : False\n",
      "64 : False\n",
      "65 : False\n",
      "66 : False\n",
      "67 : False\n",
      "68 : False\n",
      "69 : False\n",
      "7 : False\n",
      "70 : False\n",
      "71 : False\n",
      "72 : False\n",
      "73 : False\n",
      "74 : False\n",
      "75 : False\n",
      "76 : False\n",
      "77 : False\n",
      "78 : False\n",
      "79 : False\n",
      "8 : False\n",
      "80 : False\n",
      "81 : False\n",
      "82 : False\n",
      "83 : False\n",
      "84 : False\n",
      "85 : False\n",
      "86 : False\n",
      "87 : False\n",
      "88 : False\n",
      "89 : False\n",
      "90 : False\n",
      "91 : False\n",
      "92 : False\n",
      "(1489536076632, 1489546876632] : False\n",
      "(1489546876632, 1489557676632] : False\n",
      "(1489557676632, 1489568476632] : False\n",
      "(1489568476632, 1489579276632] : False\n",
      "(1489579276632, 1489590076632] : False\n",
      "(1489590076632, 1489600876632] : False\n",
      "(1489600876632, 1489611676632] : False\n",
      "(1489611676632, 1489622476632] : False\n",
      "(1489622476632, 1489633276632] : False\n",
      "(1489633276632, 1489644076632] : False\n",
      "(1489644076632, 1489654876632] : False\n",
      "(1489654876632, 1489665676632] : False\n",
      "(1489665676632, 1489676476632] : False\n",
      "(1489676476632, 1489687276632] : False\n",
      "(1489687276632, 1489698076632] : False\n",
      "(1489698076632, 1489708876632] : False\n",
      "(1489708876632, 1489719676632] : False\n",
      "(1489719676632, 1489730476632] : False\n",
      "(1489730476632, 1489741276632] : False\n",
      "(1489741276632, 1489752076632] : False\n",
      "(1489752076632, 1489762876632] : False\n",
      "(1489762876632, 1489773676632] : False\n",
      "(1489773676632, 1489784476632] : False\n",
      "(1489784476632, 1489795276632] : False\n",
      "(1489795276632, 1489806076632] : False\n",
      "(1489806076632, 1489816876632] : False\n",
      "(1489816876632, 1489827676632] : False\n",
      "(1489827676632, 1489838476632] : False\n",
      "(1489838476632, 1489849276632] : False\n",
      "(1489849276632, 1489860076632] : False\n",
      "(1489860076632, 1489870876632] : False\n",
      "(1489870876632, 1489881676632] : False\n",
      "(1489881676632, 1489892476632] : False\n",
      "(1489892476632, 1489903276632] : False\n",
      "(1489903276632, 1489914076632] : False\n",
      "(1489914076632, 1489924876632] : False\n",
      "(1489924876632, 1489935676632] : False\n",
      "(1489935676632, 1489946476632] : False\n",
      "(1489946476632, 1489957276632] : False\n",
      "(1489957276632, 1489968076632] : False\n",
      "(1489968076632, 1489978876632] : False\n",
      "(1489978876632, 1489989676632] : False\n",
      "(1489989676632, 1490000476632] : False\n",
      "(1490000476632, 1490011276632] : False\n",
      "(1490011276632, 1490022076632] : False\n",
      "(1490022076632, 1490032876632] : False\n",
      "(1490032876632, 1490043676632] : False\n",
      "(1490043676632, 1490054476632] : False\n",
      "(1490054476632, 1490065276632] : False\n",
      "(1490065276632, 1490076076632] : False\n",
      "(1490076076632, 1490086876632] : False\n",
      "(1490086876632, 1490097676632] : False\n",
      "(1490097676632, 1490108476632] : False\n",
      "(1490108476632, 1490119276632] : False\n",
      "(1490119276632, 1490130076632] : False\n",
      "(1490130076632, 1490140876632] : False\n",
      "(1490140876632, 1490151676632] : False\n",
      "(1490151676632, 1490162476632] : False\n",
      "(1490162476632, 1490173276632] : False\n",
      "(1490173276632, 1490184076632] : False\n",
      "(1490184076632, 1490194876632] : False\n",
      "(1490194876632, 1490205676632] : False\n",
      "(1490205676632, 1490216476632] : False\n",
      "(1490216476632, 1490227276632] : False\n",
      "(1490227276632, 1490238076632] : False\n",
      "(1490238076632, 1490248876632] : False\n",
      "(1490248876632, 1490259676632] : False\n",
      "(1490259676632, 1490270476632] : False\n",
      "(1490270476632, 1490281276632] : False\n",
      "(1490281276632, 1490292076632] : False\n",
      "(1490292076632, 1490302876632] : False\n",
      "(1490302876632, 1490313676632] : False\n",
      "(1490313676632, 1490324476632] : False\n",
      "(1490324476632, 1490335276632] : False\n",
      "(1490335276632, 1490346076632] : False\n",
      "(1490346076632, 1490356876632] : False\n",
      "(1490356876632, 1490367676632] : False\n",
      "(1490367676632, 1490378476632] : False\n",
      "(1490378476632, 1490389276632] : False\n",
      "(1490389276632, 1490400076632] : False\n",
      "(1490400076632, 1490410876632] : False\n",
      "(1490410876632, 1490421676632] : False\n",
      "(1490421676632, 1490432476632] : False\n",
      "(1490432476632, 1490443276632] : False\n",
      "(1490443276632, 1490454076632] : False\n",
      "(1490454076632, 1490464876632] : False\n",
      "(1490464876632, 1490475676632] : False\n",
      "(1490475676632, 1490486476632] : False\n",
      "(1490486476632, 1490497276632] : False\n",
      "(1490497276632, 1490508076632] : False\n",
      "(1490508076632, 1490518876632] : False\n",
      "(1490518876632, 1490529676632] : False\n",
      "(1490529676632, 1490540476632] : False\n",
      "(1490540476632, 1490551276632] : False\n",
      "(1490551276632, 1490562076632] : False\n",
      "(1490562076632, 1490572876632] : False\n",
      "(1490572876632, 1490583676632] : False\n",
      "(1490583676632, 1490594476632] : False\n",
      "(1490594476632, 1490605276632] : False\n",
      "(1490605276632, 1490616076632] : False\n",
      "(1490616076632, 1490626876632] : False\n",
      "(1490626876632, 1490637676632] : False\n",
      "(1490637676632, 1490648476632] : False\n",
      "(1490648476632, 1490659276632] : False\n",
      "(1490659276632, 1490670076632] : False\n",
      "(1490670076632, 1490680876632] : False\n",
      "(1490680876632, 1490691676632] : False\n",
      "(1490691676632, 1490702476632] : False\n",
      "(1490702476632, 1490713276632] : False\n",
      "(1490713276632, 1490724076632] : False\n",
      "(1490724076632, 1490734876632] : False\n",
      "(1490734876632, 1490745676632] : False\n",
      "(1490745676632, 1490756476632] : False\n",
      "(1490756476632, 1490767276632] : False\n",
      "(1490767276632, 1490778076632] : False\n",
      "(1490778076632, 1490788876632] : False\n",
      "(1490788876632, 1490799676632] : False\n",
      "(1490799676632, 1490810476632] : False\n",
      "(1490810476632, 1490821276632] : False\n",
      "(1490821276632, 1490832076632] : False\n",
      "(1490832076632, 1490842876632] : False\n",
      "(1490842876632, 1490853676632] : False\n",
      "(1490853676632, 1490864476632] : False\n",
      "(1490864476632, 1490875276632] : False\n",
      "(1490875276632, 1490886076632] : False\n",
      "(1490886076632, 1490896876632] : False\n",
      "(1490896876632, 1490907676632] : False\n",
      "(1490907676632, 1490918476632] : False\n",
      "(1490918476632, 1490929276632] : False\n",
      "(1490929276632, 1490940076632] : False\n",
      "(1490940076632, 1490950876632] : False\n",
      "(1490950876632, 1490961676632] : False\n",
      "(1490961676632, 1490972476632] : False\n",
      "(1490972476632, 1490983276632] : False\n",
      "(1490983276632, 1490994076632] : False\n",
      "(1490994076632, 1491004876632] : False\n",
      "(1491004876632, 1491015676632] : False\n",
      "(1491015676632, 1491026476632] : False\n",
      "(1491026476632, 1491037276632] : False\n",
      "(1491037276632, 1491048076632] : False\n",
      "(1491048076632, 1491058876632] : False\n",
      "(1491058876632, 1491069676632] : False\n",
      "(1491069676632, 1491080476632] : False\n",
      "(1491080476632, 1491091276632] : False\n",
      "(1491091276632, 1491102076632] : False\n",
      "(1491102076632, 1491112876632] : False\n",
      "(1491112876632, 1491123676632] : False\n",
      "(1491123676632, 1491134476632] : False\n",
      "(1491134476632, 1491145276632] : False\n",
      "(1491145276632, 1491156076632] : False\n",
      "(1491156076632, 1491166876632] : False\n",
      "(1491166876632, 1491177676632] : False\n",
      "(1491177676632, 1491188476632] : False\n",
      "(1491188476632, 1491199276632] : False\n",
      "(1491199276632, 1491210076632] : False\n",
      "(1491210076632, 1491220876632] : False\n",
      "(1491220876632, 1491231676632] : False\n",
      "(1491231676632, 1491242476632] : False\n",
      "(1491242476632, 1491253276632] : False\n",
      "(1491253276632, 1491264076632] : False\n",
      "(1491264076632, 1491274876632] : False\n",
      "(1491274876632, 1491285676632] : False\n",
      "(1491285676632, 1491296476632] : False\n",
      "(1491296476632, 1491307276632] : False\n",
      "(1491307276632, 1491318076632] : False\n",
      "(1491318076632, 1491328876632] : False\n",
      "(1491328876632, 1491339676632] : False\n",
      "(1491339676632, 1491350476632] : False\n",
      "(1491350476632, 1491361276632] : False\n",
      "(1491361276632, 1491372076632] : False\n",
      "(1491372076632, 1491382876632] : False\n",
      "(1491382876632, 1491393676632] : False\n",
      "(1491393676632, 1491404476632] : False\n",
      "(1491404476632, 1491415276632] : False\n",
      "(1491415276632, 1491426076632] : False\n",
      "(1491426076632, 1491436876632] : False\n",
      "(1491436876632, 1491447676632] : False\n",
      "(1491447676632, 1491458476632] : False\n",
      "(1491458476632, 1491469276632] : False\n",
      "(1491469276632, 1491480076632] : False\n",
      "(1491480076632, 1491490876632] : False\n",
      "(1491490876632, 1491501676632] : False\n",
      "(1491501676632, 1491512476632] : False\n",
      "(1491512476632, 1491523276632] : False\n",
      "(1491523276632, 1491534076632] : False\n",
      "(1491534076632, 1491544876632] : False\n",
      "(1491544876632, 1491555676632] : False\n",
      "(1491555676632, 1491566476632] : False\n",
      "(1491566476632, 1491577276632] : False\n",
      "(1491577276632, 1491588076632] : False\n",
      "(1491588076632, 1491598876632] : False\n",
      "(1491598876632, 1491609676632] : False\n",
      "(1491609676632, 1491620476632] : False\n",
      "(1491620476632, 1491631276632] : False\n",
      "(1491631276632, 1491642076632] : False\n",
      "(1491642076632, 1491652876632] : False\n",
      "(1491652876632, 1491663676632] : False\n",
      "(1491663676632, 1491674476632] : False\n",
      "(1491674476632, 1491685276632] : False\n",
      "(1491685276632, 1491696076632] : False\n",
      "(1491696076632, 1491706876632] : False\n",
      "(1491706876632, 1491717676632] : False\n",
      "(1491717676632, 1491728476632] : False\n",
      "(1491728476632, 1491739276632] : False\n",
      "(1491739276632, 1491750076632] : False\n",
      "(1491750076632, 1491760876632] : False\n",
      "(1491760876632, 1491771676632] : False\n",
      "(1491771676632, 1491782476632] : False\n",
      "(1491782476632, 1491793276632] : False\n",
      "(1491793276632, 1491804076632] : False\n",
      "(1491804076632, 1491814876632] : False\n",
      "(1491814876632, 1491825676632] : False\n",
      "(1491825676632, 1491836476632] : False\n",
      "(1491836476632, 1491847276632] : False\n",
      "(1491847276632, 1491858076632] : False\n",
      "(1491858076632, 1491868876632] : False\n",
      "(1491868876632, 1491879676632] : False\n",
      "(1491879676632, 1491890476632] : False\n",
      "(1491890476632, 1491901276632] : False\n",
      "(1491901276632, 1491912076632] : False\n",
      "(1491912076632, 1491922876632] : False\n",
      "(1491922876632, 1491933676632] : False\n",
      "(1491933676632, 1491944476632] : False\n",
      "(1491944476632, 1491955276632] : False\n",
      "(1491955276632, 1491966076632] : False\n",
      "(1491966076632, 1491976876632] : False\n",
      "(1491976876632, 1491987676632] : False\n",
      "(1491987676632, 1491998476632] : False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1491998476632, 1492009276632] : False\n",
      "(1492009276632, 1492020076632] : False\n",
      "(1492020076632, 1492030876632] : False\n",
      "(1492030876632, 1492041676632] : False\n",
      "(1492041676632, 1492052476632] : False\n",
      "(1492052476632, 1492063276632] : False\n",
      "(1492063276632, 1492074076632] : False\n",
      "(1492074076632, 1492084876632] : False\n",
      "(1492084876632, 1492095676632] : False\n",
      "(1492095676632, 1492106476632] : False\n",
      "(1492106476632, 1492117276632] : False\n",
      "(1492117276632, 1492128076632] : False\n",
      "(1492128076632, 1492138876632] : False\n",
      "(1492138876632, 1492149676632] : False\n",
      "(1492149676632, 1492160476632] : False\n",
      "(1492160476632, 1492171276632] : False\n",
      "(1492171276632, 1492182076632] : False\n",
      "(1492182076632, 1492192876632] : False\n",
      "(1492192876632, 1492203676632] : False\n",
      "(1492203676632, 1492214476632] : False\n",
      "(1492214476632, 1492225276632] : False\n",
      "(1492225276632, 1492236076632] : False\n",
      "(1492236076632, 1492246876632] : False\n",
      "(1492246876632, 1492257676632] : False\n",
      "(1492257676632, 1492268476632] : False\n",
      "(1492268476632, 1492279276632] : False\n",
      "(1492279276632, 1492290076632] : False\n",
      "(1492290076632, 1492300876632] : False\n",
      "(1492300876632, 1492311676632] : False\n",
      "(1492311676632, 1492322476632] : False\n",
      "(1492322476632, 1492333276632] : False\n",
      "(1492333276632, 1492344076632] : False\n",
      "(1492344076632, 1492354876632] : False\n",
      "(1492354876632, 1492365676632] : False\n",
      "(1492365676632, 1492376476632] : False\n",
      "(1492376476632, 1492387276632] : False\n",
      "(1492387276632, 1492398076632] : False\n",
      "(1492398076632, 1492408876632] : False\n",
      "(1492408876632, 1492419676632] : False\n",
      "(1492419676632, 1492430476632] : False\n",
      "(1492430476632, 1492441276632] : False\n",
      "(1492441276632, 1492452076632] : False\n",
      "(1492452076632, 1492462876632] : False\n",
      "(1492462876632, 1492473676632] : False\n",
      "(1492473676632, 1492484476632] : False\n",
      "(1492484476632, 1492495276632] : False\n",
      "(1492495276632, 1492506076632] : False\n",
      "(1492506076632, 1492516876632] : False\n",
      "(1492516876632, 1492527676632] : False\n"
     ]
    }
   ],
   "source": [
    "for i in grouped_data.keys():\n",
    "    #if (grouped_data[i].hasnull())\n",
    "    print(f'{i} : {grouped_data[i].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in grouped_data.keys():\n",
    "    if ( len(grouped_data[i]) == 0):\n",
    "        counter = counter +1;\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 100 == 0 else x + 100 - x % 100\n",
    "#Convert to 3D arrays, input dict\n",
    "def make_array(dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    zero_arrays = []\n",
    "    for i in dic.keys():\n",
    "        if ( len(dic[i]) == 0):\n",
    "            zero_arrays.append(i);\n",
    "    for i in zero_arrays:\n",
    "        del dic[i]\n",
    "    for i in dic.keys():\n",
    "        x.append(np.array(dic[i].drop(['attackType'],axis = 1)).astype(np.float32))\n",
    "       # print(f'{i}')\n",
    "        y.append(dic[i]['attackType'].values[0])\n",
    "    print(len(y))\n",
    "    o = []\n",
    "    features = len(x[1][1])\n",
    "    #for i in x:\n",
    "     #   o.append(len(i))\n",
    "   # print(min(o))\n",
    "    o = num\n",
    "    o = roundup(o)\n",
    "    print(o)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        l = len(i)\n",
    "        i = list(i)\n",
    "        if(o > l):\n",
    "            l = o-l\n",
    "            for j in range(0, l, 1):\n",
    "                i.append([0] * features)\n",
    "        elif (o<l):\n",
    "            l = l-o\n",
    "            i = i[:-l]\n",
    "        #i = [k = np.array([k]) for l in i for k in l] # Makes array elements an array \n",
    "        x[index] = np.array(i).astype(np.float32)\n",
    "        index = index + 1\n",
    "    #x = [[i] for i in x]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "X,Y = make_array(grouped_data)\n",
    "del grouped_data\n",
    "gc.collect()\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 110, 1: 73}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_4D(arr):\n",
    "    x = []\n",
    "    for i in range(0, len(arr),1):\n",
    "        temp = []\n",
    "        for j in range(0,len(arr[i]),1):\n",
    "             temp.append([np.array([k]) for k in arr[i][j]])\n",
    "        x.append(np.array(temp).astype(np.float32))\n",
    "    return np.array(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = make_4D(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2, random_state=0,  stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X,Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 88, 1: 58}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 22, 1: 15}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in X_train:\n",
    "    print(f'{np.isnan(i).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Masking, Flatten, Input, Bidirectional\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, MaxPooling1D, Dropout, Flatten, Conv1D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 146 \n",
      " X:19800 \n",
      " Y:20 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "nsamples,nx, ny = X_train.shape\n",
    "print(f\"samples: {nsamples} \\n X:{nx} \\n Y:{ny} \\n \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19800, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "wie_by_epoch = []\n",
    "model_check = ModelCheckpoint(\"/models/Testpoints/best_model.hdf5\", monitor='loss', verbose=0, save_best_only=True, mode='min', save_freq=1)\n",
    "print_weights = LambdaCallback(on_epoch_end=lambda batch, logs:wie_by_epoch.append(model.get_weights()))\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a model for only the feature extraction layers\n",
    "feature_extractor = Sequential()\n",
    "feature_extractor.add(Conv1D(64, 2, activation='relu', input_shape=X_train.shape[1:]))\n",
    "feature_extractor.add(MaxPooling1D(pool_size= 2))\n",
    "feature_extractor.add(Conv1D(128, 2, activation='relu', input_shape=X_train.shape[1:]))\n",
    "feature_extractor.add(MaxPooling1D(pool_size= 2))\n",
    "feature_extractor.add(Conv1D(64, 2, activation='relu', input_shape=X_train.shape[1:]))\n",
    "feature_extractor.add(MaxPooling1D(pool_size= 2))\n",
    "feature_extractor.add(Flatten())\n",
    "# Keep adding new layers for prediciton outside of feature extraction model\n",
    "x = feature_extractor.output\n",
    "#x = Dense(64, activation = 'relu')(x)\n",
    "prediction_layer = Dense(5, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=feature_extractor.input, outputs=prediction_layer)\n",
    "model.compile(optimizer= 'rmsprop', loss='sparse_categorical_crossentropy',  metrics=[keras.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build(input_shape = (nx,ny))\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del model, wie_by_epoch\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_input (InputLayer)    [(None, 19800, 20)]       0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 19799, 64)         2624      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 9899, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 9898, 128)         16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 4949, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4948, 64)          16448     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2474, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 158336)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 791685    \n",
      "=================================================================\n",
      "Total params: 827,269\n",
      "Trainable params: 827,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model.build(input_shape = (nx,ny))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-0.12705314, -0.18694395, -0.10240559, ..., -0.08829165,\n",
       "           0.10721366,  0.16937064],\n",
       "         [-0.00930023, -0.16492727, -0.1040516 , ...,  0.17648552,\n",
       "          -0.03116085, -0.13311192],\n",
       "         [-0.12479033,  0.07609646, -0.02535319, ..., -0.1745088 ,\n",
       "          -0.17060232, -0.06386785],\n",
       "         ...,\n",
       "         [ 0.15714465,  0.09559064, -0.16318902, ..., -0.00041695,\n",
       "          -0.12278967,  0.02025934],\n",
       "         [-0.07336327,  0.11633568,  0.12384783, ..., -0.18788262,\n",
       "           0.14227961,  0.04007886],\n",
       "         [-0.13559133,  0.05352491, -0.04016982, ..., -0.17708929,\n",
       "          -0.17708762, -0.12613362]],\n",
       " \n",
       "        [[-0.07054789,  0.08648817,  0.03763205, ...,  0.18216418,\n",
       "          -0.01444888, -0.02298716],\n",
       "         [-0.04705015,  0.1613736 , -0.06171882, ..., -0.01250012,\n",
       "          -0.17820616, -0.00965461],\n",
       "         [ 0.1639326 , -0.06377724,  0.15558775, ..., -0.11774137,\n",
       "          -0.07304084,  0.04277425],\n",
       "         ...,\n",
       "         [-0.12497043,  0.1195391 ,  0.07840158, ...,  0.02209426,\n",
       "          -0.09819957, -0.12179504],\n",
       "         [-0.09839336, -0.0493207 , -0.01893367, ..., -0.13932267,\n",
       "          -0.1108036 , -0.07932687],\n",
       "         [ 0.08887626, -0.07198963, -0.14294253, ...,  0.01582578,\n",
       "           0.10905652,  0.04897214]]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[[-0.07774559,  0.09742472,  0.07436427, ...,  0.05444682,\n",
       "           0.00732949, -0.05364245],\n",
       "         [ 0.09498864, -0.08674836,  0.08372018, ...,  0.09908792,\n",
       "           0.04229084, -0.01406407],\n",
       "         [-0.05277723,  0.06552115,  0.0124909 , ..., -0.05182034,\n",
       "          -0.08431789,  0.00614083],\n",
       "         ...,\n",
       "         [-0.00041535,  0.10543489,  0.02512395, ...,  0.08672017,\n",
       "           0.11917111, -0.02109951],\n",
       "         [ 0.11887443, -0.06192827,  0.00925082, ...,  0.09156981,\n",
       "           0.06363261, -0.03537175],\n",
       "         [ 0.08467934,  0.06646848, -0.01074669, ..., -0.0474515 ,\n",
       "          -0.06371212,  0.07848147]],\n",
       " \n",
       "        [[ 0.03772351,  0.10392523,  0.07720968, ..., -0.01490438,\n",
       "          -0.01084962, -0.12378722],\n",
       "         [ 0.09448588, -0.08055574, -0.02952287, ..., -0.01729706,\n",
       "          -0.10899663,  0.03787905],\n",
       "         [-0.1159099 ,  0.08000794,  0.10127756, ...,  0.03352982,\n",
       "          -0.1224331 , -0.04830468],\n",
       "         ...,\n",
       "         [ 0.02941272, -0.1162976 ,  0.00808978, ..., -0.07808384,\n",
       "          -0.09418547,  0.02186009],\n",
       "         [-0.04443073,  0.09048101,  0.03559867, ..., -0.04776037,\n",
       "           0.07316384,  0.03177604],\n",
       "         [ 0.04139718,  0.09448427, -0.09356907, ..., -0.01300699,\n",
       "          -0.03277272,  0.01604033]]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[[ 0.00412175,  0.04463848, -0.015167  , ...,  0.03654358,\n",
       "          -0.03729963,  0.01315138],\n",
       "         [-0.08866304,  0.00393072, -0.09823298, ..., -0.06853178,\n",
       "           0.03446999,  0.10133341],\n",
       "         [-0.05018556,  0.11405632, -0.06546021, ...,  0.07822821,\n",
       "           0.01776001,  0.01020104],\n",
       "         ...,\n",
       "         [-0.06687179, -0.11795068,  0.00945765, ..., -0.03600982,\n",
       "           0.11908862,  0.09085289],\n",
       "         [-0.07476124,  0.09873945,  0.04695046, ..., -0.12481508,\n",
       "          -0.04487452,  0.05297887],\n",
       "         [ 0.00062117,  0.09917307,  0.11681363, ...,  0.09224615,\n",
       "          -0.0152722 , -0.05429184]],\n",
       " \n",
       "        [[ 0.03969607, -0.05971891, -0.11191821, ...,  0.06794792,\n",
       "          -0.05800578, -0.09000412],\n",
       "         [ 0.03863257, -0.11338493,  0.11006686, ..., -0.08113471,\n",
       "           0.08527899, -0.0922361 ],\n",
       "         [ 0.08556199,  0.06093067, -0.11112484, ..., -0.12364453,\n",
       "          -0.01584232,  0.09838992],\n",
       "         ...,\n",
       "         [-0.07632592, -0.01170775,  0.11645511, ...,  0.00434253,\n",
       "           0.12141994, -0.01614904],\n",
       "         [-0.06201121, -0.09867585, -0.04291868, ...,  0.10441345,\n",
       "          -0.06871113, -0.06178072],\n",
       "         [ 0.11837029, -0.10277775,  0.07531425, ..., -0.06999892,\n",
       "          -0.02716708,  0.07976824]]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-4.9718241e-03,  2.8258534e-03, -2.7894902e-03,  8.2099810e-04,\n",
       "         -4.5980797e-03],\n",
       "        [-3.9179102e-03,  7.8213960e-04,  4.3191696e-03, -1.2999722e-03,\n",
       "          4.9215183e-03],\n",
       "        [-4.4217343e-03, -5.5066934e-03, -8.8127330e-05, -1.2030569e-03,\n",
       "         -3.3398399e-03],\n",
       "        ...,\n",
       "        [ 2.6519503e-03, -4.4399067e-03,  2.1662945e-03, -2.9304333e-03,\n",
       "         -5.8214501e-03],\n",
       "        [-4.0513137e-03, -5.7340786e-03,  5.6792740e-03, -9.2217885e-04,\n",
       "         -1.8210895e-04],\n",
       "        [-2.4507870e-03, -5.6373747e-03,  4.2959210e-03,  5.6587299e-03,\n",
       "         -5.1110354e-03]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wights = model.get_weights()\n",
    "wights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "5/5 - 6s - loss: 2.8657 - sparse_categorical_accuracy: 0.6096\n",
      "Epoch 2/40\n",
      "5/5 - 5s - loss: 0.0784 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 3/40\n",
      "5/5 - 5s - loss: 0.0372 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 4/40\n",
      "5/5 - 5s - loss: 0.0308 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 5/40\n",
      "5/5 - 5s - loss: 0.0320 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 6/40\n",
      "5/5 - 5s - loss: 0.0235 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 7/40\n",
      "5/5 - 5s - loss: 0.0229 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 8/40\n",
      "5/5 - 5s - loss: 0.0202 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 9/40\n",
      "5/5 - 5s - loss: 0.0157 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 10/40\n",
      "5/5 - 5s - loss: 0.0142 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 11/40\n",
      "5/5 - 5s - loss: 1.1855 - sparse_categorical_accuracy: 0.9247\n",
      "Epoch 12/40\n",
      "5/5 - 5s - loss: 0.0293 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 13/40\n",
      "5/5 - 5s - loss: 0.0159 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 14/40\n",
      "5/5 - 5s - loss: 0.0142 - sparse_categorical_accuracy: 0.9932\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "mod = model.fit(X_train,\n",
    "               Y_train, epochs=40, batch_size= 32, verbose=2 , \n",
    "                callbacks=[callback, print_weights, model_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wie_by_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#120 +21\n",
    "model.set_weights(wie_by_epoch[13])\n",
    "#mod.history.get('sparse_categorical_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 0s - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005279299803078175, 1.0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test,Y_test, verbose=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.005279299803078175, 'sparse_categorical_accuracy': 1.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(model.metrics_names, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU9bn48c+TjSQQCCSIQAgJBBAIiBLZAr3WqkW0QtXaWu21i3J7bxetdsHW6q2tS+2m/qr1RZXaXq3Wigttca3SKgFZFJFNkwBCFCEJEMISyPL8/jgzMIRAJuFkzsw5z/v1mldmzvpMGJ755nu+3+eIqmKMMSbxJXkdgDHGGHdYQjfGGJ+whG6MMT5hCd0YY3zCEroxxviEJXRjjPEJS+jGGOMTltCN74nIZhE51+s4jOlqltCNMcYnLKGbwBKRa0WkQkR2isgCERkQWi4i8hsR2SEidSKyWkSKQ+tmiMg6EakXkQ9F5LvevgtjjrCEbgJJRM4B7gQuB/oDHwBPhFafD3wCGA5kA58HakPrHgb+S1WzgGLg1RiGbcwJpXgdgDEeuRKYp6pvAYjITcAuESkAGoEs4DRgmaquj9ivERglIu+o6i5gV0yjNuYErIVugmoATqscAFXdi9MKH6iqrwK/Be4HtovIXBHpGdr0UmAG8IGI/EtEJsc4bmOOyxK6CaqPgMHhFyLSHcgBPgRQ1ftUdTwwGqfr5Xuh5ctVdSZwCvAs8GSM4zbmuCyhm6BIFZH08AMnEX9FRMaJSDfgDuBNVd0sImeJyEQRSQX2AQ1As4ikiciVItJLVRuBPUCzZ+/ImFYsoZugWAgciHhMA34MzAe2AUOBL4S27Qn8Hqd//AOcrphfhtZ9CdgsInuArwNXxSh+Y9oldoMLY4zxB2uhG2OMT7iW0EN9k8tE5B0RWSsiP3Hr2MYYY9rnWpeLiAjQXVX3hi4mvQFcp6pLXTmBMcaYE3JtYpE63wx7Qy9TQw/roDfGmBhxdaaoiCQDK4Ei4H5VffN42+bm5mpBQYGbpzfGGN9buXJljar2bWudqwldVZuBcSKSDTwjIsWquia8XkRmA7MB8vPzWbFihZunN8YY3xORD463rktGuajqbmARML3V8rmqWqKqJX37tvkFY4wxppPcHOXSN9QyR0QygHOBDW4dP1Jzi3XNG2NMa2620PsDr4nIamA58LKq/t3F4wPwpyWbmXjHKxxqanH70MYYk9DcHOWyGjjDreMdT7+e6dTsPcTbW3YxcUhOV5/OGBNnGhsbqaqqoqGhwetQulR6ejp5eXmkpqZGvU/C1UOfNCSHJIHFFTWW0I0JoKqqKrKysigoKMCZ/uI/qkptbS1VVVUUFhZGvV/CTf3vlZHK2Lxs3qio8ToUY4wHGhoayMnJ8W0yBxARcnJyOvxXSMIldIBpw3J5p6qOPQ2NXodijPGAn5N5WGfeY0Im9NKiXJpblDc37vQ6FGOMiRsJmdDPyM8mIzWZN8qrvQ7FGBMwu3fv5oEHHujwfjNmzGD37t1dENERCZnQu6UkM6Gwj/WjG2Ni7ngJvbn5xDevWrhwIdnZ2V0VFpCgCR2cfvTK6n1sqzvgdSjGmACZM2cOlZWVjBs3jrPOOotPfvKTfPGLX2TMmDEAzJo1i/HjxzN69Gjmzp17eL+CggJqamrYvHkzI0eO5Nprr2X06NGcf/75HDjgTh5LuGGLYaVFuQAsrqjlsvF5HkdjjPHCT/62lnUf7XH1mKMG9OTWz4w+7vq77rqLNWvWsGrVKhYtWsSFF17ImjVrDg8vnDdvHn369OHAgQOcddZZXHrppeTkHD3Eury8nMcff5zf//73XH755cyfP5+rrjr5uxkmbAt9RL8scnukWT+6McZTEyZMOGqs+H333cfpp5/OpEmT2Lp1K+Xl5cfsU1hYyLhx4wAYP348mzdvdiWWhG2hJyUJU4bm8kZFLaoaiGFMxpijnaglHSvdu3c//HzRokW88sorLFmyhMzMTM4+++w2x5J369bt8PPk5GTXulwStoUOMHVYLjV7D/L+9r3tb2yMMS7Iysqivr6+zXV1dXX07t2bzMxMNmzYwNKlsb1hW8K20OFIP/obFTWMODXL42iMMUGQk5NDaWkpxcXFZGRk0K9fv8Prpk+fzoMPPsjYsWMZMWIEkyZNimlsrt1TtKNKSkrUjRtcnPPLRQzOyeQPX5ngQlTGmHi3fv16Ro4c6XUYMdHWexWRlapa0tb2Cd3lAk4r/c1NO62crjEm8HyR0PcfambV1q6dgWWMMfEu4RP65KFOOV2bNWpMcHjVVRxLnXmPCZ/QD5fTtfHoxgRCeno6tbW1vk7q4Xro6enpHdovoUe5hE0tyuV3/6pkT0MjPdOjv7uHMSbx5OXlUVVVRXW1vxtx4TsWdYQvEnppUS6/fa2CNzfu5LxR/drfwRiTsFJTUzt0F58gSfguF4AzBzvldBdbP7oxJsB8kdCtnK4xxvgkoYPTj16xY6+V0zXGBJZvEnpkOV1jjAkiVxK6iAwSkddEZL2IrBWR69w4bkecdqpTTtf60Y0xQeXWKJcm4EZVfUtEsoCVIvKyqq5z6fjtOlJOt8bK6RpjAsmVFrqqblPVt0LP64H1wEA3jt0RU4tyqa63crrGmGByvQ9dRAqAM4A321g3W0RWiMiKrpgUUDrsSDldY4wJGlcTuoj0AOYD16vqMTf6U9W5qlqiqiV9+/Z189QADMzOYEhud+tHN8YEkmsJXURScZL5Y6r6tFvH7ajSolyWbqylsdnK6RpjgsWtUS4CPAysV9Vfu3HMzgqX0317i5XTNcYEi1st9FLgS8A5IrIq9Jjh0rE7xMrpGmOCypVhi6r6BhAX4wTD5XQXV9Rww3nDvQ7HGGNixjczRSNNLcpl1dbd1Dc0eh2KMcbEjC8TemlRLs0tytKNO70OxRhjYsaXCd3K6RpjgsiXCd3K6RpjgsiXCR2OlNP9uK7B61CMMSYmfJvQw+V0rZVujAkK3yb0007NIqe7ldM1xgSHbxN6UpIwpehIOV1jjPE73yZ0gGmhcrrlO6ycrjHG/3yd0A+X0y23bhdjjP/5OqEPzM6gMLe7XRg1xgSCrxM6QGlRjpXTNcYEgu8T+tSivuw/1MyqrVZO1xjjb75P6JOHhMrpWj+6McbnfJ/Qe2WmMiYv2/rRjTG+5/uEDjC1KMfK6RpjfC8gCb0vzS3Km1ZO1xjjY4FI6GcOziY9Ncm6XYwxvhaIhO6U082xhG6M8bVAJHRw+tGtnK4xsbG4oobp9/ybymoruxFLAUrofQGs+qIxXWzfwSa+/9RqNnxcz03z36WlxYrjxUpgErqV0zUmNn798vt8uPsAV07MZ9nmnfx52RavQwoM1xK6iMwTkR0issatY7rJyuka0/VWV+3mD4s3ceXEfH42q5jSohzuen6DdXXGiJst9EeA6S4ez3VTi3LYYeV0jekSTc0tzJn/Lrk9uvGDC05DRLjjs2Noamnh5mfXWEMqBlxL6Kr6byCuB3pPHeb0o1sZAGPc9/Abm1i3bQ+3zRxNz/RUAAbndOeG84bzyvrtLHz3Y48j9L/A9KHDkXK61o9ujLu21O7nN6+8z/mj+jG9uP9R675aWsiYgb24dcEadu8/5FGEwRDThC4is0VkhYisqK6ujuWpD7Nyusa4S1X50bPvkpKUxE9mjj5mfUpyEnddOoZd+xu5/R/rPYgwOGKa0FV1rqqWqGpJ3759Y3nqw6YW5bLPyuka45pnV33I6+U1fH/6CPr3ymhzm9EDevFfnxjCX1dWWZdnFwpUlwvA5CG5Vk7XGJfs3HeIn/59PWfmZ3PVxMEn3PbbnxpGYW53bnpmNQcONccowmBxc9ji48ASYISIVInI19w6tpvC5XStH92Yk/ezf6xjz4FG7rxkLElJcsJt01OTufOSMWzdeYBfv/xejCIMFjdHuVyhqv1VNVVV81T1YbeO7bapRTm8beV0jTkpb5TX8PRbH/L1/xjKiFOzotpn0pAcrpiQz8NvbGJ1lXV7ui1wXS4ApUW5Vk7XmJNw4FAzP3zmXQpzu/PNc4o6tO9NM04jt0c3vv/Uahuc4LJAJvTxg3tbOV1jTsK9/yxny8793PHZMaSnJndo357pqfx0VjEbPq5n7r83dlGEwRTIhB4up2v96MZ03LqP9vD71zdyeUkek4fmdOoYnx59KjPGnMq9/yy3iowuCmRCB6cfvXzHXrbvsRoTxkSruUW56enV9M5M5YczRp7Usf734tGkpyRx09NWkdEtgU3opUW5gA1fNKYj/li2mXeq6rjlM6PJzkw7qWOdkpXOzReOYtmmnTy+3CoyuiGwCX3kqT2tnK4xHVC1az+/fOk9PjmiL58Z27/9HaLwuZI8pgzN4a6FVpHRDYFN6FZO15joqSq3PLcWVfjprGJETjzmPFoiwp2XjOFQs1VkdENgEzocKadbYeV0jTmhf7y7jVc37ODG84eT1zvT1WNbRUb3BDqhh/vRX7d+dGOOq25/I/+7YB1j83rxldLCLjnH16YWUjywp1VkPEmBTuh5vTMpyMm0fnRjTuDO59eza/8h7vjsGJLbmd7fWSnJSfz80rFWkfEkBTqhA0wdlmvldI05jqUba3li+VaumVpI8cBeXXqu0QN6MdsqMp4US+ihcrrvWDldY47S0NjMD59+l0F9Mrj+3OExOed1oYqMP3zmXavI2AmBT+iTh+QiYv3oxrT2wGsVbKzZxx2fHUNGWsem93dWuCLjlp3OHZBMxwQ+offKTGXswF7Wj25MhPe31/O7f1VyyRkDmTYstjejcSoyDuKh1zdaRcYOCnxCB6cf3crpGuNoaVHmzF9Nj24p/OjCk5ve31lzLhhJbo9u/GD+u3Z9qwMsoXOknO6yTVZO15jH3vyAt7bs5uYLR5HTo5snMfTKSOW2mcWs37bHKjJ2gCV04Mx8K6drDMDHdQ38/IX3mFqUyyVnDvQ0lunFp3JBsVORcaNVZIyKJXScCzFnFfSxoVIm8G5dsIbG5hZu/6x70/tPxk9CFRnnWEXGqFhCD5k2LNfK6ZpAe2HNx7y4djvXnzucwTndvQ4HgFN6pvOjC0eybNNOnli+1etw4p4l9JBwGQAb7WKCaE9DI7cuWMPI/j25ZlrXTO/vrMtLBjF5SA53LlxvFRnbYQk9ZOSpPenTPc360U0g3f3CBqrrD3LXJWNITY6vtBBZkfHHz1lFxhOJr385DyUlCVOG5vBGuZXTNcGyYvNOHl26hS9PKeT0Qdleh9OmgtzufOe84by8bjvPr7GKjMdjCT3CtGG57Kg/yIaP670OxZiYONTUwk1Pv8vA7AxuPD820/s765qphYwe0JNbnltL3X6bM9KWFLcOJCLTgXuBZOAhVb3LrWPHSrgf/YJ7XyctJYmsbin0SE+hRzfnkZWeSlb4dXp4WUpoWerh1+H1WekpdEuJzZRpYzrjwX9VUr5jL/O+XEL3bq6lgy4Rrsg48/7F3L5wHXdfdrrXIcUdV/4FRSQZuB84D6gClovIAlVd58bxYyWvdyYPXnUmldX7qG9oYu/BRvY2NLH3YBN7Gpr4aPcB9h50Xtc3NNLY3H7XTFpy0lFfChlpySQJJIk4jyTnuYiQJJAc8TxyfVLEssPbJrXaNmKUWTiyyN4jDS09elnEcz12aeS2EhF3clL4p9NdlXzUsiPPw3Ee3ufwthy1rYjQ1iC51iPn2tqqrdF1rRc52wgiR9aFzykSeuCslNbrIveLeB0+T1KSHNkn/G9E+N/qyO8tcl1HtuXwueSo9xv5Po5+HbG9tF52ZJ+tO/fz21cruGhsf845rd+xv8Q4VDywF9dOG8KD/6qkR7dUemakkJqcRHKSkJIkh5+nJgspSUmkHPVTSElOcn5GPk+O2C+0bXJSxCdN3P/dpyUnkZbifgeJW1/JE4AKVd0IICJPADOBhEroANOLo79XYkNjs5PgG8JJ3kn0R5J+0zFfDAcam2lpgRZVmluUxmZ1nqtzm68W1cPrVaE5tEzVWRa53nmE93PuyB6Z3Fp/6I5eFvlOItZL6yVHloXP1dziPMLnbFalJfTTLj8klp7pKdzymVFeh9Eh1587jGWbanmkbBOJOjT9Z7OKuWrSYNeP61ZCHwhEDhKtAia23khEZgOzAfLz8106tXfSU5NJT00m16Pp0fHocMIPfyFFJvzDzzlqWfjL6dhjtXrd5vnaWNZqS9XQg6O/cCKXaSh2PXzMyOVHrzt8/ND68Bdu+DlKqy9hZ+OWUBwth491ZJvWyyO3Peq9t3odfj96vOUcvb717620KJdTstLb+M3Gr/TUZJ7+n1LAqTvT1KI0tbTQ2Ox8npqaW2hsUZqblcaWFpqanfVH/ww9miP2O3yMFppajvw125nffXu/9zPze7v7SwlxK6G39dfyMf/VVHUuMBegpKQkQb9bzYmIOH/CxndvrPGLpCQhLUlIs/EdgHujXKqAQRGv84CPXDq2McaYKLiV0JcDw0SkUETSgC8AC1w6tjHGmCiIW5NoRGQGcA/OsMV5qnp7O9tXAx908nS5QKJO6bTYvWGxx16ixg3xHftgVW3zriOuJfRYEpEVqlridRydYbF7w2KPvUSNGxI3druSYIwxPmEJ3RhjfCJRE/pcrwM4CRa7Nyz22EvUuCFBY0/IPnRjjDHHStQWugkwEVkkIrtExKboGhPBErpJKCJSAEzDmYl8cQzPa5NfTdxLuIQuItNF5D0RqRCROV7HEy0RGSQir4nIehFZKyLXeR1TR4hIsoi8LSJ/9ziU/wSWAo8AV4cXikiGiPxKRD4QkToReSO0LDv0e98vIs0isl1EvhzaZ5GIXBNxjC+LyBsRr1VEviEi5UB5aNm9IrJVRPaIyEoRmRaxfbKI/FBEKkWkPrR+kIjcLyK/inwTIvI3Ebn+RG9URL4T+qysEZHHRSRui66IyDwR2SEiayKW9RGRl0WkPPSzawqYnKTjxP4LEdkgIqtF5BkRic87f7SSUAk9okzvBcAo4AoRSZRScU3Ajao6EpgEfCOBYge4DljvdRA4Cf2x0OPTIhKu+/pLYDwwBegDfB9oAR4CJgNfA7JwfverOnC+WTiF5sL/VsuBcaFz/Bn4a0SivQG4ApgB9AS+CuwH/ojzWU0CEJFc4FPA48c7qYgMBL4NlKhqMc6EvS90IO5YewSY3mrZHOCfqjoM+GfodTx6hGNjfxkoVtWxwPvATbEOqjMSKqETUaZXVQ8B4TK9cU9Vt6nqW6Hn9TjJcaC3UUVHRPKAC3GSo5dxTAUGA0+q6kqgEvhiKFF+FbhOVT9U1WZVLQO6AecAz6vq46q6X1U3qWpHEvqdqrpTVQ8AqOqjqlqrqk2q+qvQOUaEtr0GuFlV31PHO6FtlwF1OEkcnMS8SFW3t3PuFCAj1N2TSRzXR1LVfwM7Wy2eifNlRujnrJgGFaW2YlfVl1S1KfRyKU59qriXaAm9rTK9CZEUI4X6gc8A3vQ2kqjdw5EWr5euBl5S1fCU7D+HluUC6TgJPtIQoBGnztDbIvKQiHTv4DkjP2+IyI2hbrM6EdkN9AqdH5wCda1jCPsjcFXo+VXA/53opKr6Ic5fHVuAbUCdqr7Uwdi91k9Vt4HToAFO8Tiezvoq8LzXQUQj0RJ6VGV645mI9ADmA9er6h6v42mPiFwE7Ai1iL2MIwO4HPgPEflYRD4GvgOcDvQHGoChrXZLIVSTQ1XPAPZx9J/9+3BavmGntnHqw5+vUH/5D0Jx9FbVbJyWd/hzubWNGMIeBWaKyOnASODZdt5vb5wWbiEwAOguIledaB/jPhH5EU536WNexxKNREvoCV2mV0RScZL5Y6r6tNfxRKkUuFhENuN0cZ0jIo96EMcsoBmnL3tc6DESeB2nX30e8GsRGRC6ODkZ2IHz+RgvIpcDzwATRWRc6JirgEtEJFNEinD62U8kC+c/dzWQIiK34PSVhz0E/FREholjrIjkAKhqFU7/+/8B88NdOCdwLrBJVatVtRF4Guf6QCLZLiL9AUI/d3gcT4eIyNXARcCVmiATdhItoSdsmV4REeBhYL2q/trreKKlqjepap6qFuD8vl9VVS9ailcDf1DVLar6cfgB/Ba4Eqfl/S7OZ2Qn8HOcxLsZ+C/gRuBFnIui4bsL/wY4BGzH6RJprxX2Is6f3u/jVApt4OgumV8DTwIvAXtw/r0zItb/ERhDO90tIVuASaEvG8Hpf4+Hi9IdsYAjI5GuBp7zMJYOEeem9z8ALlbV/V7HE62EmykqHSzTGy9CF/Rex0k64b7oH6rqQu+i6hgRORv4rqpe5HUs0Qq1xh8C0oCNwFdUdZdHsXwCp+ulQFXbvR4hIj8BPo/zV8HbwDWqerBro+wcEXkcOBuni2s7cCtOt9KTQD7OF9TnVLX1hVPPHSf2m3AueNeGNluqql/3JMAOSLiEbkwiCnW3PQG8o6q3eR2P8adE63IxJuGIyEhgN87F23s8Dsf4mLXQjTHGJ6yFbowxPtFuwSERmYczdGdHaApy6/UC3Isz3Xk/8OXwjMgTyc3N1YKCgg4HbIwxQbZy5cqa491TNJoKco/gDA3703HWXwAMCz0mAr8L/TyhgoICVqxYEcXpjTHGhInIB8db126Xy3FqNESaCfwpVLtiKZAdnkxgjDEmdtyo8Xy8+irbWm8oIrOB2QD5+fkunNp9az6sY0d9g9dhGBMTw/tlkdc7s/0N48z72+up2pUw832O0VW/dzcSetT1VVR1LqF79ZWUlMTd8JqavQeZef9imlviLjRjukS3lCTmXHAaV08uICmprf/K8aWhsZlfvfQeD72xiUQeoPezWcVcNWmw68d1I6EndH2VSGWVtTS3KPd+YRwFOR0tymdMYmlqaeH+1yr5yd/W8dLa7fzic2PjurX+blUdNzy5ivIde7lyYj6fKxnUZmsyEQzsndH+Rp3gRkJfAHxTRJ7AuRhaFy6ZmWjKKmrISk/horEDSE6A1ooxJ+vhq3vzl+Vb+enf1zH9nte59TOjuGx8Hs7gtfjQ2NzCA69V8v9eLSenRxqPfOUszh6RqJV4u1Y0wxYP1zkQkSqcOgepAKr6ILAQZ8hiBc6wxa90VbBdbXFlDZOG5FgyN4EhInxhQj6lRbnc+Nd3+N5Tq3lx7XbuvGQMfbO8vwd3xY693PjkKt6pquPi0wdw28zRZGemeR1W3Go3oavqFe2sV+AbrkXkka0797N15wG+VlrodSjGxNygPpk8ce0k5i3exN0vvsen7/k3d3y2mOnF3gxYa2lRHinbzM9f2EBGWjL3f/FMLhxrg+faYzNFQ8oqnZvglBbltrOlMf6UlCRcM20If//WVAZkp/P1R9/ihr+sou5AY0zjqNq1nysfepPb/r6O0qJcXrr+E5bMo+RGH7ovLK6opW9WN4pO6eF1KMZ4ani/LJ75n1L+36sV3P9aBUs21nL3ZWOZNqzNyYmuUVX+urKK2/62DlXl55eO4fKSQXHVnx/vrIWO80Eqq6xlytAc+/AYA6QmJ3HDecN5+r+nkJmWzJceXsYtz61h/6Gm9nfuhOr6g1z7p5V8/6nVjBrQkxeu/wSfPyvf/j92kLXQgfIde6nZe5DSodbdYkyk0wdl849vT+PuF95j3uJNvF5ew68uP50z83u7do7n393Gj55dw96DTdx84Ui+WlqYEGPi45G10IHFFU7/+eShOR5HYkz8SU9N5pbPjOLP107kUFMLl/2ujF+8uIFDTe3edOmE6g408p2/rOK/H3uLAdnp/ONbU7lm2hBL5ifBEjpO/3l+n0wG9YnfSRXGeG3K0FxeuH4al43P4/7XKpl5/2LWb9vTqWO9Xl7N9Hv+zYJ3PuK6Tw3jmf8pZVi/LJcjDp7AJ/Sm5hbe3FhLaZG1zo1pT1Z6KndfdjoP/WcJ1fUNzPztYn63qDLqchn7DzVxy3Nr+NLDy8hMS+bp/57Cd84bTmpy4FORKwLfh77moz3UH2xiivWfGxO1c0f148X8T3Dzs2v4+Qsb+Of67fzyc6dTkHv8khkrP9jFjU+uYnPtfr42tZDvfXoE6anJMYza/wL/tWj958Z0Tk6Pbjxw5Znc8/lxvL+9ngvufZ1Hl35A69taHmpq4e4XNvC5B8tobFYev3YSP75olCXzLhD4FvqSylpOOzWL3B7eT3M2JtGICLPOGMjEIX34/lOrufnZNby0bjt3XzqWU3uls37bHm548h3Wb9vD5SV5/PiiUWSlp3odtm8FOqE3NDazfPNOrpzofhlLY4Kkf68M/vTVCTz65hbu+Md6zv/Nv7h43AD+snwrvTLSeOg/Szh3VD+vw/S9QCf0t7bs4mBTC1Osu8WYkyYifGnSYKYV5XLDk6t4dOkWLig+lds/O4Y+3a2gViwEOqEvqawlOUmYOKSP16EY4xsFud3569enUFm9l2Gn9LDZnjEU6IS+uKKGsXm9rE/PGJclJwnDbVx5zAV2lEt9QyPvVNVZd4sxxjcCm9CXbdpJc4ta/RZjjG8ENqGXVdbSLSWJMwe7V2TIGGO8FNiEvriihpKC3ja5wRjjG4FM6DV7D7Lh43qb7m+M8ZVAJvSlG2sB7IKoMcZXApnQF1fUktUthTEDe3kdijHGuCaQCb2ssoaJQ/qQYiU7jTE+EriMVrVrPx/U7rf+c2OM7wQuoZdVOv3npUWW0I0x/hK8hF5RQ26PNIb36+F1KMYY46pAJXRVZXFlLZOH5lrBIGOM7wQqoVdW76W6/iClNlzRGONDgUroiyus/9wY418BS+g15PXOYFCfTK9DMcYY1wUmoTe3KEs31lp1RWOMbwUmoa/9qI49DU1MKbL+c2OMPwUmoYf7z21CkTHGrwKT0Msqaxjerwd9s7p5HYoxxnSJQCT0g03NLN+801rnxhhfC0RCf3vLbhoaW2y4ojHG1wKR0MsqakgSmFDYx+tQjDGmywQjoVfWMiYvm14ZqV6HYowxXcb3CX3fwSZWbd1t0/2NMb4XVUIXkeki8p6IVIjInDbW54vIayLytoisFpEZ7ofaOcs27aSpRa3/3Bjje+0mdBFJBu4HLgBGAVeIyKhWm90MPKmqZwBfAB5wO9DOKqusIS0lifGDe0/4/foAAArmSURBVHsdijHGdKloWugTgApV3aiqh4AngJmttlGgZ+h5L+Aj90I8OYsrahmf35v01GSvQzHGmC4VTUIfCGyNeF0VWhbpf4GrRKQKWAh8q60DichsEVkhIiuqq6s7EW7H7Nx3iHXb9lBq0/2NMQEQTUJv604Q2ur1FcAjqpoHzAD+T0SOObaqzlXVElUt6du3b8ej7aAlodvNTbYJRcaYAIgmoVcBgyJe53Fsl8rXgCcBVHUJkA54nkXLKmvo0S2F0/N6eR2KMcZ0uWgS+nJgmIgUikgazkXPBa222QJ8CkBERuIk9K7vU2lHWWUtEwv7kJLs+9GZxhjTfkJX1Sbgm8CLwHqc0SxrReQ2Ebk4tNmNwLUi8g7wOPBlVW3dLRNTH+0+wKaafUyx4YrGmIBIiWYjVV2Ic7EzctktEc/XAaXuhnZyyirD5XLtgqgxJhh82xdRVlFDTvc0RvTL8joUY4yJCV8mdFVlcWUNk4fmkJTU1iAdY4zxH18m9MrqfWzfc9DqnxtjAsWXCX1JZQ2ATSgyxgSKLxP64opaBmZnkN8n0+tQjDEmZnyX0JtblCUba5kyNAcR6z83xgSH7xL6+m17qDvQaOVyjTGB47uEvrjC6T+38efGmKDxX0KvrGXYKT04pWe616EYY0xM+SqhH2pqYfmmndY6N8YEkq8S+qqtuznQ2Gz1W4wxgeSrhL64ooYkgUlDrIVujAkeXyX0ssoaigf2oldGqtehGGNMzPkmoe8/1MTbW3bbdH9jTGD5JqEv27STpha16f7GmMDyTUIvq6wlLTmJksF9vA7FGGM84aOEXsMZ+dlkpCV7HYoxxnjCFwl99/5DrP1oj033N8YEmi8S+pLKWlStXK4xJth8kdAXV9bQPS2ZsXnZXodijDGeieom0fGurLKWCYV9SE32xfeTMeYEGhsbqaqqoqGhwetQulR6ejp5eXmkpkY/rybhE/rHdQ1srN7HFyfkex2KMSYGqqqqyMrKoqCgwLf3PFBVamtrqaqqorCwMOr9Er5Je6Rcrl0QNSYIGhoayMnx9w1sRIScnJwO/xWS8Am9rLKWPt3TOO3ULK9DMcbEiJ+TeVhn3mNCJ3RVpayyhslDckhK8v8/sDHGnEhCJ/RNNfvYVtfAFBuuaIyJkd27d/PAAw90eL8ZM2awe/fuLojoiIRO6GWVtYD1nxtjYud4Cb25ufmE+y1cuJDs7K4dWp3Qo1zKKmsY0CudgpxMr0MxxnjgJ39by7qP9rh6zFEDenLrZ0Yfd/2cOXOorKxk3LhxpKam0qNHD/r378+qVatYt24ds2bNYuvWrTQ0NHDdddcxe/ZsAAoKClixYgV79+7lggsuYOrUqZSVlTFw4ECee+45MjIyTjr2hG2ht7QoSyprmVKUG4gLJMaY+HDXXXcxdOhQVq1axS9+8QuWLVvG7bffzrp16wCYN28eK1euZMWKFdx3333U1tYec4zy8nK+8Y1vsHbtWrKzs5k/f74rsSVsC33dtj3s2t9o0/2NCbATtaRjZcKECUeNFb/vvvt45plnANi6dSvl5eXk5BydpwoLCxk3bhwA48ePZ/Pmza7EkrAJfYn1nxtj4kD37t0PP1+0aBGvvPIKS5YsITMzk7PPPrvNseTdunU7/Dw5OZkDBw64EkvCdrksrqxhaN/u9OuZ7nUoxpgAycrKor6+vs11dXV19O7dm8zMTDZs2MDSpUtjGltCttAPNbWwbNNOLhuf53UoxpiAycnJobS0lOLiYjIyMujXr9/hddOnT+fBBx9k7NixjBgxgkmTJsU0toRM6KurdrP/UDNThlr/uTEm9v785z+3ubxbt248//zzba4L95Pn5uayZs2aw8u/+93vuhZXQna5LK6oRQQmDbGEbowxYYmZ0CtrKB7Qi+zMNK9DMcaYuJFwCX3/oSbe3rLLpvsbE2Cq6nUIXa4z7zGqhC4i00XkPRGpEJE5x9nmchFZJyJrRaTtDiYXrNi8i8ZmteGKxgRUeno6tbW1vk7q4Xro6ekdG8XX7kVREUkG7gfOA6qA5SKyQFXXRWwzDLgJKFXVXSJySoei6IB3tu4mNVk4q6B3V53CGBPH8vLyqKqqorq62utQulT4jkUdEc0olwlAhapuBBCRJ4CZwLqIba4F7lfVXQCquqNDUXTAN88p4vKzBpGZlpADdIwxJyk1NbVDd/EJkmi6XAYCWyNeV4WWRRoODBeRxSKyVESmuxVgayJik4mMMaYN0TRz26p81brzKgUYBpwN5AGvi0ixqh5V/FdEZgOzAfLz7R6gxhjjpmha6FXAoIjXecBHbWzznKo2quom4D2cBH8UVZ2rqiWqWtK3b9/OxmyMMaYN0t6VYhFJAd4HPgV8CCwHvqiqayO2mQ5coapXi0gu8DYwTlWPrRt5ZJ9q4INOxp0L1HRyX69Z7N6w2GMvUeOG+I59sKq22SJut8tFVZtE5JvAi0AyME9V14rIbcAKVV0QWne+iKwDmoHvnSiZh47b6Sa6iKxQ1ZLO7u8li90bFnvsJWrckLixRzVURFUXAgtbLbsl4rkCN4QexhhjPJBwM0WNMca0LVET+lyvAzgJFrs3LPbYS9S4IUFjb/eiqDHGmMSQqC10Y4wxrVhCN8YYn0i4hB5N5cd4JCKDROQ1EVkfqkh5ndcxdYSIJIvI2yLyd69j6QgRyRaRp0RkQ+h3P9nrmKIlIt8JfVbWiMjjIhK3NS9EZJ6I7BCRNRHL+ojIyyJSHvoZlxX1jhP7L0KfmdUi8oyIZHsZY7QSKqFHVH68ABgFXCEio7yNKmpNwI2qOhKYBHwjgWIHuA5Y73UQnXAv8IKqngacToK8BxEZCHwbKFHVYpw5IF/wNqoTegRoXcNpDvBPVR0G/DP0Oh49wrGxvwwUq+pYnImVN8U6qM5IqIROROVHVT0EhCs/xj1V3aaqb4We1+MkltZFzuKSiOQBFwIPeR1LR4hIT+ATwMMAqnqodX2hOJcCZIRma2dybMmNuKGq/wZ2tlo8E/hj6PkfgVkxDSpKbcWuqi+palPo5VKckidxL9ESejSVH+OeiBQAZwBvehtJ1O4Bvg+0eB1IBw0BqoE/hLqLHhKR7l4HFQ1V/RD4JbAF2AbUqepL3kbVYf1UdRs4DRqgy+6T0MW+CrR95+c4k2gJPZrKj3FNRHoA84HrVXWP1/G0R0QuAnao6kqvY+mEFOBM4Heqegawj/j9s/8oof7mmUAhMADoLiJXeRtV8IjIj3C6Sx/zOpZoJFpCj6byY9wSkVScZP6Yqj7tdTxRKgUuFpHNOF1c54jIo96GFLUqoEpVw38JPYWT4BPBucAmVa1W1UbgaWCKxzF11HYR6Q8Q+tllN77pCiJyNXARcKUmyISdREvoy4FhIlIoImk4F4kWeBxTVEREcPpy16vqr72OJ1qqepOq5qlqAc7v+1VVTYiWoqp+DGwVkRGhRZ/i6DttxbMtwCQRyQx9dj5FglzQjbAAuDr0/GrgOQ9j6ZBQBdkfABer6n6v44lWQiX00EWKcOXH9cCTkWV841wp8CWcFu6q0GOG10EFwLeAx0RkNTAOuMPjeKIS+qviKeAt4F2c/6txOx1dRB4HlgAjRKRKRL4G3AWcJyLlOPckvsvLGI/nOLH/FsgCXg79X33Q0yCjZFP/jTHGJxKqhW6MMeb4LKEbY4xPWEI3xhifsIRujDE+YQndGGN8whK6Mcb4hCV0Y4zxif8PWswO5kcvg88AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(mod.history['loss'], label='train')\n",
    "#pyplot.plot(mod.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(mod.history['sparse_categorical_accuracy'], label='train')\n",
    "#pyplot.plot(mod.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test.astype('float32'), verbose=0)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "yhat_classes = np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n",
      "Precision: 1.000000\n",
      "Recall: 1.000000\n",
      "F1 score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, yhat_classes,average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes, average='macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 1.000000\n",
      "[[22  0]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(Y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "#fprate, tprate, thresholds = roc_curve(Y_test, yhat_probs, average = 'macro')\n",
    "#print('ROC AUC: %f' % thresholds)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 5\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, yhat_classes, pos_label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[22  0]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "matrix = confusion_matrix(Y_test, yhat_classes, labels= [0, 1])\n",
    "print('Confusion matrix : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = classification_report(Y_test,yhat_classes, labels= [0, 1])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/Binary Classifcation/Without IP/CNN-DBN/CNN\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/Binary Classifcation/Without IP/CNN-DBN/CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extracted from feature extractor part of deep learning model\n",
    "X_ext = feature_extractor.predict(X_train)\n",
    "X_ext_test = feature_extractor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in X_ext:\n",
    "    print(f'{np.isnan(i).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 64],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.01,\n",
    "                                         n_epochs_rbm=32,\n",
    "                                         n_iter_backprop=512,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del classifier\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 315888.562500\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 412166.468750\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 908030.562500\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2289085.250000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3896281.500000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4419977.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5050153.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 6620368.500000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 6396626.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 7576821.500000\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 6235211.500000\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 7918479.000000\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 9503445.000000\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 7980471.000000\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 10958792.000000\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 9445141.000000\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 11036441.000000\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 11301991.000000\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 12297965.000000\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 12571699.000000\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12884434.000000\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 13732457.000000\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12884850.000000\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 12842800.000000\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 13541523.000000\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 13423319.000000\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 15966062.000000\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 14326595.000000\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 13950937.000000\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 15120086.000000\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 13963269.000000\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 14123982.000000\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1163427446784.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2595501375488.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 2739684507648.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3994508853248.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 4113330339840.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4485016453120.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 4857584943104.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 5551776858112.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 10800719724544.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 11846839959552.000000\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 11679785025536.000000\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 16569515114496.000000\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 12039132020736.000000\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 12764542140416.000000\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 14837256028160.000000\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 16302963949568.000000\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 16299627380736.000000\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 15396266573824.000000\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13932409389056.000000\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 18998747987968.000000\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 20527087353856.000000\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 21546095607808.000000\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 21040069607424.000000\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 21967537176576.000000\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 26588615802880.000000\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 21248786563072.000000\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 25280173309952.000000\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 24546176401408.000000\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 20464132947968.000000\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 20604398862336.000000\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 19789604978688.000000\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 23329027129344.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 15732.897461\n",
      ">> Epoch 1 finished \tANN training loss 0.372896\n",
      ">> Epoch 2 finished \tANN training loss 0.371361\n",
      ">> Epoch 3 finished \tANN training loss 0.369926\n",
      ">> Epoch 4 finished \tANN training loss 0.368246\n",
      ">> Epoch 5 finished \tANN training loss 0.366849\n",
      ">> Epoch 6 finished \tANN training loss 0.365470\n",
      ">> Epoch 7 finished \tANN training loss 0.364213\n",
      ">> Epoch 8 finished \tANN training loss 0.362800\n",
      ">> Epoch 9 finished \tANN training loss 0.361628\n",
      ">> Epoch 10 finished \tANN training loss 0.360300\n",
      ">> Epoch 11 finished \tANN training loss 0.363383\n",
      ">> Epoch 12 finished \tANN training loss 0.362355\n",
      ">> Epoch 13 finished \tANN training loss 0.361149\n",
      ">> Epoch 14 finished \tANN training loss 0.359990\n",
      ">> Epoch 15 finished \tANN training loss 0.358833\n",
      ">> Epoch 16 finished \tANN training loss 0.357900\n",
      ">> Epoch 17 finished \tANN training loss 0.517114\n",
      ">> Epoch 18 finished \tANN training loss 0.451690\n",
      ">> Epoch 19 finished \tANN training loss 0.417238\n",
      ">> Epoch 20 finished \tANN training loss 0.398291\n",
      ">> Epoch 21 finished \tANN training loss 0.386735\n",
      ">> Epoch 22 finished \tANN training loss 0.379766\n",
      ">> Epoch 23 finished \tANN training loss 0.373033\n",
      ">> Epoch 24 finished \tANN training loss 0.368212\n",
      ">> Epoch 25 finished \tANN training loss 0.364560\n",
      ">> Epoch 26 finished \tANN training loss 0.361624\n",
      ">> Epoch 27 finished \tANN training loss 0.359255\n",
      ">> Epoch 28 finished \tANN training loss 0.358019\n",
      ">> Epoch 29 finished \tANN training loss 0.356159\n",
      ">> Epoch 30 finished \tANN training loss 0.355932\n",
      ">> Epoch 31 finished \tANN training loss 0.354036\n",
      ">> Epoch 32 finished \tANN training loss 0.352493\n",
      ">> Epoch 33 finished \tANN training loss 0.350961\n",
      ">> Epoch 34 finished \tANN training loss 0.349695\n",
      ">> Epoch 35 finished \tANN training loss 0.348569\n",
      ">> Epoch 36 finished \tANN training loss 0.347424\n",
      ">> Epoch 37 finished \tANN training loss 0.346328\n",
      ">> Epoch 38 finished \tANN training loss 0.343496\n",
      ">> Epoch 39 finished \tANN training loss 0.342764\n",
      ">> Epoch 40 finished \tANN training loss 0.342001\n",
      ">> Epoch 41 finished \tANN training loss 0.341308\n",
      ">> Epoch 42 finished \tANN training loss 0.340688\n",
      ">> Epoch 43 finished \tANN training loss 0.339976\n",
      ">> Epoch 44 finished \tANN training loss 0.339293\n",
      ">> Epoch 45 finished \tANN training loss 0.338700\n",
      ">> Epoch 46 finished \tANN training loss 0.338092\n",
      ">> Epoch 47 finished \tANN training loss 0.337590\n",
      ">> Epoch 48 finished \tANN training loss 0.336928\n",
      ">> Epoch 49 finished \tANN training loss 0.336284\n",
      ">> Epoch 50 finished \tANN training loss 0.335738\n",
      ">> Epoch 51 finished \tANN training loss 0.335145\n",
      ">> Epoch 52 finished \tANN training loss 0.334690\n",
      ">> Epoch 53 finished \tANN training loss 0.334140\n",
      ">> Epoch 54 finished \tANN training loss 0.333725\n",
      ">> Epoch 55 finished \tANN training loss 0.333297\n",
      ">> Epoch 56 finished \tANN training loss 0.332921\n",
      ">> Epoch 57 finished \tANN training loss 0.332626\n",
      ">> Epoch 58 finished \tANN training loss 0.332078\n",
      ">> Epoch 59 finished \tANN training loss 0.331647\n",
      ">> Epoch 60 finished \tANN training loss 0.331201\n",
      ">> Epoch 61 finished \tANN training loss 0.330832\n",
      ">> Epoch 62 finished \tANN training loss 0.330436\n",
      ">> Epoch 63 finished \tANN training loss 0.330074\n",
      ">> Epoch 64 finished \tANN training loss 0.329839\n",
      ">> Epoch 65 finished \tANN training loss 0.329463\n",
      ">> Epoch 66 finished \tANN training loss 0.329079\n",
      ">> Epoch 67 finished \tANN training loss 0.328691\n",
      ">> Epoch 68 finished \tANN training loss 0.328333\n",
      ">> Epoch 69 finished \tANN training loss 0.328067\n",
      ">> Epoch 70 finished \tANN training loss 0.327650\n",
      ">> Epoch 71 finished \tANN training loss 0.327174\n",
      ">> Epoch 72 finished \tANN training loss 0.326958\n",
      ">> Epoch 73 finished \tANN training loss 0.326571\n",
      ">> Epoch 74 finished \tANN training loss 0.326478\n",
      ">> Epoch 75 finished \tANN training loss 0.326216\n",
      ">> Epoch 76 finished \tANN training loss 0.325927\n",
      ">> Epoch 77 finished \tANN training loss 0.325560\n",
      ">> Epoch 78 finished \tANN training loss 0.324701\n",
      ">> Epoch 79 finished \tANN training loss 0.324626\n",
      ">> Epoch 80 finished \tANN training loss 0.323369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 81 finished \tANN training loss 0.322160\n",
      ">> Epoch 82 finished \tANN training loss 0.321079\n",
      ">> Epoch 83 finished \tANN training loss 0.320781\n",
      ">> Epoch 84 finished \tANN training loss 0.319778\n",
      ">> Epoch 85 finished \tANN training loss 0.319138\n",
      ">> Epoch 86 finished \tANN training loss 0.317872\n",
      ">> Epoch 87 finished \tANN training loss 0.317614\n",
      ">> Epoch 88 finished \tANN training loss 0.317229\n",
      ">> Epoch 89 finished \tANN training loss 0.316725\n",
      ">> Epoch 90 finished \tANN training loss 0.316332\n",
      ">> Epoch 91 finished \tANN training loss 0.316218\n",
      ">> Epoch 92 finished \tANN training loss 0.315779\n",
      ">> Epoch 93 finished \tANN training loss 0.315629\n",
      ">> Epoch 94 finished \tANN training loss 0.315195\n",
      ">> Epoch 95 finished \tANN training loss 0.314873\n",
      ">> Epoch 96 finished \tANN training loss 0.314678\n",
      ">> Epoch 97 finished \tANN training loss 0.314919\n",
      ">> Epoch 98 finished \tANN training loss 0.314546\n",
      ">> Epoch 99 finished \tANN training loss 0.314671\n",
      ">> Epoch 100 finished \tANN training loss 0.314754\n",
      ">> Epoch 101 finished \tANN training loss 0.314576\n",
      ">> Epoch 102 finished \tANN training loss 0.314278\n",
      ">> Epoch 103 finished \tANN training loss 0.314067\n",
      ">> Epoch 104 finished \tANN training loss 0.313779\n",
      ">> Epoch 105 finished \tANN training loss 0.313263\n",
      ">> Epoch 106 finished \tANN training loss 0.313356\n",
      ">> Epoch 107 finished \tANN training loss 0.313007\n",
      ">> Epoch 108 finished \tANN training loss 0.312773\n",
      ">> Epoch 109 finished \tANN training loss 0.312661\n",
      ">> Epoch 110 finished \tANN training loss 0.312142\n",
      ">> Epoch 111 finished \tANN training loss 0.312139\n",
      ">> Epoch 112 finished \tANN training loss 0.310865\n",
      ">> Epoch 113 finished \tANN training loss 0.309419\n",
      ">> Epoch 114 finished \tANN training loss 0.308228\n",
      ">> Epoch 115 finished \tANN training loss 0.307676\n",
      ">> Epoch 116 finished \tANN training loss 0.306074\n",
      ">> Epoch 117 finished \tANN training loss 0.305640\n",
      ">> Epoch 118 finished \tANN training loss 0.304121\n",
      ">> Epoch 119 finished \tANN training loss 0.303125\n",
      ">> Epoch 120 finished \tANN training loss 0.301763\n",
      ">> Epoch 121 finished \tANN training loss 0.300387\n",
      ">> Epoch 122 finished \tANN training loss 0.299138\n",
      ">> Epoch 123 finished \tANN training loss 0.298437\n",
      ">> Epoch 124 finished \tANN training loss 0.297363\n",
      ">> Epoch 125 finished \tANN training loss 0.296436\n",
      ">> Epoch 126 finished \tANN training loss 0.296013\n",
      ">> Epoch 127 finished \tANN training loss 0.295942\n",
      ">> Epoch 128 finished \tANN training loss 0.295802\n",
      ">> Epoch 129 finished \tANN training loss 0.295296\n",
      ">> Epoch 130 finished \tANN training loss 0.294766\n",
      ">> Epoch 131 finished \tANN training loss 0.294475\n",
      ">> Epoch 132 finished \tANN training loss 0.294114\n",
      ">> Epoch 133 finished \tANN training loss 0.293866\n",
      ">> Epoch 134 finished \tANN training loss 0.293579\n",
      ">> Epoch 135 finished \tANN training loss 0.293446\n",
      ">> Epoch 136 finished \tANN training loss 0.293187\n",
      ">> Epoch 137 finished \tANN training loss 0.292918\n",
      ">> Epoch 138 finished \tANN training loss 0.292784\n",
      ">> Epoch 139 finished \tANN training loss 0.292459\n",
      ">> Epoch 140 finished \tANN training loss 0.292213\n",
      ">> Epoch 141 finished \tANN training loss 0.292277\n",
      ">> Epoch 142 finished \tANN training loss 0.292043\n",
      ">> Epoch 143 finished \tANN training loss 0.291830\n",
      ">> Epoch 144 finished \tANN training loss 0.291588\n",
      ">> Epoch 145 finished \tANN training loss 0.291486\n",
      ">> Epoch 146 finished \tANN training loss 0.291306\n",
      ">> Epoch 147 finished \tANN training loss 0.291346\n",
      ">> Epoch 148 finished \tANN training loss 0.291311\n",
      ">> Epoch 149 finished \tANN training loss 0.291121\n",
      ">> Epoch 150 finished \tANN training loss 0.290982\n",
      ">> Epoch 151 finished \tANN training loss 0.290853\n",
      ">> Epoch 152 finished \tANN training loss 0.290732\n",
      ">> Epoch 153 finished \tANN training loss 0.290570\n",
      ">> Epoch 154 finished \tANN training loss 0.290913\n",
      ">> Epoch 155 finished \tANN training loss 0.290925\n",
      ">> Epoch 156 finished \tANN training loss 0.290752\n",
      ">> Epoch 157 finished \tANN training loss 0.290593\n",
      ">> Epoch 158 finished \tANN training loss 0.290653\n",
      ">> Epoch 159 finished \tANN training loss 0.290758\n",
      ">> Epoch 160 finished \tANN training loss 0.290814\n",
      ">> Epoch 161 finished \tANN training loss 0.290625\n",
      ">> Epoch 162 finished \tANN training loss 0.290435\n",
      ">> Epoch 163 finished \tANN training loss 0.290210\n",
      ">> Epoch 164 finished \tANN training loss 0.290171\n",
      ">> Epoch 165 finished \tANN training loss 0.289974\n",
      ">> Epoch 166 finished \tANN training loss 0.289911\n",
      ">> Epoch 167 finished \tANN training loss 0.289717\n",
      ">> Epoch 168 finished \tANN training loss 0.289567\n",
      ">> Epoch 169 finished \tANN training loss 0.289445\n",
      ">> Epoch 170 finished \tANN training loss 0.289314\n",
      ">> Epoch 171 finished \tANN training loss 0.289165\n",
      ">> Epoch 172 finished \tANN training loss 0.289111\n",
      ">> Epoch 173 finished \tANN training loss 0.288992\n",
      ">> Epoch 174 finished \tANN training loss 0.288898\n",
      ">> Epoch 175 finished \tANN training loss 0.288911\n",
      ">> Epoch 176 finished \tANN training loss 0.288801\n",
      ">> Epoch 177 finished \tANN training loss 0.288780\n",
      ">> Epoch 178 finished \tANN training loss 0.288665\n",
      ">> Epoch 179 finished \tANN training loss 0.288756\n",
      ">> Epoch 180 finished \tANN training loss 0.288662\n",
      ">> Epoch 181 finished \tANN training loss 0.288592\n",
      ">> Epoch 182 finished \tANN training loss 0.288493\n",
      ">> Epoch 183 finished \tANN training loss 0.288403\n",
      ">> Epoch 184 finished \tANN training loss 0.288675\n",
      ">> Epoch 185 finished \tANN training loss 0.288661\n",
      ">> Epoch 186 finished \tANN training loss 0.288632\n",
      ">> Epoch 187 finished \tANN training loss 0.288531\n",
      ">> Epoch 188 finished \tANN training loss 0.288442\n",
      ">> Epoch 189 finished \tANN training loss 0.288328\n",
      ">> Epoch 190 finished \tANN training loss 0.288213\n",
      ">> Epoch 191 finished \tANN training loss 0.288126\n",
      ">> Epoch 192 finished \tANN training loss 0.288099\n",
      ">> Epoch 193 finished \tANN training loss 0.288138\n",
      ">> Epoch 194 finished \tANN training loss 0.288058\n",
      ">> Epoch 195 finished \tANN training loss 0.288311\n",
      ">> Epoch 196 finished \tANN training loss 0.288297\n",
      ">> Epoch 197 finished \tANN training loss 0.288553\n",
      ">> Epoch 198 finished \tANN training loss 0.288502\n",
      ">> Epoch 199 finished \tANN training loss 0.288373\n",
      ">> Epoch 200 finished \tANN training loss 0.288253\n",
      ">> Epoch 201 finished \tANN training loss 0.288407\n",
      ">> Epoch 202 finished \tANN training loss 0.288221\n",
      ">> Epoch 203 finished \tANN training loss 0.288091\n",
      ">> Epoch 204 finished \tANN training loss 0.288007\n",
      ">> Epoch 205 finished \tANN training loss 0.287910\n",
      ">> Epoch 206 finished \tANN training loss 0.287838\n",
      ">> Epoch 207 finished \tANN training loss 0.287794\n",
      ">> Epoch 208 finished \tANN training loss 0.288257\n",
      ">> Epoch 209 finished \tANN training loss 0.288405\n",
      ">> Epoch 210 finished \tANN training loss 0.288283\n",
      ">> Epoch 211 finished \tANN training loss 0.288382\n",
      ">> Epoch 212 finished \tANN training loss 0.288544\n",
      ">> Epoch 213 finished \tANN training loss 0.288637\n",
      ">> Epoch 214 finished \tANN training loss 0.288711\n",
      ">> Epoch 215 finished \tANN training loss 0.288575\n",
      ">> Epoch 216 finished \tANN training loss 0.288302\n",
      ">> Epoch 217 finished \tANN training loss 0.288480\n",
      ">> Epoch 218 finished \tANN training loss 0.288491\n",
      ">> Epoch 219 finished \tANN training loss 0.288327\n",
      ">> Epoch 220 finished \tANN training loss 0.288200\n",
      ">> Epoch 221 finished \tANN training loss 0.288024\n",
      ">> Epoch 222 finished \tANN training loss 0.287956\n",
      ">> Epoch 223 finished \tANN training loss 0.287835\n",
      ">> Epoch 224 finished \tANN training loss 0.287717\n",
      ">> Epoch 225 finished \tANN training loss 0.287905\n",
      ">> Epoch 226 finished \tANN training loss 0.287800\n",
      ">> Epoch 227 finished \tANN training loss 0.287847\n",
      ">> Epoch 228 finished \tANN training loss 0.287719\n",
      ">> Epoch 229 finished \tANN training loss 0.287800\n",
      ">> Epoch 230 finished \tANN training loss 0.287693\n",
      ">> Epoch 231 finished \tANN training loss 0.287686\n",
      ">> Epoch 232 finished \tANN training loss 0.287522\n",
      ">> Epoch 233 finished \tANN training loss 0.287438\n",
      ">> Epoch 234 finished \tANN training loss 0.287343\n",
      ">> Epoch 235 finished \tANN training loss 0.287492\n",
      ">> Epoch 236 finished \tANN training loss 0.287390\n",
      ">> Epoch 237 finished \tANN training loss 0.287300\n",
      ">> Epoch 238 finished \tANN training loss 0.287201\n",
      ">> Epoch 239 finished \tANN training loss 0.287111\n",
      ">> Epoch 240 finished \tANN training loss 0.287043\n",
      ">> Epoch 241 finished \tANN training loss 0.287077\n",
      ">> Epoch 242 finished \tANN training loss 0.287016\n",
      ">> Epoch 243 finished \tANN training loss 0.286965\n",
      ">> Epoch 244 finished \tANN training loss 0.286914\n",
      ">> Epoch 245 finished \tANN training loss 0.287144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 246 finished \tANN training loss 0.287326\n",
      ">> Epoch 247 finished \tANN training loss 0.287235\n",
      ">> Epoch 248 finished \tANN training loss 0.287150\n",
      ">> Epoch 249 finished \tANN training loss 0.287323\n",
      ">> Epoch 250 finished \tANN training loss 0.287151\n",
      ">> Epoch 251 finished \tANN training loss 0.287064\n",
      ">> Epoch 252 finished \tANN training loss 0.286996\n",
      ">> Epoch 253 finished \tANN training loss 0.287005\n",
      ">> Epoch 254 finished \tANN training loss 0.286905\n",
      ">> Epoch 255 finished \tANN training loss 0.286835\n",
      ">> Epoch 256 finished \tANN training loss 0.286775\n",
      ">> Epoch 257 finished \tANN training loss 0.287059\n",
      ">> Epoch 258 finished \tANN training loss 0.287226\n",
      ">> Epoch 259 finished \tANN training loss 0.287198\n",
      ">> Epoch 260 finished \tANN training loss 0.287047\n",
      ">> Epoch 261 finished \tANN training loss 0.287068\n",
      ">> Epoch 262 finished \tANN training loss 0.286965\n",
      ">> Epoch 263 finished \tANN training loss 0.287043\n",
      ">> Epoch 264 finished \tANN training loss 0.286975\n",
      ">> Epoch 265 finished \tANN training loss 0.287291\n",
      ">> Epoch 266 finished \tANN training loss 0.287326\n",
      ">> Epoch 267 finished \tANN training loss 0.287341\n",
      ">> Epoch 268 finished \tANN training loss 0.287821\n",
      ">> Epoch 269 finished \tANN training loss 0.287661\n",
      ">> Epoch 270 finished \tANN training loss 0.287477\n",
      ">> Epoch 271 finished \tANN training loss 0.287348\n",
      ">> Epoch 272 finished \tANN training loss 0.287252\n",
      ">> Epoch 273 finished \tANN training loss 0.287157\n",
      ">> Epoch 274 finished \tANN training loss 0.287057\n",
      ">> Epoch 275 finished \tANN training loss 0.286975\n",
      ">> Epoch 276 finished \tANN training loss 0.286893\n",
      ">> Epoch 277 finished \tANN training loss 0.286790\n",
      ">> Epoch 278 finished \tANN training loss 0.286729\n",
      ">> Epoch 279 finished \tANN training loss 0.286792\n",
      ">> Epoch 280 finished \tANN training loss 0.286723\n",
      ">> Epoch 281 finished \tANN training loss 0.286655\n",
      ">> Epoch 282 finished \tANN training loss 0.286603\n",
      ">> Epoch 283 finished \tANN training loss 0.286789\n",
      ">> Epoch 284 finished \tANN training loss 0.286763\n",
      ">> Epoch 285 finished \tANN training loss 0.286937\n",
      ">> Epoch 286 finished \tANN training loss 0.286999\n",
      ">> Epoch 287 finished \tANN training loss 0.287219\n",
      ">> Epoch 288 finished \tANN training loss 0.287120\n",
      ">> Epoch 289 finished \tANN training loss 0.287242\n",
      ">> Epoch 290 finished \tANN training loss 0.287213\n",
      ">> Epoch 291 finished \tANN training loss 0.287137\n",
      ">> Epoch 292 finished \tANN training loss 0.287158\n",
      ">> Epoch 293 finished \tANN training loss 0.287199\n",
      ">> Epoch 294 finished \tANN training loss 0.287167\n",
      ">> Epoch 295 finished \tANN training loss 0.287046\n",
      ">> Epoch 296 finished \tANN training loss 0.287153\n",
      ">> Epoch 297 finished \tANN training loss 0.287086\n",
      ">> Epoch 298 finished \tANN training loss 0.286980\n",
      ">> Epoch 299 finished \tANN training loss 0.286896\n",
      ">> Epoch 300 finished \tANN training loss 0.286810\n",
      ">> Epoch 301 finished \tANN training loss 0.286798\n",
      ">> Epoch 302 finished \tANN training loss 0.286689\n",
      ">> Epoch 303 finished \tANN training loss 0.286836\n",
      ">> Epoch 304 finished \tANN training loss 0.286746\n",
      ">> Epoch 305 finished \tANN training loss 0.286668\n",
      ">> Epoch 306 finished \tANN training loss 0.286573\n",
      ">> Epoch 307 finished \tANN training loss 0.286701\n",
      ">> Epoch 308 finished \tANN training loss 0.286861\n",
      ">> Epoch 309 finished \tANN training loss 0.286698\n",
      ">> Epoch 310 finished \tANN training loss 0.286629\n",
      ">> Epoch 311 finished \tANN training loss 0.286543\n",
      ">> Epoch 312 finished \tANN training loss 0.286484\n",
      ">> Epoch 313 finished \tANN training loss 0.286438\n",
      ">> Epoch 314 finished \tANN training loss 0.286350\n",
      ">> Epoch 315 finished \tANN training loss 0.286602\n",
      ">> Epoch 316 finished \tANN training loss 0.286578\n",
      ">> Epoch 317 finished \tANN training loss 0.286602\n",
      ">> Epoch 318 finished \tANN training loss 0.286517\n",
      ">> Epoch 319 finished \tANN training loss 0.286679\n",
      ">> Epoch 320 finished \tANN training loss 0.286577\n",
      ">> Epoch 321 finished \tANN training loss 0.286613\n",
      ">> Epoch 322 finished \tANN training loss 0.286632\n",
      ">> Epoch 323 finished \tANN training loss 0.286645\n",
      ">> Epoch 324 finished \tANN training loss 0.286611\n",
      ">> Epoch 325 finished \tANN training loss 0.286520\n",
      ">> Epoch 326 finished \tANN training loss 0.286463\n",
      ">> Epoch 327 finished \tANN training loss 0.286390\n",
      ">> Epoch 328 finished \tANN training loss 0.286340\n",
      ">> Epoch 329 finished \tANN training loss 0.286298\n",
      ">> Epoch 330 finished \tANN training loss 0.286266\n",
      ">> Epoch 331 finished \tANN training loss 0.286413\n",
      ">> Epoch 332 finished \tANN training loss 0.286508\n",
      ">> Epoch 333 finished \tANN training loss 0.286529\n",
      ">> Epoch 334 finished \tANN training loss 0.286503\n",
      ">> Epoch 335 finished \tANN training loss 0.286492\n",
      ">> Epoch 336 finished \tANN training loss 0.286424\n",
      ">> Epoch 337 finished \tANN training loss 0.286446\n",
      ">> Epoch 338 finished \tANN training loss 0.286370\n",
      ">> Epoch 339 finished \tANN training loss 0.286311\n",
      ">> Epoch 340 finished \tANN training loss 0.286280\n",
      ">> Epoch 341 finished \tANN training loss 0.286211\n",
      ">> Epoch 342 finished \tANN training loss 0.286200\n",
      ">> Epoch 343 finished \tANN training loss 0.286229\n",
      ">> Epoch 344 finished \tANN training loss 0.286256\n",
      ">> Epoch 345 finished \tANN training loss 0.286331\n",
      ">> Epoch 346 finished \tANN training loss 0.286301\n",
      ">> Epoch 347 finished \tANN training loss 0.286263\n",
      ">> Epoch 348 finished \tANN training loss 0.286369\n",
      ">> Epoch 349 finished \tANN training loss 0.286328\n",
      ">> Epoch 350 finished \tANN training loss 0.286283\n",
      ">> Epoch 351 finished \tANN training loss 0.286215\n",
      ">> Epoch 352 finished \tANN training loss 0.286619\n",
      ">> Epoch 353 finished \tANN training loss 0.286540\n",
      ">> Epoch 354 finished \tANN training loss 0.286433\n",
      ">> Epoch 355 finished \tANN training loss 0.286573\n",
      ">> Epoch 356 finished \tANN training loss 0.286629\n",
      ">> Epoch 357 finished \tANN training loss 0.286549\n",
      ">> Epoch 358 finished \tANN training loss 0.286468\n",
      ">> Epoch 359 finished \tANN training loss 0.286406\n",
      ">> Epoch 360 finished \tANN training loss 0.286352\n",
      ">> Epoch 361 finished \tANN training loss 0.286302\n",
      ">> Epoch 362 finished \tANN training loss 0.286266\n",
      ">> Epoch 363 finished \tANN training loss 0.286222\n",
      ">> Epoch 364 finished \tANN training loss 0.286162\n",
      ">> Epoch 365 finished \tANN training loss 0.286448\n",
      ">> Epoch 366 finished \tANN training loss 0.286381\n",
      ">> Epoch 367 finished \tANN training loss 0.286322\n",
      ">> Epoch 368 finished \tANN training loss 0.286318\n",
      ">> Epoch 369 finished \tANN training loss 0.286230\n",
      ">> Epoch 370 finished \tANN training loss 0.286201\n",
      ">> Epoch 371 finished \tANN training loss 0.286196\n",
      ">> Epoch 372 finished \tANN training loss 0.286593\n",
      ">> Epoch 373 finished \tANN training loss 0.286558\n",
      ">> Epoch 374 finished \tANN training loss 0.286591\n",
      ">> Epoch 375 finished \tANN training loss 0.286591\n",
      ">> Epoch 376 finished \tANN training loss 0.286507\n",
      ">> Epoch 377 finished \tANN training loss 0.286436\n",
      ">> Epoch 378 finished \tANN training loss 0.286336\n",
      ">> Epoch 379 finished \tANN training loss 0.286390\n",
      ">> Epoch 380 finished \tANN training loss 0.286289\n",
      ">> Epoch 381 finished \tANN training loss 0.286244\n",
      ">> Epoch 382 finished \tANN training loss 0.286202\n",
      ">> Epoch 383 finished \tANN training loss 0.286136\n",
      ">> Epoch 384 finished \tANN training loss 0.286079\n",
      ">> Epoch 385 finished \tANN training loss 0.286043\n",
      ">> Epoch 386 finished \tANN training loss 0.286024\n",
      ">> Epoch 387 finished \tANN training loss 0.286042\n",
      ">> Epoch 388 finished \tANN training loss 0.285999\n",
      ">> Epoch 389 finished \tANN training loss 0.285973\n",
      ">> Epoch 390 finished \tANN training loss 0.286025\n",
      ">> Epoch 391 finished \tANN training loss 0.286028\n",
      ">> Epoch 392 finished \tANN training loss 0.285997\n",
      ">> Epoch 393 finished \tANN training loss 0.286277\n",
      ">> Epoch 394 finished \tANN training loss 0.286191\n",
      ">> Epoch 395 finished \tANN training loss 0.286267\n",
      ">> Epoch 396 finished \tANN training loss 0.286202\n",
      ">> Epoch 397 finished \tANN training loss 0.286148\n",
      ">> Epoch 398 finished \tANN training loss 0.286112\n",
      ">> Epoch 399 finished \tANN training loss 0.286035\n",
      ">> Epoch 400 finished \tANN training loss 0.286001\n",
      ">> Epoch 401 finished \tANN training loss 0.285966\n",
      ">> Epoch 402 finished \tANN training loss 0.286018\n",
      ">> Epoch 403 finished \tANN training loss 0.285960\n",
      ">> Epoch 404 finished \tANN training loss 0.285981\n",
      ">> Epoch 405 finished \tANN training loss 0.286114\n",
      ">> Epoch 406 finished \tANN training loss 0.286033\n",
      ">> Epoch 407 finished \tANN training loss 0.285972\n",
      ">> Epoch 408 finished \tANN training loss 0.285979\n",
      ">> Epoch 409 finished \tANN training loss 0.285942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 410 finished \tANN training loss 0.285916\n",
      ">> Epoch 411 finished \tANN training loss 0.285883\n",
      ">> Epoch 412 finished \tANN training loss 0.285901\n",
      ">> Epoch 413 finished \tANN training loss 0.285963\n",
      ">> Epoch 414 finished \tANN training loss 0.285900\n",
      ">> Epoch 415 finished \tANN training loss 0.285860\n",
      ">> Epoch 416 finished \tANN training loss 0.285893\n",
      ">> Epoch 417 finished \tANN training loss 0.285906\n",
      ">> Epoch 418 finished \tANN training loss 0.285915\n",
      ">> Epoch 419 finished \tANN training loss 0.286043\n",
      ">> Epoch 420 finished \tANN training loss 0.286044\n",
      ">> Epoch 421 finished \tANN training loss 0.286004\n",
      ">> Epoch 422 finished \tANN training loss 0.285950\n",
      ">> Epoch 423 finished \tANN training loss 0.285913\n",
      ">> Epoch 424 finished \tANN training loss 0.285870\n",
      ">> Epoch 425 finished \tANN training loss 0.285883\n",
      ">> Epoch 426 finished \tANN training loss 0.285841\n",
      ">> Epoch 427 finished \tANN training loss 0.285794\n",
      ">> Epoch 428 finished \tANN training loss 0.285930\n",
      ">> Epoch 429 finished \tANN training loss 0.285853\n",
      ">> Epoch 430 finished \tANN training loss 0.285830\n",
      ">> Epoch 431 finished \tANN training loss 0.285794\n",
      ">> Epoch 432 finished \tANN training loss 0.285752\n",
      ">> Epoch 433 finished \tANN training loss 0.285751\n",
      ">> Epoch 434 finished \tANN training loss 0.285725\n",
      ">> Epoch 435 finished \tANN training loss 0.285793\n",
      ">> Epoch 436 finished \tANN training loss 0.285815\n",
      ">> Epoch 437 finished \tANN training loss 0.285757\n",
      ">> Epoch 438 finished \tANN training loss 0.285837\n",
      ">> Epoch 439 finished \tANN training loss 0.285811\n",
      ">> Epoch 440 finished \tANN training loss 0.285777\n",
      ">> Epoch 441 finished \tANN training loss 0.285734\n",
      ">> Epoch 442 finished \tANN training loss 0.285754\n",
      ">> Epoch 443 finished \tANN training loss 0.285632\n",
      ">> Epoch 444 finished \tANN training loss 0.285681\n",
      ">> Epoch 445 finished \tANN training loss 0.285650\n",
      ">> Epoch 446 finished \tANN training loss 0.285634\n",
      ">> Epoch 447 finished \tANN training loss 0.285649\n",
      ">> Epoch 448 finished \tANN training loss 0.285634\n",
      ">> Epoch 449 finished \tANN training loss 0.285678\n",
      ">> Epoch 450 finished \tANN training loss 0.285806\n",
      ">> Epoch 451 finished \tANN training loss 0.285991\n",
      ">> Epoch 452 finished \tANN training loss 0.285921\n",
      ">> Epoch 453 finished \tANN training loss 0.285870\n",
      ">> Epoch 454 finished \tANN training loss 0.285818\n",
      ">> Epoch 455 finished \tANN training loss 0.285927\n",
      ">> Epoch 456 finished \tANN training loss 0.285872\n",
      ">> Epoch 457 finished \tANN training loss 0.285869\n",
      ">> Epoch 458 finished \tANN training loss 0.285892\n",
      ">> Epoch 459 finished \tANN training loss 0.285954\n",
      ">> Epoch 460 finished \tANN training loss 0.286065\n",
      ">> Epoch 461 finished \tANN training loss 0.286045\n",
      ">> Epoch 462 finished \tANN training loss 0.286060\n",
      ">> Epoch 463 finished \tANN training loss 0.286169\n",
      ">> Epoch 464 finished \tANN training loss 0.286103\n",
      ">> Epoch 465 finished \tANN training loss 0.286044\n",
      ">> Epoch 466 finished \tANN training loss 0.286110\n",
      ">> Epoch 467 finished \tANN training loss 0.286056\n",
      ">> Epoch 468 finished \tANN training loss 0.286040\n",
      ">> Epoch 469 finished \tANN training loss 0.286188\n",
      ">> Epoch 470 finished \tANN training loss 0.286128\n",
      ">> Epoch 471 finished \tANN training loss 0.286257\n",
      ">> Epoch 472 finished \tANN training loss 0.286262\n",
      ">> Epoch 473 finished \tANN training loss 0.286379\n",
      ">> Epoch 474 finished \tANN training loss 0.286676\n",
      ">> Epoch 475 finished \tANN training loss 0.286565\n",
      ">> Epoch 476 finished \tANN training loss 0.286473\n",
      ">> Epoch 477 finished \tANN training loss 0.286674\n",
      ">> Epoch 478 finished \tANN training loss 0.286640\n",
      ">> Epoch 479 finished \tANN training loss 0.286535\n",
      ">> Epoch 480 finished \tANN training loss 0.286602\n",
      ">> Epoch 481 finished \tANN training loss 0.286598\n",
      ">> Epoch 482 finished \tANN training loss 0.286528\n",
      ">> Epoch 483 finished \tANN training loss 0.286415\n",
      ">> Epoch 484 finished \tANN training loss 0.286355\n",
      ">> Epoch 485 finished \tANN training loss 0.286265\n",
      ">> Epoch 486 finished \tANN training loss 0.286299\n",
      ">> Epoch 487 finished \tANN training loss 0.286338\n",
      ">> Epoch 488 finished \tANN training loss 0.286313\n",
      ">> Epoch 489 finished \tANN training loss 0.286773\n",
      ">> Epoch 490 finished \tANN training loss 0.286633\n",
      ">> Epoch 491 finished \tANN training loss 0.286554\n",
      ">> Epoch 492 finished \tANN training loss 0.286454\n",
      ">> Epoch 493 finished \tANN training loss 0.286375\n",
      ">> Epoch 494 finished \tANN training loss 0.286303\n",
      ">> Epoch 495 finished \tANN training loss 0.286254\n",
      ">> Epoch 496 finished \tANN training loss 0.286186\n",
      ">> Epoch 497 finished \tANN training loss 0.286212\n",
      ">> Epoch 498 finished \tANN training loss 0.286197\n",
      ">> Epoch 499 finished \tANN training loss 0.286602\n",
      ">> Epoch 500 finished \tANN training loss 0.286680\n",
      ">> Epoch 501 finished \tANN training loss 0.286510\n",
      ">> Epoch 502 finished \tANN training loss 0.286464\n",
      ">> Epoch 503 finished \tANN training loss 0.287094\n",
      ">> Epoch 504 finished \tANN training loss 0.287123\n",
      ">> Epoch 505 finished \tANN training loss 0.286989\n",
      ">> Epoch 506 finished \tANN training loss 0.286866\n",
      ">> Epoch 507 finished \tANN training loss 0.286745\n",
      ">> Epoch 508 finished \tANN training loss 0.286654\n",
      ">> Epoch 509 finished \tANN training loss 0.286546\n",
      ">> Epoch 510 finished \tANN training loss 0.286649\n",
      ">> Epoch 511 finished \tANN training loss 0.286578\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=0.01,\n",
       "                            idx_to_label_map={0: 0, 1: 1},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1}, learning_rate=0.01,\n",
       "                            n_iter_backprop=512, verbose=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_ext, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_ext_test)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "yhat_classes = yhat_probs# np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972973\n",
      "Precision: 0.968750\n",
      "Recall: 0.977273\n",
      "F1 score: 0.972243\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, yhat_classes,average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes, average='macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 0.944528\n",
      "[[21  1]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(Y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "#fprate, tprate, thresholds = roc_curve(Y_test, yhat_probs, average = 'macro')\n",
    "#print('ROC AUC: %f' % thresholds)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 5\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, yhat_classes, pos_label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[21  1]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_test,yhat_classes, labels= [0, 1])\n",
    "print('Confusion matrix : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        22\n",
      "           1       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.97      0.98      0.97        37\n",
      "weighted avg       0.97      0.97      0.97        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = classification_report(Y_test,yhat_classes, labels= [0, 1])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('./models/Binary Classifcation/Without IP/CNN-DBN/DBN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#model1 = tf.keras.models.load_model('./models/Without IP address/CNN -LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
