{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer,StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "## for explainer\n",
    "#from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#b = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "a = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "b = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "c =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "d =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "e =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#c = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week3.csv', low_memory=False , encoding='cp1252')\n",
    "#d = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#e =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "#g =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "#h =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10310733, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1795404, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape)\n",
    "#a.drop(a[a['attackType'] == '---'].index, axis = 0, inplace= True) \n",
    "b.drop(b[b['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "c.drop(c[c['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "d.drop(d[d['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "#e.drop(e[e['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_external = pd.concat([c,d,e], axis = 0)\n",
    "data_external.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to Increment attackID values\n",
    "data_external['attackID'] = data_external['attackID'].apply(lambda x: str(int(x) + 70) if x != '---' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(df):\n",
    "    if 'M' in df:\n",
    "        df = df.split('M')\n",
    "        df = df[0].strip()\n",
    "        df = float(df) * 1000000\n",
    "    elif 'B' in df:\n",
    "        df = df.split('B')\n",
    "        df = df[0].strip()\n",
    "        df =  float(df) * 1000000000\n",
    "    else: \n",
    "        df =float(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([a,b,data_external], axis = 0)\n",
    "data.reset_index(drop= True, inplace= True)\n",
    "data['Bytes'] = data['Bytes'].apply(lambda x: convert_bytes(x))\n",
    "data['attackType'] = data['attackType'].apply(lambda x:  'attack' if (x!= '---') else x )\n",
    "columns = ['Src Pt', 'Dst Pt','Tos','Flows','Packets', 'Bytes']\n",
    "for i in columns:\n",
    "    data[i] = pd.to_numeric(data[i]);\n",
    "del columns\n",
    "del a,b,c,d,e, data_external\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Hexadecimal value to Binary\n",
    "def hex_to_binary(hexdata):\n",
    "    scale = 16 ## equals to hexadecimal\n",
    "    num_of_bits = 9\n",
    "    return bin(int(hexdata, scale))[2:].zfill(num_of_bits);\n",
    "#Converts TCP flags to Binary\n",
    "def to_Binary(x):\n",
    "    l = 0\n",
    "    x = '...' + x\n",
    "    x = list(x)\n",
    "    for i in x:\n",
    "        if (i=='.'):\n",
    "            x[l]= '0'\n",
    "        else:\n",
    "            x[l] = '1'\n",
    "        l = l +1\n",
    "    return ''.join(x)\n",
    "#Converts the 'Flags' column to 9 indiviual columns (manual oneshot encoding)\n",
    "def flag_convert(df):  \n",
    "   # df['Flags'] = df['Flags'].apply(lambda x: (list(x)))\n",
    "   # temp = df['Flags'].apply(lambda x: toBinary(x))\n",
    "    hex_values = list(df[(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    flag_values = list(df[~(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    binary_values = {}\n",
    "    for i in hex_values:\n",
    "         binary_values[i] = (hex_to_binary(i))\n",
    "    for i in flag_values:\n",
    "         binary_values[i] = (to_Binary(i))\n",
    "    temp = df['Flags'].replace(binary_values)\n",
    "#temp = temp.apply(lambda x: pd.Series(x)) \n",
    "    temp = pd.DataFrame(temp.apply(list).tolist())\n",
    "#temp = pd.DataFrame(temp)\n",
    "#a = a.iloc[: , 1:]\n",
    "   # print(temp.head())\n",
    "    temp.columns = ['N','C','E','U' ,'A','P','R','S','F']\n",
    "    for i in temp.columns:\n",
    "        temp[i] = pd.to_numeric(temp[i]);\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a IP_pairs \n",
    "def make_pair(df):\n",
    "    ip_pair = df['Src IP Addr'] +'/' +df['Dst IP Addr']\n",
    "    source_ip = df['Src IP Addr'].unique().tolist()\n",
    "    destination_ip = df['Dst IP Addr'].unique().tolist()\n",
    "   # df = df.drop(columns = ['Src IP Addr', 'Dst IP Addr'])\n",
    "    df.insert(1, ' IP Pair', ip_pair)\n",
    "    return df\n",
    "\n",
    "def check_inverse(df):\n",
    "    list_pairs = df[' IP Pair'].unique()\n",
    "    tuple_pair = []\n",
    "    for i in list_pairs:\n",
    "        tuple_pair.append(tuple((i.split('/'))))\n",
    "    dic_store = {}\n",
    "    for i in tuple_pair:\n",
    "        if (i  not in dic_store.keys()) and (i[::-1] not in dic_store.keys()):\n",
    "            dic_store[i] = i[0] + '/' +i[1]\n",
    "    print(len(dic_store.keys()))\n",
    "    dic_final = {}\n",
    "    for i in dic_store.keys():\n",
    "        dic_final[i[0] + '/' +i[1]] = dic_store[i]\n",
    "        dic_final[i[1] + '/' +i[0]] = dic_store[i]\n",
    "    df[' IP Pair'] = df[' IP Pair'].map(dic_final)               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_IP(df):\n",
    "    columns = ['sourceIP_feature 1', 'sourceIP_feature 2', 'sourceIP_feature 3', 'sourceIP_feature 4', 'destIP_feature 1',\n",
    "              'destIP_feature 2', 'destIP_feature 3', 'destIP_feature 4']\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    columns = data.select_dtypes(include=numerics).columns\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    #df.astype({'attackType': 'str'})\n",
    "    df['attackType'] = label_encoder.fit_transform(df['attackType'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df['Proto'] = label_encoder.fit_transform(df['Proto'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    onehot_encoder1 = OneHotEncoder()\n",
    "    onehot_encoder1.fit(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = onehot_encoder1.transform(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = pd.DataFrame.sparse.from_spmatrix(proto)\n",
    "    proto.astype('int32')\n",
    "    proto.columns = label_encoder.classes_\n",
    "   # print(proto.head(1))\n",
    "    df = pd.concat([df, proto], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(columns = ['Date first seen', ' IP Pair', 'Flows', 'class', 'attackID','Flags',\n",
    "                              'attackDescription', 'Src IP Addr', 'Dst IP Addr','Proto'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplit IP address into features, 7 features\n",
    "def split_to_net(IP_address):\n",
    "    IP_list = IP_address.split(\".\")\n",
    "    needed_len = 7\n",
    "    needed_len = needed_len - len(IP_list)\n",
    "    for i in range(0,needed_len,1):\n",
    "        IP_list.append('0')\n",
    "    return IP_list\n",
    "#replace unknown IP address, and convert to columns\n",
    "def IP_split(df): \n",
    "    replace = {\"ATTACKER1\":\"0.0.0.0\",\n",
    "           \"ATTACKER2\":\"0.0.0.0\",\n",
    "           \"ATTACKER3\":\"0.0.0.0\",\n",
    "           \"EXT_SERVER\": \"0.0.0.0.1\",\n",
    "          \"OPENSTACK_NET\": \"0.0.0.0.0.1\",\n",
    "          \"DNS\": \"0.0.0.0.0.0.1\"}\n",
    "    df = df.replace({\"Src IP Addr\": replace, \"Dst IP Addr\": replace}, value=None)\n",
    "    temp_source = df[\"Src IP Addr\"].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "    temp_des = df['Dst IP Addr'].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "   # sourceIP = list(df[\"Src IP Addr\"].unique())\n",
    "   # destIP = list(df[\"Dst IP Addr\"].unique())\n",
    "   # sourceIP_values = {}\n",
    "   # desIP_values = {}\n",
    "   # for i in sourceIP:\n",
    "   #      sourceIP_values[i] = (split_to_net(i))\n",
    "   # for i in destIP:\n",
    "   #      desIP_values[i] = (split_to_net(i))\n",
    "    #print(sourceIP_values)\n",
    "   # print(desIP_values)\n",
    "#for Source IP\n",
    "    temp_source = temp_source.apply(lambda x: split_to_net(x) )\n",
    "    temp_source = pd.DataFrame(temp_source.apply(list).tolist())\n",
    "    temp_source.columns = ['sourceIP_feature 1','sourceIP_feature 2','sourceIP_feature 3','sourceIP_feature 4' ,\n",
    "                    'sourceEXT_SERVER','sourceOPENSTACK_NET','sourceDNS']\n",
    "    for i in temp_source.columns:\n",
    "        temp_source[i] = pd.to_numeric(temp_source[i]);\n",
    "    temp_source = temp_source.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_source], axis = 1)\n",
    "    #for Destination IP\n",
    "    temp_des = temp_des.apply(lambda x: split_to_net(x) )\n",
    "    temp_des = pd.DataFrame(temp_des.apply(list).tolist())\n",
    "    temp_des.columns = ['destIP_feature 1','destIP_feature 2','destIP_feature 3','destIP_feature 4' ,\n",
    "                    'destEXT_SERVER','destOPENSTACK_NET','destDNS']\n",
    "   # for i in temp_des.columns:\n",
    "       # temp_des[i] = pd.to_numeric(temp_des[i]);\n",
    "    temp_des = temp_des.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_des], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59362\n"
     ]
    }
   ],
   "source": [
    "data = make_pair(data)\n",
    "data = check_inverse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = IP_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Duration', 'Src Pt', 'Dst Pt', 'Packets', 'Bytes', 'Flows', 'Tos'], dtype='object')\n",
      "['---', 'attack']\n",
      "[0, 1]\n",
      "['GRE  ', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = normalize(data)\n",
    "data =  one_shot(data) \n",
    "#data = normalize_IP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def unix_time(df):\n",
    "  #  df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x + ':00' if len(x) != 19 else x)\n",
    "   # df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x[0 : 5 : ] + x[7 : :] if len(x) != 19 else x[0 : 7 : ] + x[9 : :])\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: x.timestamp()*1000)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_profile(grouped):\n",
    "    grouped['---'] = unix_time(grouped['---'])\n",
    "    start_time = int(grouped['---'].head(1)['Date first seen'].values[0])\n",
    "    end_time = int(grouped['---'].tail(1)['Date first seen'].values[0])\n",
    "#date_bins = pd.IntervalIndex.from_tuples(\n",
    "#        [(i, i+3600000) for i in range(start_time, end_time, 3600000)],\n",
    "#        closed=\"left\")\n",
    "#date_labels = [f\"{i}\" for i in range(1, len(date_bins)+1, 1)]\n",
    "    normal_data = dict(tuple( grouped['---'].groupby( pd.cut(\n",
    "            grouped['---']['Date first seen'],\n",
    "               np.arange(start_time, end_time, 3*3600000)))))\n",
    "    del grouped['---']\n",
    "    num = []\n",
    "    for i in grouped_data.keys():\n",
    "          num.append(len(grouped_data[i]))\n",
    "    print(min(num))\n",
    "    num = max(num)\n",
    "    print(num)\n",
    "    print(len(grouped.keys()))\n",
    "    grouped = {**grouped, **normal_data}\n",
    "    print(len(grouped.keys()))\n",
    "    return grouped, num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data= dict(tuple(data.groupby(['attackID'])))\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---: 7195669 : 0\n",
      "Attack ID: 1; Lenght of Attack: 7657; Attack Type: 1\n",
      "Attack ID: 10; Lenght of Attack: 311; Attack Type: 1\n",
      "Attack ID: 11; Lenght of Attack: 17401; Attack Type: 1\n",
      "Attack ID: 12; Lenght of Attack: 11526; Attack Type: 1\n",
      "Attack ID: 13; Lenght of Attack: 513; Attack Type: 1\n",
      "Attack ID: 14; Lenght of Attack: 13807; Attack Type: 1\n",
      "Attack ID: 15; Lenght of Attack: 64; Attack Type: 1\n",
      "Attack ID: 16; Lenght of Attack: 261003; Attack Type: 1\n",
      "Attack ID: 17; Lenght of Attack: 13338; Attack Type: 1\n",
      "Attack ID: 18; Lenght of Attack: 295302; Attack Type: 1\n",
      "Attack ID: 19; Lenght of Attack: 11672; Attack Type: 1\n",
      "Attack ID: 2; Lenght of Attack: 1927; Attack Type: 1\n",
      "Attack ID: 20; Lenght of Attack: 11748; Attack Type: 1\n",
      "Attack ID: 21; Lenght of Attack: 5113; Attack Type: 1\n",
      "Attack ID: 22; Lenght of Attack: 295; Attack Type: 1\n",
      "Attack ID: 23; Lenght of Attack: 72788; Attack Type: 1\n",
      "Attack ID: 24; Lenght of Attack: 466; Attack Type: 1\n",
      "Attack ID: 25; Lenght of Attack: 201; Attack Type: 1\n",
      "Attack ID: 26; Lenght of Attack: 74471; Attack Type: 1\n",
      "Attack ID: 27; Lenght of Attack: 680; Attack Type: 1\n",
      "Attack ID: 28; Lenght of Attack: 36306; Attack Type: 1\n",
      "Attack ID: 29; Lenght of Attack: 19732; Attack Type: 1\n",
      "Attack ID: 3; Lenght of Attack: 37118; Attack Type: 1\n",
      "Attack ID: 30; Lenght of Attack: 46; Attack Type: 1\n",
      "Attack ID: 31; Lenght of Attack: 144845; Attack Type: 1\n",
      "Attack ID: 32; Lenght of Attack: 335; Attack Type: 1\n",
      "Attack ID: 33; Lenght of Attack: 307; Attack Type: 1\n",
      "Attack ID: 34; Lenght of Attack: 12909; Attack Type: 1\n",
      "Attack ID: 35; Lenght of Attack: 263; Attack Type: 1\n",
      "Attack ID: 36; Lenght of Attack: 494; Attack Type: 1\n",
      "Attack ID: 37; Lenght of Attack: 26114; Attack Type: 1\n",
      "Attack ID: 38; Lenght of Attack: 267; Attack Type: 1\n",
      "Attack ID: 39; Lenght of Attack: 364; Attack Type: 1\n",
      "Attack ID: 4; Lenght of Attack: 72063; Attack Type: 1\n",
      "Attack ID: 40; Lenght of Attack: 11609; Attack Type: 1\n",
      "Attack ID: 41; Lenght of Attack: 379; Attack Type: 1\n",
      "Attack ID: 42; Lenght of Attack: 184040; Attack Type: 1\n",
      "Attack ID: 43; Lenght of Attack: 2143; Attack Type: 1\n",
      "Attack ID: 44; Lenght of Attack: 261169; Attack Type: 1\n",
      "Attack ID: 45; Lenght of Attack: 224960; Attack Type: 1\n",
      "Attack ID: 46; Lenght of Attack: 111720; Attack Type: 1\n",
      "Attack ID: 47; Lenght of Attack: 13420; Attack Type: 1\n",
      "Attack ID: 48; Lenght of Attack: 13600; Attack Type: 1\n",
      "Attack ID: 49; Lenght of Attack: 17629; Attack Type: 1\n",
      "Attack ID: 5; Lenght of Attack: 4948; Attack Type: 1\n",
      "Attack ID: 50; Lenght of Attack: 4589; Attack Type: 1\n",
      "Attack ID: 51; Lenght of Attack: 11968; Attack Type: 1\n",
      "Attack ID: 52; Lenght of Attack: 607; Attack Type: 1\n",
      "Attack ID: 53; Lenght of Attack: 516299; Attack Type: 1\n",
      "Attack ID: 54; Lenght of Attack: 183; Attack Type: 1\n",
      "Attack ID: 55; Lenght of Attack: 757; Attack Type: 1\n",
      "Attack ID: 56; Lenght of Attack: 427; Attack Type: 1\n",
      "Attack ID: 57; Lenght of Attack: 522; Attack Type: 1\n",
      "Attack ID: 58; Lenght of Attack: 510; Attack Type: 1\n",
      "Attack ID: 59; Lenght of Attack: 110484; Attack Type: 1\n",
      "Attack ID: 6; Lenght of Attack: 37134; Attack Type: 1\n",
      "Attack ID: 60; Lenght of Attack: 333627; Attack Type: 1\n",
      "Attack ID: 61; Lenght of Attack: 705; Attack Type: 1\n",
      "Attack ID: 62; Lenght of Attack: 574; Attack Type: 1\n",
      "Attack ID: 63; Lenght of Attack: 148641; Attack Type: 1\n",
      "Attack ID: 64; Lenght of Attack: 480; Attack Type: 1\n",
      "Attack ID: 65; Lenght of Attack: 373; Attack Type: 1\n",
      "Attack ID: 66; Lenght of Attack: 359; Attack Type: 1\n",
      "Attack ID: 67; Lenght of Attack: 13426; Attack Type: 1\n",
      "Attack ID: 68; Lenght of Attack: 5632; Attack Type: 1\n",
      "Attack ID: 69; Lenght of Attack: 360; Attack Type: 1\n",
      "Attack ID: 7; Lenght of Attack: 9586; Attack Type: 1\n",
      "Attack ID: 70; Lenght of Attack: 240; Attack Type: 1\n",
      "Attack ID: 71; Lenght of Attack: 2008; Attack Type: 1\n",
      "Attack ID: 72; Lenght of Attack: 2002; Attack Type: 1\n",
      "Attack ID: 73; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 74; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 75; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 76; Lenght of Attack: 168; Attack Type: 1\n",
      "Attack ID: 77; Lenght of Attack: 6410; Attack Type: 1\n",
      "Attack ID: 78; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 79; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 8; Lenght of Attack: 4424; Attack Type: 1\n",
      "Attack ID: 80; Lenght of Attack: 1991; Attack Type: 1\n",
      "Attack ID: 81; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 82; Lenght of Attack: 1370; Attack Type: 1\n",
      "Attack ID: 83; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 84; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 85; Lenght of Attack: 1984; Attack Type: 1\n",
      "Attack ID: 86; Lenght of Attack: 2002; Attack Type: 1\n",
      "Attack ID: 87; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 88; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 89; Lenght of Attack: 952; Attack Type: 1\n",
      "Attack ID: 9; Lenght of Attack: 37057; Attack Type: 1\n",
      "Attack ID: 90; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 91; Lenght of Attack: 40; Attack Type: 1\n",
      "Attack ID: 92; Lenght of Attack: 40; Attack Type: 1\n",
      "dos : 0\n",
      "pingScan: 0\n",
      "portScan : 0\n"
     ]
    }
   ],
   "source": [
    "no_1 = []\n",
    "no_2 = []\n",
    "no_3 = []\n",
    "no_4 = []\n",
    "for i in grouped_data.keys():\n",
    "   \n",
    "    if grouped_data[i]['attackType'].unique()[0] == 0:\n",
    "        print(f\"{i}: {len(grouped_data[i])} : {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 1:\n",
    "              no_1.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 2:\n",
    "              no_2.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 3:\n",
    "              no_3.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 4:\n",
    "              no_4.append(i)\n",
    "for i in no_1:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"dos : {len(no_2)}\")\n",
    "for i in no_2:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"pingScan: {len(no_3)}\")\n",
    "for i in no_3:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"portScan : {len(no_4)}\")\n",
    "for i in no_4:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del no_1\n",
    "del no_2\n",
    "del no_3\n",
    "del no_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_largeInstances(dic, length):\n",
    "    remove_ID = []\n",
    "    for i in dic.keys():\n",
    "        if (i != '---'):\n",
    "            if(len(dic[i]) >= length):\n",
    "                remove_ID.append(i)\n",
    "    print(len(remove_ID))\n",
    "    removed_attacks = {}\n",
    "    for i in remove_ID:\n",
    "        removed_attacks[i] = dic[i]\n",
    "        del dic[i]\n",
    "    return dic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "grouped_data = del_largeInstances(grouped_data, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "19732\n",
      "73\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "#grouped_data, num = normal_profile(grouped_data)\n",
    "grouped_data1= {}\n",
    "for i in grouped_data.keys():\n",
    "    grouped_data[i] = flag_convert(grouped_data[i])\n",
    "   # grouped_data[i] =  drop_columns(grouped_data[i])\n",
    "grouped_data, num = normal_profile(grouped_data)\n",
    "for i in grouped_data.keys():\n",
    "   # grouped_data[i] = flag_convert(grouped_data[i])\n",
    "    grouped_data[i] =  drop_columns(grouped_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : False\n",
      "10 : False\n",
      "11 : False\n",
      "12 : False\n",
      "13 : False\n",
      "14 : False\n",
      "15 : False\n",
      "17 : False\n",
      "19 : False\n",
      "2 : False\n",
      "20 : False\n",
      "21 : False\n",
      "22 : False\n",
      "24 : False\n",
      "25 : False\n",
      "27 : False\n",
      "29 : False\n",
      "30 : False\n",
      "32 : False\n",
      "33 : False\n",
      "34 : False\n",
      "35 : False\n",
      "36 : False\n",
      "38 : False\n",
      "39 : False\n",
      "40 : False\n",
      "41 : False\n",
      "43 : False\n",
      "47 : False\n",
      "48 : False\n",
      "49 : False\n",
      "5 : False\n",
      "50 : False\n",
      "51 : False\n",
      "52 : False\n",
      "54 : False\n",
      "55 : False\n",
      "56 : False\n",
      "57 : False\n",
      "58 : False\n",
      "61 : False\n",
      "62 : False\n",
      "64 : False\n",
      "65 : False\n",
      "66 : False\n",
      "67 : False\n",
      "68 : False\n",
      "69 : False\n",
      "7 : False\n",
      "70 : False\n",
      "71 : False\n",
      "72 : False\n",
      "73 : False\n",
      "74 : False\n",
      "75 : False\n",
      "76 : False\n",
      "77 : False\n",
      "78 : False\n",
      "79 : False\n",
      "8 : False\n",
      "80 : False\n",
      "81 : False\n",
      "82 : False\n",
      "83 : False\n",
      "84 : False\n",
      "85 : False\n",
      "86 : False\n",
      "87 : False\n",
      "88 : False\n",
      "89 : False\n",
      "90 : False\n",
      "91 : False\n",
      "92 : False\n",
      "(1489536076632, 1489546876632] : False\n",
      "(1489546876632, 1489557676632] : False\n",
      "(1489557676632, 1489568476632] : False\n",
      "(1489568476632, 1489579276632] : False\n",
      "(1489579276632, 1489590076632] : False\n",
      "(1489590076632, 1489600876632] : False\n",
      "(1489600876632, 1489611676632] : False\n",
      "(1489611676632, 1489622476632] : False\n",
      "(1489622476632, 1489633276632] : False\n",
      "(1489633276632, 1489644076632] : False\n",
      "(1489644076632, 1489654876632] : False\n",
      "(1489654876632, 1489665676632] : False\n",
      "(1489665676632, 1489676476632] : False\n",
      "(1489676476632, 1489687276632] : False\n",
      "(1489687276632, 1489698076632] : False\n",
      "(1489698076632, 1489708876632] : False\n",
      "(1489708876632, 1489719676632] : False\n",
      "(1489719676632, 1489730476632] : False\n",
      "(1489730476632, 1489741276632] : False\n",
      "(1489741276632, 1489752076632] : False\n",
      "(1489752076632, 1489762876632] : False\n",
      "(1489762876632, 1489773676632] : False\n",
      "(1489773676632, 1489784476632] : False\n",
      "(1489784476632, 1489795276632] : False\n",
      "(1489795276632, 1489806076632] : False\n",
      "(1489806076632, 1489816876632] : False\n",
      "(1489816876632, 1489827676632] : False\n",
      "(1489827676632, 1489838476632] : False\n",
      "(1489838476632, 1489849276632] : False\n",
      "(1489849276632, 1489860076632] : False\n",
      "(1489860076632, 1489870876632] : False\n",
      "(1489870876632, 1489881676632] : False\n",
      "(1489881676632, 1489892476632] : False\n",
      "(1489892476632, 1489903276632] : False\n",
      "(1489903276632, 1489914076632] : False\n",
      "(1489914076632, 1489924876632] : False\n",
      "(1489924876632, 1489935676632] : False\n",
      "(1489935676632, 1489946476632] : False\n",
      "(1489946476632, 1489957276632] : False\n",
      "(1489957276632, 1489968076632] : False\n",
      "(1489968076632, 1489978876632] : False\n",
      "(1489978876632, 1489989676632] : False\n",
      "(1489989676632, 1490000476632] : False\n",
      "(1490000476632, 1490011276632] : False\n",
      "(1490011276632, 1490022076632] : False\n",
      "(1490022076632, 1490032876632] : False\n",
      "(1490032876632, 1490043676632] : False\n",
      "(1490043676632, 1490054476632] : False\n",
      "(1490054476632, 1490065276632] : False\n",
      "(1490065276632, 1490076076632] : False\n",
      "(1490076076632, 1490086876632] : False\n",
      "(1490086876632, 1490097676632] : False\n",
      "(1490097676632, 1490108476632] : False\n",
      "(1490108476632, 1490119276632] : False\n",
      "(1490119276632, 1490130076632] : False\n",
      "(1490130076632, 1490140876632] : False\n",
      "(1490140876632, 1490151676632] : False\n",
      "(1490151676632, 1490162476632] : False\n",
      "(1490162476632, 1490173276632] : False\n",
      "(1490173276632, 1490184076632] : False\n",
      "(1490184076632, 1490194876632] : False\n",
      "(1490194876632, 1490205676632] : False\n",
      "(1490205676632, 1490216476632] : False\n",
      "(1490216476632, 1490227276632] : False\n",
      "(1490227276632, 1490238076632] : False\n",
      "(1490238076632, 1490248876632] : False\n",
      "(1490248876632, 1490259676632] : False\n",
      "(1490259676632, 1490270476632] : False\n",
      "(1490270476632, 1490281276632] : False\n",
      "(1490281276632, 1490292076632] : False\n",
      "(1490292076632, 1490302876632] : False\n",
      "(1490302876632, 1490313676632] : False\n",
      "(1490313676632, 1490324476632] : False\n",
      "(1490324476632, 1490335276632] : False\n",
      "(1490335276632, 1490346076632] : False\n",
      "(1490346076632, 1490356876632] : False\n",
      "(1490356876632, 1490367676632] : False\n",
      "(1490367676632, 1490378476632] : False\n",
      "(1490378476632, 1490389276632] : False\n",
      "(1490389276632, 1490400076632] : False\n",
      "(1490400076632, 1490410876632] : False\n",
      "(1490410876632, 1490421676632] : False\n",
      "(1490421676632, 1490432476632] : False\n",
      "(1490432476632, 1490443276632] : False\n",
      "(1490443276632, 1490454076632] : False\n",
      "(1490454076632, 1490464876632] : False\n",
      "(1490464876632, 1490475676632] : False\n",
      "(1490475676632, 1490486476632] : False\n",
      "(1490486476632, 1490497276632] : False\n",
      "(1490497276632, 1490508076632] : False\n",
      "(1490508076632, 1490518876632] : False\n",
      "(1490518876632, 1490529676632] : False\n",
      "(1490529676632, 1490540476632] : False\n",
      "(1490540476632, 1490551276632] : False\n",
      "(1490551276632, 1490562076632] : False\n",
      "(1490562076632, 1490572876632] : False\n",
      "(1490572876632, 1490583676632] : False\n",
      "(1490583676632, 1490594476632] : False\n",
      "(1490594476632, 1490605276632] : False\n",
      "(1490605276632, 1490616076632] : False\n",
      "(1490616076632, 1490626876632] : False\n",
      "(1490626876632, 1490637676632] : False\n",
      "(1490637676632, 1490648476632] : False\n",
      "(1490648476632, 1490659276632] : False\n",
      "(1490659276632, 1490670076632] : False\n",
      "(1490670076632, 1490680876632] : False\n",
      "(1490680876632, 1490691676632] : False\n",
      "(1490691676632, 1490702476632] : False\n",
      "(1490702476632, 1490713276632] : False\n",
      "(1490713276632, 1490724076632] : False\n",
      "(1490724076632, 1490734876632] : False\n",
      "(1490734876632, 1490745676632] : False\n",
      "(1490745676632, 1490756476632] : False\n",
      "(1490756476632, 1490767276632] : False\n",
      "(1490767276632, 1490778076632] : False\n",
      "(1490778076632, 1490788876632] : False\n",
      "(1490788876632, 1490799676632] : False\n",
      "(1490799676632, 1490810476632] : False\n",
      "(1490810476632, 1490821276632] : False\n",
      "(1490821276632, 1490832076632] : False\n",
      "(1490832076632, 1490842876632] : False\n",
      "(1490842876632, 1490853676632] : False\n",
      "(1490853676632, 1490864476632] : False\n",
      "(1490864476632, 1490875276632] : False\n",
      "(1490875276632, 1490886076632] : False\n",
      "(1490886076632, 1490896876632] : False\n",
      "(1490896876632, 1490907676632] : False\n",
      "(1490907676632, 1490918476632] : False\n",
      "(1490918476632, 1490929276632] : False\n",
      "(1490929276632, 1490940076632] : False\n",
      "(1490940076632, 1490950876632] : False\n",
      "(1490950876632, 1490961676632] : False\n",
      "(1490961676632, 1490972476632] : False\n",
      "(1490972476632, 1490983276632] : False\n",
      "(1490983276632, 1490994076632] : False\n",
      "(1490994076632, 1491004876632] : False\n",
      "(1491004876632, 1491015676632] : False\n",
      "(1491015676632, 1491026476632] : False\n",
      "(1491026476632, 1491037276632] : False\n",
      "(1491037276632, 1491048076632] : False\n",
      "(1491048076632, 1491058876632] : False\n",
      "(1491058876632, 1491069676632] : False\n",
      "(1491069676632, 1491080476632] : False\n",
      "(1491080476632, 1491091276632] : False\n",
      "(1491091276632, 1491102076632] : False\n",
      "(1491102076632, 1491112876632] : False\n",
      "(1491112876632, 1491123676632] : False\n",
      "(1491123676632, 1491134476632] : False\n",
      "(1491134476632, 1491145276632] : False\n",
      "(1491145276632, 1491156076632] : False\n",
      "(1491156076632, 1491166876632] : False\n",
      "(1491166876632, 1491177676632] : False\n",
      "(1491177676632, 1491188476632] : False\n",
      "(1491188476632, 1491199276632] : False\n",
      "(1491199276632, 1491210076632] : False\n",
      "(1491210076632, 1491220876632] : False\n",
      "(1491220876632, 1491231676632] : False\n",
      "(1491231676632, 1491242476632] : False\n",
      "(1491242476632, 1491253276632] : False\n",
      "(1491253276632, 1491264076632] : False\n",
      "(1491264076632, 1491274876632] : False\n",
      "(1491274876632, 1491285676632] : False\n",
      "(1491285676632, 1491296476632] : False\n",
      "(1491296476632, 1491307276632] : False\n",
      "(1491307276632, 1491318076632] : False\n",
      "(1491318076632, 1491328876632] : False\n",
      "(1491328876632, 1491339676632] : False\n",
      "(1491339676632, 1491350476632] : False\n",
      "(1491350476632, 1491361276632] : False\n",
      "(1491361276632, 1491372076632] : False\n",
      "(1491372076632, 1491382876632] : False\n",
      "(1491382876632, 1491393676632] : False\n",
      "(1491393676632, 1491404476632] : False\n",
      "(1491404476632, 1491415276632] : False\n",
      "(1491415276632, 1491426076632] : False\n",
      "(1491426076632, 1491436876632] : False\n",
      "(1491436876632, 1491447676632] : False\n",
      "(1491447676632, 1491458476632] : False\n",
      "(1491458476632, 1491469276632] : False\n",
      "(1491469276632, 1491480076632] : False\n",
      "(1491480076632, 1491490876632] : False\n",
      "(1491490876632, 1491501676632] : False\n",
      "(1491501676632, 1491512476632] : False\n",
      "(1491512476632, 1491523276632] : False\n",
      "(1491523276632, 1491534076632] : False\n",
      "(1491534076632, 1491544876632] : False\n",
      "(1491544876632, 1491555676632] : False\n",
      "(1491555676632, 1491566476632] : False\n",
      "(1491566476632, 1491577276632] : False\n",
      "(1491577276632, 1491588076632] : False\n",
      "(1491588076632, 1491598876632] : False\n",
      "(1491598876632, 1491609676632] : False\n",
      "(1491609676632, 1491620476632] : False\n",
      "(1491620476632, 1491631276632] : False\n",
      "(1491631276632, 1491642076632] : False\n",
      "(1491642076632, 1491652876632] : False\n",
      "(1491652876632, 1491663676632] : False\n",
      "(1491663676632, 1491674476632] : False\n",
      "(1491674476632, 1491685276632] : False\n",
      "(1491685276632, 1491696076632] : False\n",
      "(1491696076632, 1491706876632] : False\n",
      "(1491706876632, 1491717676632] : False\n",
      "(1491717676632, 1491728476632] : False\n",
      "(1491728476632, 1491739276632] : False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1491739276632, 1491750076632] : False\n",
      "(1491750076632, 1491760876632] : False\n",
      "(1491760876632, 1491771676632] : False\n",
      "(1491771676632, 1491782476632] : False\n",
      "(1491782476632, 1491793276632] : False\n",
      "(1491793276632, 1491804076632] : False\n",
      "(1491804076632, 1491814876632] : False\n",
      "(1491814876632, 1491825676632] : False\n",
      "(1491825676632, 1491836476632] : False\n",
      "(1491836476632, 1491847276632] : False\n",
      "(1491847276632, 1491858076632] : False\n",
      "(1491858076632, 1491868876632] : False\n",
      "(1491868876632, 1491879676632] : False\n",
      "(1491879676632, 1491890476632] : False\n",
      "(1491890476632, 1491901276632] : False\n",
      "(1491901276632, 1491912076632] : False\n",
      "(1491912076632, 1491922876632] : False\n",
      "(1491922876632, 1491933676632] : False\n",
      "(1491933676632, 1491944476632] : False\n",
      "(1491944476632, 1491955276632] : False\n",
      "(1491955276632, 1491966076632] : False\n",
      "(1491966076632, 1491976876632] : False\n",
      "(1491976876632, 1491987676632] : False\n",
      "(1491987676632, 1491998476632] : False\n",
      "(1491998476632, 1492009276632] : False\n",
      "(1492009276632, 1492020076632] : False\n",
      "(1492020076632, 1492030876632] : False\n",
      "(1492030876632, 1492041676632] : False\n",
      "(1492041676632, 1492052476632] : False\n",
      "(1492052476632, 1492063276632] : False\n",
      "(1492063276632, 1492074076632] : False\n",
      "(1492074076632, 1492084876632] : False\n",
      "(1492084876632, 1492095676632] : False\n",
      "(1492095676632, 1492106476632] : False\n",
      "(1492106476632, 1492117276632] : False\n",
      "(1492117276632, 1492128076632] : False\n",
      "(1492128076632, 1492138876632] : False\n",
      "(1492138876632, 1492149676632] : False\n",
      "(1492149676632, 1492160476632] : False\n",
      "(1492160476632, 1492171276632] : False\n",
      "(1492171276632, 1492182076632] : False\n",
      "(1492182076632, 1492192876632] : False\n",
      "(1492192876632, 1492203676632] : False\n",
      "(1492203676632, 1492214476632] : False\n",
      "(1492214476632, 1492225276632] : False\n",
      "(1492225276632, 1492236076632] : False\n",
      "(1492236076632, 1492246876632] : False\n",
      "(1492246876632, 1492257676632] : False\n",
      "(1492257676632, 1492268476632] : False\n",
      "(1492268476632, 1492279276632] : False\n",
      "(1492279276632, 1492290076632] : False\n",
      "(1492290076632, 1492300876632] : False\n",
      "(1492300876632, 1492311676632] : False\n",
      "(1492311676632, 1492322476632] : False\n",
      "(1492322476632, 1492333276632] : False\n",
      "(1492333276632, 1492344076632] : False\n",
      "(1492344076632, 1492354876632] : False\n",
      "(1492354876632, 1492365676632] : False\n",
      "(1492365676632, 1492376476632] : False\n",
      "(1492376476632, 1492387276632] : False\n",
      "(1492387276632, 1492398076632] : False\n",
      "(1492398076632, 1492408876632] : False\n",
      "(1492408876632, 1492419676632] : False\n",
      "(1492419676632, 1492430476632] : False\n",
      "(1492430476632, 1492441276632] : False\n",
      "(1492441276632, 1492452076632] : False\n",
      "(1492452076632, 1492462876632] : False\n",
      "(1492462876632, 1492473676632] : False\n",
      "(1492473676632, 1492484476632] : False\n",
      "(1492484476632, 1492495276632] : False\n",
      "(1492495276632, 1492506076632] : False\n",
      "(1492506076632, 1492516876632] : False\n",
      "(1492516876632, 1492527676632] : False\n"
     ]
    }
   ],
   "source": [
    "for i in grouped_data.keys():\n",
    "    #if (grouped_data[i].hasnull())\n",
    "    print(f'{i} : {grouped_data[i].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances which are empty: 167\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in grouped_data.keys():\n",
    "    if ( len(grouped_data[i]) == 0):\n",
    "        counter = counter +1;\n",
    "print(f\"Number of Instances which are empty: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 100 == 0 else x + 100 - x % 100\n",
    "#Convert to 3D arrays, input dict\n",
    "def make_array(dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    zero_arrays = []\n",
    "    for i in dic.keys():\n",
    "        if ( len(dic[i]) == 0):\n",
    "            zero_arrays.append(i);\n",
    "    for i in zero_arrays:\n",
    "        del dic[i]\n",
    "    for i in dic.keys():\n",
    "        x.append(np.array(dic[i].drop(['attackType'],axis = 1)).astype(np.float32))\n",
    "       # print(f'{i}')\n",
    "        y.append(dic[i]['attackType'].values[0])\n",
    "    print(len(y))\n",
    "    o = []\n",
    "    features = len(x[1][1])\n",
    "    #for i in x:\n",
    "     #   o.append(len(i))\n",
    "   # print(min(o))\n",
    "    o = num\n",
    "    o = roundup(o)\n",
    "    print(o)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        l = len(i)\n",
    "        i = list(i)\n",
    "        if(o > l):\n",
    "            l = o-l\n",
    "            for j in range(0, l, 1):\n",
    "                i.append([0] * features)\n",
    "        elif (o<l):\n",
    "            l = l-o\n",
    "            i = i[:-l]\n",
    "        #i = [k = np.array([k]) for l in i for k in l] # Makes array elements an array \n",
    "        x[index] = np.array(i).astype(np.float32)\n",
    "        index = index + 1\n",
    "    #x = [[i] for i in x]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "X,Y = make_array(grouped_data)\n",
    "del grouped_data\n",
    "gc.collect()\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 110, 1: 73}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_4D(arr):\n",
    "    x = []\n",
    "    for i in range(0, len(arr),1):\n",
    "        temp = []\n",
    "        for j in range(0,len(arr[i]),1):\n",
    "             temp.append([np.array([k]) for k in arr[i][j]])\n",
    "        x.append(np.array(temp).astype(np.float32))\n",
    "    return np.array(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = make_4D(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2, random_state=0,  stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X,Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 88, 1: 58}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 22, 1: 15}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in X_train:\n",
    "    print(f'{np.isnan(i).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 146 \n",
      " X:19800 \n",
      " Y:20 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "nsamples,nx, ny = X_train.shape\n",
    "print(f\"samples: {nsamples} \\n X:{nx} \\n Y:{ny} \\n \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19800"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((37,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128,64, 64],\n",
    "                                         learning_rate_rbm=0.00001,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=12,\n",
    "                                         n_iter_backprop=512,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del classifier\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Desktop\\Project dISSERATAION\\dbn\\tensorflow\\models.py:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 24709.646484\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 24708.871094\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 24707.687500\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 24705.380859\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24701.072266\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 24692.123047\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 24673.673828\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 24638.171875\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 24579.462891\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24490.673828\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 24366.794922\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 24204.080078\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 248.430756\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 248.394882\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 248.358139\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 248.323822\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 248.287857\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 248.250717\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 248.213028\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 248.174179\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 248.133530\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 248.093292\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 248.055359\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 248.013428\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.537147\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.537118\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.537089\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.537059\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.537030\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.537001\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.536971\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.536942\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.536912\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.536883\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.536854\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.536824\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.682583\n",
      ">> Epoch 1 finished \tANN training loss 0.676475\n",
      ">> Epoch 2 finished \tANN training loss 0.672514\n",
      ">> Epoch 3 finished \tANN training loss 0.665496\n",
      ">> Epoch 4 finished \tANN training loss 0.643430\n",
      ">> Epoch 5 finished \tANN training loss 0.543272\n",
      ">> Epoch 6 finished \tANN training loss 0.436012\n",
      ">> Epoch 7 finished \tANN training loss 0.302947\n",
      ">> Epoch 8 finished \tANN training loss 0.248957\n",
      ">> Epoch 9 finished \tANN training loss 0.215550\n",
      ">> Epoch 10 finished \tANN training loss 0.190344\n",
      ">> Epoch 11 finished \tANN training loss 0.170848\n",
      ">> Epoch 12 finished \tANN training loss 0.155079\n",
      ">> Epoch 13 finished \tANN training loss 0.141466\n",
      ">> Epoch 14 finished \tANN training loss 0.129660\n",
      ">> Epoch 15 finished \tANN training loss 0.119764\n",
      ">> Epoch 16 finished \tANN training loss 0.111338\n",
      ">> Epoch 17 finished \tANN training loss 0.103953\n",
      ">> Epoch 18 finished \tANN training loss 0.097620\n",
      ">> Epoch 19 finished \tANN training loss 0.091892\n",
      ">> Epoch 20 finished \tANN training loss 0.087491\n",
      ">> Epoch 21 finished \tANN training loss 0.083320\n",
      ">> Epoch 22 finished \tANN training loss 0.079360\n",
      ">> Epoch 23 finished \tANN training loss 0.075597\n",
      ">> Epoch 24 finished \tANN training loss 0.072379\n",
      ">> Epoch 25 finished \tANN training loss 0.069462\n",
      ">> Epoch 26 finished \tANN training loss 0.066765\n",
      ">> Epoch 27 finished \tANN training loss 0.064464\n",
      ">> Epoch 28 finished \tANN training loss 0.062263\n",
      ">> Epoch 29 finished \tANN training loss 0.060155\n",
      ">> Epoch 30 finished \tANN training loss 0.058241\n",
      ">> Epoch 31 finished \tANN training loss 0.056401\n",
      ">> Epoch 32 finished \tANN training loss 0.054936\n",
      ">> Epoch 33 finished \tANN training loss 0.053302\n",
      ">> Epoch 34 finished \tANN training loss 0.051781\n",
      ">> Epoch 35 finished \tANN training loss 0.050300\n",
      ">> Epoch 36 finished \tANN training loss 0.048922\n",
      ">> Epoch 37 finished \tANN training loss 0.047508\n",
      ">> Epoch 38 finished \tANN training loss 0.046188\n",
      ">> Epoch 39 finished \tANN training loss 0.044870\n",
      ">> Epoch 40 finished \tANN training loss 0.043631\n",
      ">> Epoch 41 finished \tANN training loss 0.042390\n",
      ">> Epoch 42 finished \tANN training loss 0.041187\n",
      ">> Epoch 43 finished \tANN training loss 0.040056\n",
      ">> Epoch 44 finished \tANN training loss 0.038991\n",
      ">> Epoch 45 finished \tANN training loss 0.038045\n",
      ">> Epoch 46 finished \tANN training loss 0.037162\n",
      ">> Epoch 47 finished \tANN training loss 0.036336\n",
      ">> Epoch 48 finished \tANN training loss 0.035558\n",
      ">> Epoch 49 finished \tANN training loss 0.034853\n",
      ">> Epoch 50 finished \tANN training loss 0.034148\n",
      ">> Epoch 51 finished \tANN training loss 0.033539\n",
      ">> Epoch 52 finished \tANN training loss 0.032962\n",
      ">> Epoch 53 finished \tANN training loss 0.032385\n",
      ">> Epoch 54 finished \tANN training loss 0.031886\n",
      ">> Epoch 55 finished \tANN training loss 0.031406\n",
      ">> Epoch 56 finished \tANN training loss 0.030935\n",
      ">> Epoch 57 finished \tANN training loss 0.030499\n",
      ">> Epoch 58 finished \tANN training loss 0.030115\n",
      ">> Epoch 59 finished \tANN training loss 0.029735\n",
      ">> Epoch 60 finished \tANN training loss 0.029379\n",
      ">> Epoch 61 finished \tANN training loss 0.029070\n",
      ">> Epoch 62 finished \tANN training loss 0.028731\n",
      ">> Epoch 63 finished \tANN training loss 0.028403\n",
      ">> Epoch 64 finished \tANN training loss 0.028093\n",
      ">> Epoch 65 finished \tANN training loss 0.027791\n",
      ">> Epoch 66 finished \tANN training loss 0.027529\n",
      ">> Epoch 67 finished \tANN training loss 0.027238\n",
      ">> Epoch 68 finished \tANN training loss 0.026971\n",
      ">> Epoch 69 finished \tANN training loss 0.026745\n",
      ">> Epoch 70 finished \tANN training loss 0.026533\n",
      ">> Epoch 71 finished \tANN training loss 0.026302\n",
      ">> Epoch 72 finished \tANN training loss 0.026086\n",
      ">> Epoch 73 finished \tANN training loss 0.025860\n",
      ">> Epoch 74 finished \tANN training loss 0.025665\n",
      ">> Epoch 75 finished \tANN training loss 0.025479\n",
      ">> Epoch 76 finished \tANN training loss 0.025291\n",
      ">> Epoch 77 finished \tANN training loss 0.025116\n",
      ">> Epoch 78 finished \tANN training loss 0.024950\n",
      ">> Epoch 79 finished \tANN training loss 0.024782\n",
      ">> Epoch 80 finished \tANN training loss 0.024623\n",
      ">> Epoch 81 finished \tANN training loss 0.024470\n",
      ">> Epoch 82 finished \tANN training loss 0.024325\n",
      ">> Epoch 83 finished \tANN training loss 0.024181\n",
      ">> Epoch 84 finished \tANN training loss 0.024045\n",
      ">> Epoch 85 finished \tANN training loss 0.023911\n",
      ">> Epoch 86 finished \tANN training loss 0.023781\n",
      ">> Epoch 87 finished \tANN training loss 0.023658\n",
      ">> Epoch 88 finished \tANN training loss 0.023520\n",
      ">> Epoch 89 finished \tANN training loss 0.023404\n",
      ">> Epoch 90 finished \tANN training loss 0.023282\n",
      ">> Epoch 91 finished \tANN training loss 0.023173\n",
      ">> Epoch 92 finished \tANN training loss 0.023064\n",
      ">> Epoch 93 finished \tANN training loss 0.022954\n",
      ">> Epoch 94 finished \tANN training loss 0.022851\n",
      ">> Epoch 95 finished \tANN training loss 0.022744\n",
      ">> Epoch 96 finished \tANN training loss 0.022648\n",
      ">> Epoch 97 finished \tANN training loss 0.022548\n",
      ">> Epoch 98 finished \tANN training loss 0.022451\n",
      ">> Epoch 99 finished \tANN training loss 0.022355\n",
      ">> Epoch 100 finished \tANN training loss 0.022259\n",
      ">> Epoch 101 finished \tANN training loss 0.022139\n",
      ">> Epoch 102 finished \tANN training loss 0.022045\n",
      ">> Epoch 103 finished \tANN training loss 0.021924\n",
      ">> Epoch 104 finished \tANN training loss 0.021837\n",
      ">> Epoch 105 finished \tANN training loss 0.021716\n",
      ">> Epoch 106 finished \tANN training loss 0.021625\n",
      ">> Epoch 107 finished \tANN training loss 0.021535\n",
      ">> Epoch 108 finished \tANN training loss 0.021449\n",
      ">> Epoch 109 finished \tANN training loss 0.021363\n",
      ">> Epoch 110 finished \tANN training loss 0.021242\n",
      ">> Epoch 111 finished \tANN training loss 0.021158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 112 finished \tANN training loss 0.021070\n",
      ">> Epoch 113 finished \tANN training loss 0.020986\n",
      ">> Epoch 114 finished \tANN training loss 0.020869\n",
      ">> Epoch 115 finished \tANN training loss 0.020783\n",
      ">> Epoch 116 finished \tANN training loss 0.020659\n",
      ">> Epoch 117 finished \tANN training loss 0.020575\n",
      ">> Epoch 118 finished \tANN training loss 0.020484\n",
      ">> Epoch 119 finished \tANN training loss 0.020393\n",
      ">> Epoch 120 finished \tANN training loss 0.020301\n",
      ">> Epoch 121 finished \tANN training loss 0.020194\n",
      ">> Epoch 122 finished \tANN training loss 0.020084\n",
      ">> Epoch 123 finished \tANN training loss 0.019947\n",
      ">> Epoch 124 finished \tANN training loss 0.019809\n",
      ">> Epoch 125 finished \tANN training loss 0.019670\n",
      ">> Epoch 126 finished \tANN training loss 0.019531\n",
      ">> Epoch 127 finished \tANN training loss 0.019392\n",
      ">> Epoch 128 finished \tANN training loss 0.019253\n",
      ">> Epoch 129 finished \tANN training loss 0.019037\n",
      ">> Epoch 130 finished \tANN training loss 0.018897\n",
      ">> Epoch 131 finished \tANN training loss 0.018667\n",
      ">> Epoch 132 finished \tANN training loss 0.018528\n",
      ">> Epoch 133 finished \tANN training loss 0.018296\n",
      ">> Epoch 134 finished \tANN training loss 0.018064\n",
      ">> Epoch 135 finished \tANN training loss 0.017924\n",
      ">> Epoch 136 finished \tANN training loss 0.017783\n",
      ">> Epoch 137 finished \tANN training loss 0.017641\n",
      ">> Epoch 138 finished \tANN training loss 0.017498\n",
      ">> Epoch 139 finished \tANN training loss 0.017256\n",
      ">> Epoch 140 finished \tANN training loss 0.017107\n",
      ">> Epoch 141 finished \tANN training loss 0.016958\n",
      ">> Epoch 142 finished \tANN training loss 0.016704\n",
      ">> Epoch 143 finished \tANN training loss 0.016554\n",
      ">> Epoch 144 finished \tANN training loss 0.016399\n",
      ">> Epoch 145 finished \tANN training loss 0.016244\n",
      ">> Epoch 146 finished \tANN training loss 0.016097\n",
      ">> Epoch 147 finished \tANN training loss 0.015935\n",
      ">> Epoch 148 finished \tANN training loss 0.015771\n",
      ">> Epoch 149 finished \tANN training loss 0.015605\n",
      ">> Epoch 150 finished \tANN training loss 0.015439\n",
      ">> Epoch 151 finished \tANN training loss 0.015272\n",
      ">> Epoch 152 finished \tANN training loss 0.015098\n",
      ">> Epoch 153 finished \tANN training loss 0.014925\n",
      ">> Epoch 154 finished \tANN training loss 0.014748\n",
      ">> Epoch 155 finished \tANN training loss 0.014580\n",
      ">> Epoch 156 finished \tANN training loss 0.014397\n",
      ">> Epoch 157 finished \tANN training loss 0.014210\n",
      ">> Epoch 158 finished \tANN training loss 0.014021\n",
      ">> Epoch 159 finished \tANN training loss 0.013829\n",
      ">> Epoch 160 finished \tANN training loss 0.013635\n",
      ">> Epoch 161 finished \tANN training loss 0.013437\n",
      ">> Epoch 162 finished \tANN training loss 0.013233\n",
      ">> Epoch 163 finished \tANN training loss 0.013028\n",
      ">> Epoch 164 finished \tANN training loss 0.012822\n",
      ">> Epoch 165 finished \tANN training loss 0.012611\n",
      ">> Epoch 166 finished \tANN training loss 0.012395\n",
      ">> Epoch 167 finished \tANN training loss 0.012177\n",
      ">> Epoch 168 finished \tANN training loss 0.011952\n",
      ">> Epoch 169 finished \tANN training loss 0.011571\n",
      ">> Epoch 170 finished \tANN training loss 0.011335\n",
      ">> Epoch 171 finished \tANN training loss 0.011098\n",
      ">> Epoch 172 finished \tANN training loss 0.010858\n",
      ">> Epoch 173 finished \tANN training loss 0.010616\n",
      ">> Epoch 174 finished \tANN training loss 0.010372\n",
      ">> Epoch 175 finished \tANN training loss 0.010124\n",
      ">> Epoch 176 finished \tANN training loss 0.009876\n",
      ">> Epoch 177 finished \tANN training loss 0.009628\n",
      ">> Epoch 178 finished \tANN training loss 0.009377\n",
      ">> Epoch 179 finished \tANN training loss 0.009130\n",
      ">> Epoch 180 finished \tANN training loss 0.008880\n",
      ">> Epoch 181 finished \tANN training loss 0.008464\n",
      ">> Epoch 182 finished \tANN training loss 0.008049\n",
      ">> Epoch 183 finished \tANN training loss 0.007820\n",
      ">> Epoch 184 finished \tANN training loss 0.007598\n",
      ">> Epoch 185 finished \tANN training loss 0.007379\n",
      ">> Epoch 186 finished \tANN training loss 0.007175\n",
      ">> Epoch 187 finished \tANN training loss 0.006979\n",
      ">> Epoch 188 finished \tANN training loss 0.006789\n",
      ">> Epoch 189 finished \tANN training loss 0.006607\n",
      ">> Epoch 190 finished \tANN training loss 0.006428\n",
      ">> Epoch 191 finished \tANN training loss 0.006248\n",
      ">> Epoch 192 finished \tANN training loss 0.006071\n",
      ">> Epoch 193 finished \tANN training loss 0.005894\n",
      ">> Epoch 194 finished \tANN training loss 0.005721\n",
      ">> Epoch 195 finished \tANN training loss 0.005549\n",
      ">> Epoch 196 finished \tANN training loss 0.005260\n",
      ">> Epoch 197 finished \tANN training loss 0.005091\n",
      ">> Epoch 198 finished \tANN training loss 0.004927\n",
      ">> Epoch 199 finished \tANN training loss 0.004657\n",
      ">> Epoch 200 finished \tANN training loss 0.004396\n",
      ">> Epoch 201 finished \tANN training loss 0.004247\n",
      ">> Epoch 202 finished \tANN training loss 0.004101\n",
      ">> Epoch 203 finished \tANN training loss 0.003871\n",
      ">> Epoch 204 finished \tANN training loss 0.003736\n",
      ">> Epoch 205 finished \tANN training loss 0.003607\n",
      ">> Epoch 206 finished \tANN training loss 0.003404\n",
      ">> Epoch 207 finished \tANN training loss 0.003286\n",
      ">> Epoch 208 finished \tANN training loss 0.003172\n",
      ">> Epoch 209 finished \tANN training loss 0.003063\n",
      ">> Epoch 210 finished \tANN training loss 0.002959\n",
      ">> Epoch 211 finished \tANN training loss 0.002859\n",
      ">> Epoch 212 finished \tANN training loss 0.002760\n",
      ">> Epoch 213 finished \tANN training loss 0.002666\n",
      ">> Epoch 214 finished \tANN training loss 0.002573\n",
      ">> Epoch 215 finished \tANN training loss 0.002487\n",
      ">> Epoch 216 finished \tANN training loss 0.002403\n",
      ">> Epoch 217 finished \tANN training loss 0.002324\n",
      ">> Epoch 218 finished \tANN training loss 0.002249\n",
      ">> Epoch 219 finished \tANN training loss 0.002131\n",
      ">> Epoch 220 finished \tANN training loss 0.002060\n",
      ">> Epoch 221 finished \tANN training loss 0.001995\n",
      ">> Epoch 222 finished \tANN training loss 0.001931\n",
      ">> Epoch 223 finished \tANN training loss 0.001872\n",
      ">> Epoch 224 finished \tANN training loss 0.001815\n",
      ">> Epoch 225 finished \tANN training loss 0.001726\n",
      ">> Epoch 226 finished \tANN training loss 0.001671\n",
      ">> Epoch 227 finished \tANN training loss 0.001622\n",
      ">> Epoch 228 finished \tANN training loss 0.001547\n",
      ">> Epoch 229 finished \tANN training loss 0.001502\n",
      ">> Epoch 230 finished \tANN training loss 0.001458\n",
      ">> Epoch 231 finished \tANN training loss 0.001418\n",
      ">> Epoch 232 finished \tANN training loss 0.001379\n",
      ">> Epoch 233 finished \tANN training loss 0.001342\n",
      ">> Epoch 234 finished \tANN training loss 0.001306\n",
      ">> Epoch 235 finished \tANN training loss 0.001272\n",
      ">> Epoch 236 finished \tANN training loss 0.001240\n",
      ">> Epoch 237 finished \tANN training loss 0.001208\n",
      ">> Epoch 238 finished \tANN training loss 0.001175\n",
      ">> Epoch 239 finished \tANN training loss 0.001146\n",
      ">> Epoch 240 finished \tANN training loss 0.001117\n",
      ">> Epoch 241 finished \tANN training loss 0.001089\n",
      ">> Epoch 242 finished \tANN training loss 0.001063\n",
      ">> Epoch 243 finished \tANN training loss 0.001037\n",
      ">> Epoch 244 finished \tANN training loss 0.000998\n",
      ">> Epoch 245 finished \tANN training loss 0.000973\n",
      ">> Epoch 246 finished \tANN training loss 0.000939\n",
      ">> Epoch 247 finished \tANN training loss 0.000905\n",
      ">> Epoch 248 finished \tANN training loss 0.000874\n",
      ">> Epoch 249 finished \tANN training loss 0.000854\n",
      ">> Epoch 250 finished \tANN training loss 0.000836\n",
      ">> Epoch 251 finished \tANN training loss 0.000818\n",
      ">> Epoch 252 finished \tANN training loss 0.000801\n",
      ">> Epoch 253 finished \tANN training loss 0.000785\n",
      ">> Epoch 254 finished \tANN training loss 0.000768\n",
      ">> Epoch 255 finished \tANN training loss 0.000752\n",
      ">> Epoch 256 finished \tANN training loss 0.000738\n",
      ">> Epoch 257 finished \tANN training loss 0.000716\n",
      ">> Epoch 258 finished \tANN training loss 0.000702\n",
      ">> Epoch 259 finished \tANN training loss 0.000688\n",
      ">> Epoch 260 finished \tANN training loss 0.000675\n",
      ">> Epoch 261 finished \tANN training loss 0.000663\n",
      ">> Epoch 262 finished \tANN training loss 0.000650\n",
      ">> Epoch 263 finished \tANN training loss 0.000638\n",
      ">> Epoch 264 finished \tANN training loss 0.000627\n",
      ">> Epoch 265 finished \tANN training loss 0.000615\n",
      ">> Epoch 266 finished \tANN training loss 0.000604\n",
      ">> Epoch 267 finished \tANN training loss 0.000594\n",
      ">> Epoch 268 finished \tANN training loss 0.000583\n",
      ">> Epoch 269 finished \tANN training loss 0.000573\n",
      ">> Epoch 270 finished \tANN training loss 0.000563\n",
      ">> Epoch 271 finished \tANN training loss 0.000554\n",
      ">> Epoch 272 finished \tANN training loss 0.000545\n",
      ">> Epoch 273 finished \tANN training loss 0.000536\n",
      ">> Epoch 274 finished \tANN training loss 0.000528\n",
      ">> Epoch 275 finished \tANN training loss 0.000520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 276 finished \tANN training loss 0.000512\n",
      ">> Epoch 277 finished \tANN training loss 0.000503\n",
      ">> Epoch 278 finished \tANN training loss 0.000491\n",
      ">> Epoch 279 finished \tANN training loss 0.000484\n",
      ">> Epoch 280 finished \tANN training loss 0.000476\n",
      ">> Epoch 281 finished \tANN training loss 0.000469\n",
      ">> Epoch 282 finished \tANN training loss 0.000458\n",
      ">> Epoch 283 finished \tANN training loss 0.000451\n",
      ">> Epoch 284 finished \tANN training loss 0.000445\n",
      ">> Epoch 285 finished \tANN training loss 0.000439\n",
      ">> Epoch 286 finished \tANN training loss 0.000433\n",
      ">> Epoch 287 finished \tANN training loss 0.000428\n",
      ">> Epoch 288 finished \tANN training loss 0.000422\n",
      ">> Epoch 289 finished \tANN training loss 0.000413\n",
      ">> Epoch 290 finished \tANN training loss 0.000408\n",
      ">> Epoch 291 finished \tANN training loss 0.000403\n",
      ">> Epoch 292 finished \tANN training loss 0.000398\n",
      ">> Epoch 293 finished \tANN training loss 0.000393\n",
      ">> Epoch 294 finished \tANN training loss 0.000388\n",
      ">> Epoch 295 finished \tANN training loss 0.000383\n",
      ">> Epoch 296 finished \tANN training loss 0.000378\n",
      ">> Epoch 297 finished \tANN training loss 0.000374\n",
      ">> Epoch 298 finished \tANN training loss 0.000369\n",
      ">> Epoch 299 finished \tANN training loss 0.000365\n",
      ">> Epoch 300 finished \tANN training loss 0.000361\n",
      ">> Epoch 301 finished \tANN training loss 0.000356\n",
      ">> Epoch 302 finished \tANN training loss 0.000352\n",
      ">> Epoch 303 finished \tANN training loss 0.000348\n",
      ">> Epoch 304 finished \tANN training loss 0.000344\n",
      ">> Epoch 305 finished \tANN training loss 0.000340\n",
      ">> Epoch 306 finished \tANN training loss 0.000337\n",
      ">> Epoch 307 finished \tANN training loss 0.000333\n",
      ">> Epoch 308 finished \tANN training loss 0.000327\n",
      ">> Epoch 309 finished \tANN training loss 0.000322\n",
      ">> Epoch 310 finished \tANN training loss 0.000318\n",
      ">> Epoch 311 finished \tANN training loss 0.000315\n",
      ">> Epoch 312 finished \tANN training loss 0.000312\n",
      ">> Epoch 313 finished \tANN training loss 0.000308\n",
      ">> Epoch 314 finished \tANN training loss 0.000305\n",
      ">> Epoch 315 finished \tANN training loss 0.000302\n",
      ">> Epoch 316 finished \tANN training loss 0.000299\n",
      ">> Epoch 317 finished \tANN training loss 0.000296\n",
      ">> Epoch 318 finished \tANN training loss 0.000293\n",
      ">> Epoch 319 finished \tANN training loss 0.000289\n",
      ">> Epoch 320 finished \tANN training loss 0.000286\n",
      ">> Epoch 321 finished \tANN training loss 0.000281\n",
      ">> Epoch 322 finished \tANN training loss 0.000277\n",
      ">> Epoch 323 finished \tANN training loss 0.000275\n",
      ">> Epoch 324 finished \tANN training loss 0.000272\n",
      ">> Epoch 325 finished \tANN training loss 0.000269\n",
      ">> Epoch 326 finished \tANN training loss 0.000267\n",
      ">> Epoch 327 finished \tANN training loss 0.000265\n",
      ">> Epoch 328 finished \tANN training loss 0.000262\n",
      ">> Epoch 329 finished \tANN training loss 0.000260\n",
      ">> Epoch 330 finished \tANN training loss 0.000258\n",
      ">> Epoch 331 finished \tANN training loss 0.000255\n",
      ">> Epoch 332 finished \tANN training loss 0.000252\n",
      ">> Epoch 333 finished \tANN training loss 0.000249\n",
      ">> Epoch 334 finished \tANN training loss 0.000247\n",
      ">> Epoch 335 finished \tANN training loss 0.000245\n",
      ">> Epoch 336 finished \tANN training loss 0.000243\n",
      ">> Epoch 337 finished \tANN training loss 0.000241\n",
      ">> Epoch 338 finished \tANN training loss 0.000239\n",
      ">> Epoch 339 finished \tANN training loss 0.000237\n",
      ">> Epoch 340 finished \tANN training loss 0.000235\n",
      ">> Epoch 341 finished \tANN training loss 0.000233\n",
      ">> Epoch 342 finished \tANN training loss 0.000231\n",
      ">> Epoch 343 finished \tANN training loss 0.000229\n",
      ">> Epoch 344 finished \tANN training loss 0.000227\n",
      ">> Epoch 345 finished \tANN training loss 0.000226\n",
      ">> Epoch 346 finished \tANN training loss 0.000224\n",
      ">> Epoch 347 finished \tANN training loss 0.000222\n",
      ">> Epoch 348 finished \tANN training loss 0.000220\n",
      ">> Epoch 349 finished \tANN training loss 0.000218\n",
      ">> Epoch 350 finished \tANN training loss 0.000217\n",
      ">> Epoch 351 finished \tANN training loss 0.000215\n",
      ">> Epoch 352 finished \tANN training loss 0.000213\n",
      ">> Epoch 353 finished \tANN training loss 0.000212\n",
      ">> Epoch 354 finished \tANN training loss 0.000209\n",
      ">> Epoch 355 finished \tANN training loss 0.000208\n",
      ">> Epoch 356 finished \tANN training loss 0.000206\n",
      ">> Epoch 357 finished \tANN training loss 0.000204\n",
      ">> Epoch 358 finished \tANN training loss 0.000203\n",
      ">> Epoch 359 finished \tANN training loss 0.000201\n",
      ">> Epoch 360 finished \tANN training loss 0.000200\n",
      ">> Epoch 361 finished \tANN training loss 0.000198\n",
      ">> Epoch 362 finished \tANN training loss 0.000197\n",
      ">> Epoch 363 finished \tANN training loss 0.000195\n",
      ">> Epoch 364 finished \tANN training loss 0.000194\n",
      ">> Epoch 365 finished \tANN training loss 0.000193\n",
      ">> Epoch 366 finished \tANN training loss 0.000190\n",
      ">> Epoch 367 finished \tANN training loss 0.000189\n",
      ">> Epoch 368 finished \tANN training loss 0.000188\n",
      ">> Epoch 369 finished \tANN training loss 0.000186\n",
      ">> Epoch 370 finished \tANN training loss 0.000185\n",
      ">> Epoch 371 finished \tANN training loss 0.000184\n",
      ">> Epoch 372 finished \tANN training loss 0.000182\n",
      ">> Epoch 373 finished \tANN training loss 0.000181\n",
      ">> Epoch 374 finished \tANN training loss 0.000180\n",
      ">> Epoch 375 finished \tANN training loss 0.000179\n",
      ">> Epoch 376 finished \tANN training loss 0.000178\n",
      ">> Epoch 377 finished \tANN training loss 0.000176\n",
      ">> Epoch 378 finished \tANN training loss 0.000175\n",
      ">> Epoch 379 finished \tANN training loss 0.000174\n",
      ">> Epoch 380 finished \tANN training loss 0.000173\n",
      ">> Epoch 381 finished \tANN training loss 0.000171\n",
      ">> Epoch 382 finished \tANN training loss 0.000170\n",
      ">> Epoch 383 finished \tANN training loss 0.000168\n",
      ">> Epoch 384 finished \tANN training loss 0.000167\n",
      ">> Epoch 385 finished \tANN training loss 0.000166\n",
      ">> Epoch 386 finished \tANN training loss 0.000165\n",
      ">> Epoch 387 finished \tANN training loss 0.000164\n",
      ">> Epoch 388 finished \tANN training loss 0.000163\n",
      ">> Epoch 389 finished \tANN training loss 0.000162\n",
      ">> Epoch 390 finished \tANN training loss 0.000161\n",
      ">> Epoch 391 finished \tANN training loss 0.000160\n",
      ">> Epoch 392 finished \tANN training loss 0.000159\n",
      ">> Epoch 393 finished \tANN training loss 0.000158\n",
      ">> Epoch 394 finished \tANN training loss 0.000157\n",
      ">> Epoch 395 finished \tANN training loss 0.000156\n",
      ">> Epoch 396 finished \tANN training loss 0.000155\n",
      ">> Epoch 397 finished \tANN training loss 0.000154\n",
      ">> Epoch 398 finished \tANN training loss 0.000153\n",
      ">> Epoch 399 finished \tANN training loss 0.000152\n",
      ">> Epoch 400 finished \tANN training loss 0.000151\n",
      ">> Epoch 401 finished \tANN training loss 0.000150\n",
      ">> Epoch 402 finished \tANN training loss 0.000149\n",
      ">> Epoch 403 finished \tANN training loss 0.000149\n",
      ">> Epoch 404 finished \tANN training loss 0.000148\n",
      ">> Epoch 405 finished \tANN training loss 0.000146\n",
      ">> Epoch 406 finished \tANN training loss 0.000146\n",
      ">> Epoch 407 finished \tANN training loss 0.000145\n",
      ">> Epoch 408 finished \tANN training loss 0.000144\n",
      ">> Epoch 409 finished \tANN training loss 0.000143\n",
      ">> Epoch 410 finished \tANN training loss 0.000142\n",
      ">> Epoch 411 finished \tANN training loss 0.000141\n",
      ">> Epoch 412 finished \tANN training loss 0.000141\n",
      ">> Epoch 413 finished \tANN training loss 0.000140\n",
      ">> Epoch 414 finished \tANN training loss 0.000139\n",
      ">> Epoch 415 finished \tANN training loss 0.000138\n",
      ">> Epoch 416 finished \tANN training loss 0.000137\n",
      ">> Epoch 417 finished \tANN training loss 0.000137\n",
      ">> Epoch 418 finished \tANN training loss 0.000136\n",
      ">> Epoch 419 finished \tANN training loss 0.000135\n",
      ">> Epoch 420 finished \tANN training loss 0.000134\n",
      ">> Epoch 421 finished \tANN training loss 0.000133\n",
      ">> Epoch 422 finished \tANN training loss 0.000133\n",
      ">> Epoch 423 finished \tANN training loss 0.000132\n",
      ">> Epoch 424 finished \tANN training loss 0.000131\n",
      ">> Epoch 425 finished \tANN training loss 0.000131\n",
      ">> Epoch 426 finished \tANN training loss 0.000130\n",
      ">> Epoch 427 finished \tANN training loss 0.000129\n",
      ">> Epoch 428 finished \tANN training loss 0.000129\n",
      ">> Epoch 429 finished \tANN training loss 0.000128\n",
      ">> Epoch 430 finished \tANN training loss 0.000127\n",
      ">> Epoch 431 finished \tANN training loss 0.000127\n",
      ">> Epoch 432 finished \tANN training loss 0.000126\n",
      ">> Epoch 433 finished \tANN training loss 0.000125\n",
      ">> Epoch 434 finished \tANN training loss 0.000125\n",
      ">> Epoch 435 finished \tANN training loss 0.000124\n",
      ">> Epoch 436 finished \tANN training loss 0.000123\n",
      ">> Epoch 437 finished \tANN training loss 0.000123\n",
      ">> Epoch 438 finished \tANN training loss 0.000122\n",
      ">> Epoch 439 finished \tANN training loss 0.000121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 440 finished \tANN training loss 0.000121\n",
      ">> Epoch 441 finished \tANN training loss 0.000120\n",
      ">> Epoch 442 finished \tANN training loss 0.000119\n",
      ">> Epoch 443 finished \tANN training loss 0.000119\n",
      ">> Epoch 444 finished \tANN training loss 0.000118\n",
      ">> Epoch 445 finished \tANN training loss 0.000117\n",
      ">> Epoch 446 finished \tANN training loss 0.000117\n",
      ">> Epoch 447 finished \tANN training loss 0.000116\n",
      ">> Epoch 448 finished \tANN training loss 0.000116\n",
      ">> Epoch 449 finished \tANN training loss 0.000115\n",
      ">> Epoch 450 finished \tANN training loss 0.000114\n",
      ">> Epoch 451 finished \tANN training loss 0.000114\n",
      ">> Epoch 452 finished \tANN training loss 0.000113\n",
      ">> Epoch 453 finished \tANN training loss 0.000113\n",
      ">> Epoch 454 finished \tANN training loss 0.000112\n",
      ">> Epoch 455 finished \tANN training loss 0.000111\n",
      ">> Epoch 456 finished \tANN training loss 0.000111\n",
      ">> Epoch 457 finished \tANN training loss 0.000110\n",
      ">> Epoch 458 finished \tANN training loss 0.000110\n",
      ">> Epoch 459 finished \tANN training loss 0.000109\n",
      ">> Epoch 460 finished \tANN training loss 0.000109\n",
      ">> Epoch 461 finished \tANN training loss 0.000108\n",
      ">> Epoch 462 finished \tANN training loss 0.000108\n",
      ">> Epoch 463 finished \tANN training loss 0.000107\n",
      ">> Epoch 464 finished \tANN training loss 0.000107\n",
      ">> Epoch 465 finished \tANN training loss 0.000107\n",
      ">> Epoch 466 finished \tANN training loss 0.000106\n",
      ">> Epoch 467 finished \tANN training loss 0.000105\n",
      ">> Epoch 468 finished \tANN training loss 0.000105\n",
      ">> Epoch 469 finished \tANN training loss 0.000104\n",
      ">> Epoch 470 finished \tANN training loss 0.000104\n",
      ">> Epoch 471 finished \tANN training loss 0.000103\n",
      ">> Epoch 472 finished \tANN training loss 0.000103\n",
      ">> Epoch 473 finished \tANN training loss 0.000103\n",
      ">> Epoch 474 finished \tANN training loss 0.000102\n",
      ">> Epoch 475 finished \tANN training loss 0.000102\n",
      ">> Epoch 476 finished \tANN training loss 0.000101\n",
      ">> Epoch 477 finished \tANN training loss 0.000101\n",
      ">> Epoch 478 finished \tANN training loss 0.000100\n",
      ">> Epoch 479 finished \tANN training loss 0.000100\n",
      ">> Epoch 480 finished \tANN training loss 0.000099\n",
      ">> Epoch 481 finished \tANN training loss 0.000099\n",
      ">> Epoch 482 finished \tANN training loss 0.000098\n",
      ">> Epoch 483 finished \tANN training loss 0.000098\n",
      ">> Epoch 484 finished \tANN training loss 0.000098\n",
      ">> Epoch 485 finished \tANN training loss 0.000097\n",
      ">> Epoch 486 finished \tANN training loss 0.000097\n",
      ">> Epoch 487 finished \tANN training loss 0.000096\n",
      ">> Epoch 488 finished \tANN training loss 0.000096\n",
      ">> Epoch 489 finished \tANN training loss 0.000096\n",
      ">> Epoch 490 finished \tANN training loss 0.000095\n",
      ">> Epoch 491 finished \tANN training loss 0.000095\n",
      ">> Epoch 492 finished \tANN training loss 0.000094\n",
      ">> Epoch 493 finished \tANN training loss 0.000094\n",
      ">> Epoch 494 finished \tANN training loss 0.000094\n",
      ">> Epoch 495 finished \tANN training loss 0.000093\n",
      ">> Epoch 496 finished \tANN training loss 0.000093\n",
      ">> Epoch 497 finished \tANN training loss 0.000092\n",
      ">> Epoch 498 finished \tANN training loss 0.000092\n",
      ">> Epoch 499 finished \tANN training loss 0.000092\n",
      ">> Epoch 500 finished \tANN training loss 0.000091\n",
      ">> Epoch 501 finished \tANN training loss 0.000091\n",
      ">> Epoch 502 finished \tANN training loss 0.000090\n",
      ">> Epoch 503 finished \tANN training loss 0.000090\n",
      ">> Epoch 504 finished \tANN training loss 0.000090\n",
      ">> Epoch 505 finished \tANN training loss 0.000089\n",
      ">> Epoch 506 finished \tANN training loss 0.000089\n",
      ">> Epoch 507 finished \tANN training loss 0.000089\n",
      ">> Epoch 508 finished \tANN training loss 0.000088\n",
      ">> Epoch 509 finished \tANN training loss 0.000088\n",
      ">> Epoch 510 finished \tANN training loss 0.000088\n",
      ">> Epoch 511 finished \tANN training loss 0.000087\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=1e-05,\n",
       "                            idx_to_label_map={0: 0, 1: 1},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1}, learning_rate=0.1,\n",
       "                            n_iter_backprop=512, verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_test)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "yhat_classes = yhat_probs# np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n",
      "Precision: 1.000000\n",
      "Recall: 1.000000\n",
      "F1 score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, yhat_classes,average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes, average='macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 1.000000\n",
      "[[22  0]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(Y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "#fprate, tprate, thresholds = roc_curve(Y_test, yhat_probs, average = 'macro')\n",
    "#print('ROC AUC: %f' % thresholds)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 5\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, yhat_classes, pos_label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[22  0]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_test,yhat_classes, labels= [0, 1])\n",
    "print('Confusion matrix : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = classification_report(Y_test,yhat_classes, labels= [0, 1, ])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier.save('./models/Binary/Without IP/DBN/DBN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
