{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer,StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "## for explainer\n",
    "#from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#b = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "a = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "b = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "c =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "d =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "e =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#c = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week3.csv', low_memory=False , encoding='cp1252')\n",
    "#d = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#e =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "#g =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "#h =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10310733, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1795404, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape)\n",
    "#a.drop(a[a['attackType'] == '---'].index, axis = 0, inplace= True) \n",
    "b.drop(b[b['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "c.drop(c[c['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "d.drop(d[d['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "#e.drop(e[e['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_external = pd.concat([c,d,e], axis = 0)\n",
    "data_external.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to Increment attackID values\n",
    "data_external['attackID'] = data_external['attackID'].apply(lambda x: str(int(x) + 70) if x != '---' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(df):\n",
    "    if 'M' in df:\n",
    "        df = df.split('M')\n",
    "        df = df[0].strip()\n",
    "        df = float(df) * 1000000\n",
    "    elif 'B' in df:\n",
    "        df = df.split('B')\n",
    "        df = df[0].strip()\n",
    "        df =  float(df) * 1000000000\n",
    "    else: \n",
    "        df =float(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([a,b,data_external], axis = 0)\n",
    "data.reset_index(drop= True, inplace= True)\n",
    "data['Bytes'] = data['Bytes'].apply(lambda x: convert_bytes(x))\n",
    "columns = ['Src Pt', 'Dst Pt','Tos','Flows','Packets', 'Bytes']\n",
    "for i in columns:\n",
    "    data[i] = pd.to_numeric(data[i]);\n",
    "del columns\n",
    "del a,b,c,d,e, data_external\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Hexadecimal value to Binary\n",
    "def hex_to_binary(hexdata):\n",
    "    scale = 16 ## equals to hexadecimal\n",
    "    num_of_bits = 9\n",
    "    return bin(int(hexdata, scale))[2:].zfill(num_of_bits);\n",
    "#Converts TCP flags to Binary\n",
    "def to_Binary(x):\n",
    "    l = 0\n",
    "    x = '...' + x\n",
    "    x = list(x)\n",
    "    for i in x:\n",
    "        if (i=='.'):\n",
    "            x[l]= '0'\n",
    "        else:\n",
    "            x[l] = '1'\n",
    "        l = l +1\n",
    "    return ''.join(x)\n",
    "#Converts the 'Flags' column to 9 indiviual columns (manual oneshot encoding)\n",
    "def flag_convert(df):  \n",
    "   # df['Flags'] = df['Flags'].apply(lambda x: (list(x)))\n",
    "   # temp = df['Flags'].apply(lambda x: toBinary(x))\n",
    "    hex_values = list(df[(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    flag_values = list(df[~(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    binary_values = {}\n",
    "    for i in hex_values:\n",
    "         binary_values[i] = (hex_to_binary(i))\n",
    "    for i in flag_values:\n",
    "         binary_values[i] = (to_Binary(i))\n",
    "    temp = df['Flags'].replace(binary_values)\n",
    "#temp = temp.apply(lambda x: pd.Series(x)) \n",
    "    temp = pd.DataFrame(temp.apply(list).tolist())\n",
    "#temp = pd.DataFrame(temp)\n",
    "#a = a.iloc[: , 1:]\n",
    "   # print(temp.head())\n",
    "    temp.columns = ['N','C','E','U' ,'A','P','R','S','F']\n",
    "    for i in temp.columns:\n",
    "        temp[i] = pd.to_numeric(temp[i]);\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a IP_pairs \n",
    "def make_pair(df):\n",
    "    ip_pair = df['Src IP Addr'] +'/' +df['Dst IP Addr']\n",
    "    source_ip = df['Src IP Addr'].unique().tolist()\n",
    "    destination_ip = df['Dst IP Addr'].unique().tolist()\n",
    "   # df = df.drop(columns = ['Src IP Addr', 'Dst IP Addr'])\n",
    "    df.insert(1, ' IP Pair', ip_pair)\n",
    "    return df\n",
    "\n",
    "def check_inverse(df):\n",
    "    list_pairs = df[' IP Pair'].unique()\n",
    "    tuple_pair = []\n",
    "    for i in list_pairs:\n",
    "        tuple_pair.append(tuple((i.split('/'))))\n",
    "    dic_store = {}\n",
    "    for i in tuple_pair:\n",
    "        if (i  not in dic_store.keys()) and (i[::-1] not in dic_store.keys()):\n",
    "            dic_store[i] = i[0] + '/' +i[1]\n",
    "    print(len(dic_store.keys()))\n",
    "    dic_final = {}\n",
    "    for i in dic_store.keys():\n",
    "        dic_final[i[0] + '/' +i[1]] = dic_store[i]\n",
    "        dic_final[i[1] + '/' +i[0]] = dic_store[i]\n",
    "    df[' IP Pair'] = df[' IP Pair'].map(dic_final)               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_IP(df):\n",
    "    columns = ['sourceIP_feature 1', 'sourceIP_feature 2', 'sourceIP_feature 3', 'sourceIP_feature 4', 'destIP_feature 1',\n",
    "              'destIP_feature 2', 'destIP_feature 3', 'destIP_feature 4']\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    columns = data.select_dtypes(include=numerics).columns\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    #df.astype({'attackType': 'str'})\n",
    "    df['attackType'] = label_encoder.fit_transform(df['attackType'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df['Proto'] = label_encoder.fit_transform(df['Proto'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    onehot_encoder1 = OneHotEncoder()\n",
    "    onehot_encoder1.fit(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = onehot_encoder1.transform(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = pd.DataFrame.sparse.from_spmatrix(proto)\n",
    "    proto.astype('int32')\n",
    "    proto.columns = label_encoder.classes_\n",
    "   # print(proto.head(1))\n",
    "    df = pd.concat([df, proto], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(columns = ['Date first seen', ' IP Pair', 'Flows', 'class', 'attackID','Flags',\n",
    "                              'attackDescription', 'Src IP Addr', 'Dst IP Addr','Proto'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplit IP address into features, 7 features\n",
    "def split_to_net(IP_address):\n",
    "    IP_list = IP_address.split(\".\")\n",
    "    needed_len = 7\n",
    "    needed_len = needed_len - len(IP_list)\n",
    "    for i in range(0,needed_len,1):\n",
    "        IP_list.append('0')\n",
    "    return IP_list\n",
    "#replace unknown IP address, and convert to columns\n",
    "def IP_split(df): \n",
    "    replace = {\"ATTACKER1\":\"0.0.0.0\",\n",
    "           \"ATTACKER2\":\"0.0.0.0\",\n",
    "           \"ATTACKER3\":\"0.0.0.0\",\n",
    "           \"EXT_SERVER\": \"0.0.0.0.1\",\n",
    "          \"OPENSTACK_NET\": \"0.0.0.0.0.1\",\n",
    "          \"DNS\": \"0.0.0.0.0.0.1\"}\n",
    "    df = df.replace({\"Src IP Addr\": replace, \"Dst IP Addr\": replace}, value=None)\n",
    "    temp_source = df[\"Src IP Addr\"].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "    temp_des = df['Dst IP Addr'].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "   # sourceIP = list(df[\"Src IP Addr\"].unique())\n",
    "   # destIP = list(df[\"Dst IP Addr\"].unique())\n",
    "   # sourceIP_values = {}\n",
    "   # desIP_values = {}\n",
    "   # for i in sourceIP:\n",
    "   #      sourceIP_values[i] = (split_to_net(i))\n",
    "   # for i in destIP:\n",
    "   #      desIP_values[i] = (split_to_net(i))\n",
    "    #print(sourceIP_values)\n",
    "   # print(desIP_values)\n",
    "#for Source IP\n",
    "    temp_source = temp_source.apply(lambda x: split_to_net(x) )\n",
    "    temp_source = pd.DataFrame(temp_source.apply(list).tolist())\n",
    "    temp_source.columns = ['sourceIP_feature 1','sourceIP_feature 2','sourceIP_feature 3','sourceIP_feature 4' ,\n",
    "                    'sourceEXT_SERVER','sourceOPENSTACK_NET','sourceDNS']\n",
    "    for i in temp_source.columns:\n",
    "        temp_source[i] = pd.to_numeric(temp_source[i]);\n",
    "    temp_source = temp_source.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_source], axis = 1)\n",
    "    #for Destination IP\n",
    "    temp_des = temp_des.apply(lambda x: split_to_net(x) )\n",
    "    temp_des = pd.DataFrame(temp_des.apply(list).tolist())\n",
    "    temp_des.columns = ['destIP_feature 1','destIP_feature 2','destIP_feature 3','destIP_feature 4' ,\n",
    "                    'destEXT_SERVER','destOPENSTACK_NET','destDNS']\n",
    "   # for i in temp_des.columns:\n",
    "       # temp_des[i] = pd.to_numeric(temp_des[i]);\n",
    "    temp_des = temp_des.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_des], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59362\n"
     ]
    }
   ],
   "source": [
    "data = make_pair(data)\n",
    "data = check_inverse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = IP_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Duration', 'Src Pt', 'Dst Pt', 'Packets', 'Bytes', 'Flows', 'Tos'], dtype='object')\n",
      "['---', 'bruteForce', 'dos', 'pingScan', 'portScan']\n",
      "[0, 1, 2, 3, 4]\n",
      "['GRE  ', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = normalize(data)\n",
    "data =  one_shot(data) \n",
    "#data = normalize_IP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def unix_time(df):\n",
    "  #  df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x + ':00' if len(x) != 19 else x)\n",
    "   # df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x[0 : 5 : ] + x[7 : :] if len(x) != 19 else x[0 : 7 : ] + x[9 : :])\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: x.timestamp()*1000)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_profile(grouped):\n",
    "    grouped['---'] = unix_time(grouped['---'])\n",
    "    start_time = int(grouped['---'].head(1)['Date first seen'].values[0])\n",
    "    end_time = int(grouped['---'].tail(1)['Date first seen'].values[0])\n",
    "#date_bins = pd.IntervalIndex.from_tuples(\n",
    "#        [(i, i+3600000) for i in range(start_time, end_time, 3600000)],\n",
    "#        closed=\"left\")\n",
    "#date_labels = [f\"{i}\" for i in range(1, len(date_bins)+1, 1)]\n",
    "    normal_data = dict(tuple( grouped['---'].groupby( pd.cut(\n",
    "            grouped['---']['Date first seen'],\n",
    "               np.arange(start_time, end_time, 3*3600000)))))\n",
    "    del grouped['---']\n",
    "    num = []\n",
    "    for i in grouped_data.keys():\n",
    "          num.append(len(grouped_data[i]))\n",
    "    print(min(num))\n",
    "    num = max(num)\n",
    "    print(num)\n",
    "    print(len(grouped.keys()))\n",
    "    grouped = {**grouped, **normal_data}\n",
    "    print(len(grouped.keys()))\n",
    "    return grouped, num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data= dict(tuple(data.groupby(['attackID'])))\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---: 7195669 : 0\n",
      "Attack ID: 25; Lenght of Attack: 201; Attack Type: 1\n",
      "Attack ID: 27; Lenght of Attack: 680; Attack Type: 1\n",
      "Attack ID: 30; Lenght of Attack: 46; Attack Type: 1\n",
      "Attack ID: 32; Lenght of Attack: 335; Attack Type: 1\n",
      "Attack ID: 39; Lenght of Attack: 364; Attack Type: 1\n",
      "Attack ID: 54; Lenght of Attack: 183; Attack Type: 1\n",
      "Attack ID: 55; Lenght of Attack: 757; Attack Type: 1\n",
      "Attack ID: 56; Lenght of Attack: 427; Attack Type: 1\n",
      "Attack ID: 61; Lenght of Attack: 705; Attack Type: 1\n",
      "Attack ID: 62; Lenght of Attack: 574; Attack Type: 1\n",
      "Attack ID: 64; Lenght of Attack: 480; Attack Type: 1\n",
      "Attack ID: 70; Lenght of Attack: 240; Attack Type: 1\n",
      "Attack ID: 73; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 74; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 75; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 76; Lenght of Attack: 168; Attack Type: 1\n",
      "Attack ID: 78; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 79; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 81; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 83; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 84; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 87; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 88; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 90; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 91; Lenght of Attack: 40; Attack Type: 1\n",
      "Attack ID: 92; Lenght of Attack: 40; Attack Type: 1\n",
      "dos : 18\n",
      "Attack ID: 16; Lenght of Attack: 261003; Attack Type: 2\n",
      "Attack ID: 18; Lenght of Attack: 295302; Attack Type: 2\n",
      "Attack ID: 23; Lenght of Attack: 72788; Attack Type: 2\n",
      "Attack ID: 26; Lenght of Attack: 74471; Attack Type: 2\n",
      "Attack ID: 28; Lenght of Attack: 36306; Attack Type: 2\n",
      "Attack ID: 3; Lenght of Attack: 37118; Attack Type: 2\n",
      "Attack ID: 31; Lenght of Attack: 144845; Attack Type: 2\n",
      "Attack ID: 4; Lenght of Attack: 72063; Attack Type: 2\n",
      "Attack ID: 42; Lenght of Attack: 184040; Attack Type: 2\n",
      "Attack ID: 44; Lenght of Attack: 261169; Attack Type: 2\n",
      "Attack ID: 45; Lenght of Attack: 224960; Attack Type: 2\n",
      "Attack ID: 46; Lenght of Attack: 111720; Attack Type: 2\n",
      "Attack ID: 53; Lenght of Attack: 516299; Attack Type: 2\n",
      "Attack ID: 59; Lenght of Attack: 110484; Attack Type: 2\n",
      "Attack ID: 6; Lenght of Attack: 37134; Attack Type: 2\n",
      "Attack ID: 60; Lenght of Attack: 333627; Attack Type: 2\n",
      "Attack ID: 63; Lenght of Attack: 148641; Attack Type: 2\n",
      "Attack ID: 9; Lenght of Attack: 37057; Attack Type: 2\n",
      "pingScan: 16\n",
      "Attack ID: 10; Lenght of Attack: 311; Attack Type: 3\n",
      "Attack ID: 13; Lenght of Attack: 513; Attack Type: 3\n",
      "Attack ID: 15; Lenght of Attack: 64; Attack Type: 3\n",
      "Attack ID: 22; Lenght of Attack: 295; Attack Type: 3\n",
      "Attack ID: 24; Lenght of Attack: 466; Attack Type: 3\n",
      "Attack ID: 33; Lenght of Attack: 307; Attack Type: 3\n",
      "Attack ID: 35; Lenght of Attack: 263; Attack Type: 3\n",
      "Attack ID: 36; Lenght of Attack: 494; Attack Type: 3\n",
      "Attack ID: 38; Lenght of Attack: 267; Attack Type: 3\n",
      "Attack ID: 41; Lenght of Attack: 379; Attack Type: 3\n",
      "Attack ID: 52; Lenght of Attack: 607; Attack Type: 3\n",
      "Attack ID: 57; Lenght of Attack: 522; Attack Type: 3\n",
      "Attack ID: 58; Lenght of Attack: 510; Attack Type: 3\n",
      "Attack ID: 65; Lenght of Attack: 373; Attack Type: 3\n",
      "Attack ID: 66; Lenght of Attack: 359; Attack Type: 3\n",
      "Attack ID: 69; Lenght of Attack: 360; Attack Type: 3\n",
      "portScan : 32\n",
      "Attack ID: 1; Lenght of Attack: 7657; Attack Type: 4\n",
      "Attack ID: 11; Lenght of Attack: 17401; Attack Type: 4\n",
      "Attack ID: 12; Lenght of Attack: 11526; Attack Type: 4\n",
      "Attack ID: 14; Lenght of Attack: 13807; Attack Type: 4\n",
      "Attack ID: 17; Lenght of Attack: 13338; Attack Type: 4\n",
      "Attack ID: 19; Lenght of Attack: 11672; Attack Type: 4\n",
      "Attack ID: 2; Lenght of Attack: 1927; Attack Type: 4\n",
      "Attack ID: 20; Lenght of Attack: 11748; Attack Type: 4\n",
      "Attack ID: 21; Lenght of Attack: 5113; Attack Type: 4\n",
      "Attack ID: 29; Lenght of Attack: 19732; Attack Type: 4\n",
      "Attack ID: 34; Lenght of Attack: 12909; Attack Type: 4\n",
      "Attack ID: 37; Lenght of Attack: 26114; Attack Type: 4\n",
      "Attack ID: 40; Lenght of Attack: 11609; Attack Type: 4\n",
      "Attack ID: 43; Lenght of Attack: 2143; Attack Type: 4\n",
      "Attack ID: 47; Lenght of Attack: 13420; Attack Type: 4\n",
      "Attack ID: 48; Lenght of Attack: 13600; Attack Type: 4\n",
      "Attack ID: 49; Lenght of Attack: 17629; Attack Type: 4\n",
      "Attack ID: 5; Lenght of Attack: 4948; Attack Type: 4\n",
      "Attack ID: 50; Lenght of Attack: 4589; Attack Type: 4\n",
      "Attack ID: 51; Lenght of Attack: 11968; Attack Type: 4\n",
      "Attack ID: 67; Lenght of Attack: 13426; Attack Type: 4\n",
      "Attack ID: 68; Lenght of Attack: 5632; Attack Type: 4\n",
      "Attack ID: 7; Lenght of Attack: 9586; Attack Type: 4\n",
      "Attack ID: 71; Lenght of Attack: 2008; Attack Type: 4\n",
      "Attack ID: 72; Lenght of Attack: 2002; Attack Type: 4\n",
      "Attack ID: 77; Lenght of Attack: 6410; Attack Type: 4\n",
      "Attack ID: 8; Lenght of Attack: 4424; Attack Type: 4\n",
      "Attack ID: 80; Lenght of Attack: 1991; Attack Type: 4\n",
      "Attack ID: 82; Lenght of Attack: 1370; Attack Type: 4\n",
      "Attack ID: 85; Lenght of Attack: 1984; Attack Type: 4\n",
      "Attack ID: 86; Lenght of Attack: 2002; Attack Type: 4\n",
      "Attack ID: 89; Lenght of Attack: 952; Attack Type: 4\n"
     ]
    }
   ],
   "source": [
    "no_1 = []\n",
    "no_2 = []\n",
    "no_3 = []\n",
    "no_4 = []\n",
    "for i in grouped_data.keys():\n",
    "   \n",
    "    if grouped_data[i]['attackType'].unique()[0] == 0:\n",
    "        print(f\"{i}: {len(grouped_data[i])} : {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 1:\n",
    "              no_1.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 2:\n",
    "              no_2.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 3:\n",
    "              no_3.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 4:\n",
    "              no_4.append(i)\n",
    "for i in no_1:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"dos : {len(no_2)}\")\n",
    "for i in no_2:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"pingScan: {len(no_3)}\")\n",
    "for i in no_3:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"portScan : {len(no_4)}\")\n",
    "for i in no_4:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del no_1\n",
    "del no_2\n",
    "del no_3\n",
    "del no_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_largeInstances(dic, length):\n",
    "    remove_ID = []\n",
    "    for i in dic.keys():\n",
    "        if (i != '---'):\n",
    "            if(len(dic[i]) >= length):\n",
    "                remove_ID.append(i)\n",
    "    print(len(remove_ID))\n",
    "    removed_attacks = {}\n",
    "    for i in remove_ID:\n",
    "        removed_attacks[i] = dic[i]\n",
    "        del dic[i]\n",
    "    return dic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "grouped_data = del_largeInstances(grouped_data, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "19732\n",
      "73\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "#grouped_data, num = normal_profile(grouped_data)\n",
    "grouped_data1= {}\n",
    "for i in grouped_data.keys():\n",
    "    grouped_data[i] = flag_convert(grouped_data[i])\n",
    "   # grouped_data[i] =  drop_columns(grouped_data[i])\n",
    "grouped_data, num = normal_profile(grouped_data)\n",
    "for i in grouped_data.keys():\n",
    "   # grouped_data[i] = flag_convert(grouped_data[i])\n",
    "    grouped_data[i] =  drop_columns(grouped_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : False\n",
      "10 : False\n",
      "11 : False\n",
      "12 : False\n",
      "13 : False\n",
      "14 : False\n",
      "15 : False\n",
      "17 : False\n",
      "19 : False\n",
      "2 : False\n",
      "20 : False\n",
      "21 : False\n",
      "22 : False\n",
      "24 : False\n",
      "25 : False\n",
      "27 : False\n",
      "29 : False\n",
      "30 : False\n",
      "32 : False\n",
      "33 : False\n",
      "34 : False\n",
      "35 : False\n",
      "36 : False\n",
      "38 : False\n",
      "39 : False\n",
      "40 : False\n",
      "41 : False\n",
      "43 : False\n",
      "47 : False\n",
      "48 : False\n",
      "49 : False\n",
      "5 : False\n",
      "50 : False\n",
      "51 : False\n",
      "52 : False\n",
      "54 : False\n",
      "55 : False\n",
      "56 : False\n",
      "57 : False\n",
      "58 : False\n",
      "61 : False\n",
      "62 : False\n",
      "64 : False\n",
      "65 : False\n",
      "66 : False\n",
      "67 : False\n",
      "68 : False\n",
      "69 : False\n",
      "7 : False\n",
      "70 : False\n",
      "71 : False\n",
      "72 : False\n",
      "73 : False\n",
      "74 : False\n",
      "75 : False\n",
      "76 : False\n",
      "77 : False\n",
      "78 : False\n",
      "79 : False\n",
      "8 : False\n",
      "80 : False\n",
      "81 : False\n",
      "82 : False\n",
      "83 : False\n",
      "84 : False\n",
      "85 : False\n",
      "86 : False\n",
      "87 : False\n",
      "88 : False\n",
      "89 : False\n",
      "90 : False\n",
      "91 : False\n",
      "92 : False\n",
      "(1489536076632, 1489546876632] : False\n",
      "(1489546876632, 1489557676632] : False\n",
      "(1489557676632, 1489568476632] : False\n",
      "(1489568476632, 1489579276632] : False\n",
      "(1489579276632, 1489590076632] : False\n",
      "(1489590076632, 1489600876632] : False\n",
      "(1489600876632, 1489611676632] : False\n",
      "(1489611676632, 1489622476632] : False\n",
      "(1489622476632, 1489633276632] : False\n",
      "(1489633276632, 1489644076632] : False\n",
      "(1489644076632, 1489654876632] : False\n",
      "(1489654876632, 1489665676632] : False\n",
      "(1489665676632, 1489676476632] : False\n",
      "(1489676476632, 1489687276632] : False\n",
      "(1489687276632, 1489698076632] : False\n",
      "(1489698076632, 1489708876632] : False\n",
      "(1489708876632, 1489719676632] : False\n",
      "(1489719676632, 1489730476632] : False\n",
      "(1489730476632, 1489741276632] : False\n",
      "(1489741276632, 1489752076632] : False\n",
      "(1489752076632, 1489762876632] : False\n",
      "(1489762876632, 1489773676632] : False\n",
      "(1489773676632, 1489784476632] : False\n",
      "(1489784476632, 1489795276632] : False\n",
      "(1489795276632, 1489806076632] : False\n",
      "(1489806076632, 1489816876632] : False\n",
      "(1489816876632, 1489827676632] : False\n",
      "(1489827676632, 1489838476632] : False\n",
      "(1489838476632, 1489849276632] : False\n",
      "(1489849276632, 1489860076632] : False\n",
      "(1489860076632, 1489870876632] : False\n",
      "(1489870876632, 1489881676632] : False\n",
      "(1489881676632, 1489892476632] : False\n",
      "(1489892476632, 1489903276632] : False\n",
      "(1489903276632, 1489914076632] : False\n",
      "(1489914076632, 1489924876632] : False\n",
      "(1489924876632, 1489935676632] : False\n",
      "(1489935676632, 1489946476632] : False\n",
      "(1489946476632, 1489957276632] : False\n",
      "(1489957276632, 1489968076632] : False\n",
      "(1489968076632, 1489978876632] : False\n",
      "(1489978876632, 1489989676632] : False\n",
      "(1489989676632, 1490000476632] : False\n",
      "(1490000476632, 1490011276632] : False\n",
      "(1490011276632, 1490022076632] : False\n",
      "(1490022076632, 1490032876632] : False\n",
      "(1490032876632, 1490043676632] : False\n",
      "(1490043676632, 1490054476632] : False\n",
      "(1490054476632, 1490065276632] : False\n",
      "(1490065276632, 1490076076632] : False\n",
      "(1490076076632, 1490086876632] : False\n",
      "(1490086876632, 1490097676632] : False\n",
      "(1490097676632, 1490108476632] : False\n",
      "(1490108476632, 1490119276632] : False\n",
      "(1490119276632, 1490130076632] : False\n",
      "(1490130076632, 1490140876632] : False\n",
      "(1490140876632, 1490151676632] : False\n",
      "(1490151676632, 1490162476632] : False\n",
      "(1490162476632, 1490173276632] : False\n",
      "(1490173276632, 1490184076632] : False\n",
      "(1490184076632, 1490194876632] : False\n",
      "(1490194876632, 1490205676632] : False\n",
      "(1490205676632, 1490216476632] : False\n",
      "(1490216476632, 1490227276632] : False\n",
      "(1490227276632, 1490238076632] : False\n",
      "(1490238076632, 1490248876632] : False\n",
      "(1490248876632, 1490259676632] : False\n",
      "(1490259676632, 1490270476632] : False\n",
      "(1490270476632, 1490281276632] : False\n",
      "(1490281276632, 1490292076632] : False\n",
      "(1490292076632, 1490302876632] : False\n",
      "(1490302876632, 1490313676632] : False\n",
      "(1490313676632, 1490324476632] : False\n",
      "(1490324476632, 1490335276632] : False\n",
      "(1490335276632, 1490346076632] : False\n",
      "(1490346076632, 1490356876632] : False\n",
      "(1490356876632, 1490367676632] : False\n",
      "(1490367676632, 1490378476632] : False\n",
      "(1490378476632, 1490389276632] : False\n",
      "(1490389276632, 1490400076632] : False\n",
      "(1490400076632, 1490410876632] : False\n",
      "(1490410876632, 1490421676632] : False\n",
      "(1490421676632, 1490432476632] : False\n",
      "(1490432476632, 1490443276632] : False\n",
      "(1490443276632, 1490454076632] : False\n",
      "(1490454076632, 1490464876632] : False\n",
      "(1490464876632, 1490475676632] : False\n",
      "(1490475676632, 1490486476632] : False\n",
      "(1490486476632, 1490497276632] : False\n",
      "(1490497276632, 1490508076632] : False\n",
      "(1490508076632, 1490518876632] : False\n",
      "(1490518876632, 1490529676632] : False\n",
      "(1490529676632, 1490540476632] : False\n",
      "(1490540476632, 1490551276632] : False\n",
      "(1490551276632, 1490562076632] : False\n",
      "(1490562076632, 1490572876632] : False\n",
      "(1490572876632, 1490583676632] : False\n",
      "(1490583676632, 1490594476632] : False\n",
      "(1490594476632, 1490605276632] : False\n",
      "(1490605276632, 1490616076632] : False\n",
      "(1490616076632, 1490626876632] : False\n",
      "(1490626876632, 1490637676632] : False\n",
      "(1490637676632, 1490648476632] : False\n",
      "(1490648476632, 1490659276632] : False\n",
      "(1490659276632, 1490670076632] : False\n",
      "(1490670076632, 1490680876632] : False\n",
      "(1490680876632, 1490691676632] : False\n",
      "(1490691676632, 1490702476632] : False\n",
      "(1490702476632, 1490713276632] : False\n",
      "(1490713276632, 1490724076632] : False\n",
      "(1490724076632, 1490734876632] : False\n",
      "(1490734876632, 1490745676632] : False\n",
      "(1490745676632, 1490756476632] : False\n",
      "(1490756476632, 1490767276632] : False\n",
      "(1490767276632, 1490778076632] : False\n",
      "(1490778076632, 1490788876632] : False\n",
      "(1490788876632, 1490799676632] : False\n",
      "(1490799676632, 1490810476632] : False\n",
      "(1490810476632, 1490821276632] : False\n",
      "(1490821276632, 1490832076632] : False\n",
      "(1490832076632, 1490842876632] : False\n",
      "(1490842876632, 1490853676632] : False\n",
      "(1490853676632, 1490864476632] : False\n",
      "(1490864476632, 1490875276632] : False\n",
      "(1490875276632, 1490886076632] : False\n",
      "(1490886076632, 1490896876632] : False\n",
      "(1490896876632, 1490907676632] : False\n",
      "(1490907676632, 1490918476632] : False\n",
      "(1490918476632, 1490929276632] : False\n",
      "(1490929276632, 1490940076632] : False\n",
      "(1490940076632, 1490950876632] : False\n",
      "(1490950876632, 1490961676632] : False\n",
      "(1490961676632, 1490972476632] : False\n",
      "(1490972476632, 1490983276632] : False\n",
      "(1490983276632, 1490994076632] : False\n",
      "(1490994076632, 1491004876632] : False\n",
      "(1491004876632, 1491015676632] : False\n",
      "(1491015676632, 1491026476632] : False\n",
      "(1491026476632, 1491037276632] : False\n",
      "(1491037276632, 1491048076632] : False\n",
      "(1491048076632, 1491058876632] : False\n",
      "(1491058876632, 1491069676632] : False\n",
      "(1491069676632, 1491080476632] : False\n",
      "(1491080476632, 1491091276632] : False\n",
      "(1491091276632, 1491102076632] : False\n",
      "(1491102076632, 1491112876632] : False\n",
      "(1491112876632, 1491123676632] : False\n",
      "(1491123676632, 1491134476632] : False\n",
      "(1491134476632, 1491145276632] : False\n",
      "(1491145276632, 1491156076632] : False\n",
      "(1491156076632, 1491166876632] : False\n",
      "(1491166876632, 1491177676632] : False\n",
      "(1491177676632, 1491188476632] : False\n",
      "(1491188476632, 1491199276632] : False\n",
      "(1491199276632, 1491210076632] : False\n",
      "(1491210076632, 1491220876632] : False\n",
      "(1491220876632, 1491231676632] : False\n",
      "(1491231676632, 1491242476632] : False\n",
      "(1491242476632, 1491253276632] : False\n",
      "(1491253276632, 1491264076632] : False\n",
      "(1491264076632, 1491274876632] : False\n",
      "(1491274876632, 1491285676632] : False\n",
      "(1491285676632, 1491296476632] : False\n",
      "(1491296476632, 1491307276632] : False\n",
      "(1491307276632, 1491318076632] : False\n",
      "(1491318076632, 1491328876632] : False\n",
      "(1491328876632, 1491339676632] : False\n",
      "(1491339676632, 1491350476632] : False\n",
      "(1491350476632, 1491361276632] : False\n",
      "(1491361276632, 1491372076632] : False\n",
      "(1491372076632, 1491382876632] : False\n",
      "(1491382876632, 1491393676632] : False\n",
      "(1491393676632, 1491404476632] : False\n",
      "(1491404476632, 1491415276632] : False\n",
      "(1491415276632, 1491426076632] : False\n",
      "(1491426076632, 1491436876632] : False\n",
      "(1491436876632, 1491447676632] : False\n",
      "(1491447676632, 1491458476632] : False\n",
      "(1491458476632, 1491469276632] : False\n",
      "(1491469276632, 1491480076632] : False\n",
      "(1491480076632, 1491490876632] : False\n",
      "(1491490876632, 1491501676632] : False\n",
      "(1491501676632, 1491512476632] : False\n",
      "(1491512476632, 1491523276632] : False\n",
      "(1491523276632, 1491534076632] : False\n",
      "(1491534076632, 1491544876632] : False\n",
      "(1491544876632, 1491555676632] : False\n",
      "(1491555676632, 1491566476632] : False\n",
      "(1491566476632, 1491577276632] : False\n",
      "(1491577276632, 1491588076632] : False\n",
      "(1491588076632, 1491598876632] : False\n",
      "(1491598876632, 1491609676632] : False\n",
      "(1491609676632, 1491620476632] : False\n",
      "(1491620476632, 1491631276632] : False\n",
      "(1491631276632, 1491642076632] : False\n",
      "(1491642076632, 1491652876632] : False\n",
      "(1491652876632, 1491663676632] : False\n",
      "(1491663676632, 1491674476632] : False\n",
      "(1491674476632, 1491685276632] : False\n",
      "(1491685276632, 1491696076632] : False\n",
      "(1491696076632, 1491706876632] : False\n",
      "(1491706876632, 1491717676632] : False\n",
      "(1491717676632, 1491728476632] : False\n",
      "(1491728476632, 1491739276632] : False\n",
      "(1491739276632, 1491750076632] : False\n",
      "(1491750076632, 1491760876632] : False\n",
      "(1491760876632, 1491771676632] : False\n",
      "(1491771676632, 1491782476632] : False\n",
      "(1491782476632, 1491793276632] : False\n",
      "(1491793276632, 1491804076632] : False\n",
      "(1491804076632, 1491814876632] : False\n",
      "(1491814876632, 1491825676632] : False\n",
      "(1491825676632, 1491836476632] : False\n",
      "(1491836476632, 1491847276632] : False\n",
      "(1491847276632, 1491858076632] : False\n",
      "(1491858076632, 1491868876632] : False\n",
      "(1491868876632, 1491879676632] : False\n",
      "(1491879676632, 1491890476632] : False\n",
      "(1491890476632, 1491901276632] : False\n",
      "(1491901276632, 1491912076632] : False\n",
      "(1491912076632, 1491922876632] : False\n",
      "(1491922876632, 1491933676632] : False\n",
      "(1491933676632, 1491944476632] : False\n",
      "(1491944476632, 1491955276632] : False\n",
      "(1491955276632, 1491966076632] : False\n",
      "(1491966076632, 1491976876632] : False\n",
      "(1491976876632, 1491987676632] : False\n",
      "(1491987676632, 1491998476632] : False\n",
      "(1491998476632, 1492009276632] : False\n",
      "(1492009276632, 1492020076632] : False\n",
      "(1492020076632, 1492030876632] : False\n",
      "(1492030876632, 1492041676632] : False\n",
      "(1492041676632, 1492052476632] : False\n",
      "(1492052476632, 1492063276632] : False\n",
      "(1492063276632, 1492074076632] : False\n",
      "(1492074076632, 1492084876632] : False\n",
      "(1492084876632, 1492095676632] : False\n",
      "(1492095676632, 1492106476632] : False\n",
      "(1492106476632, 1492117276632] : False\n",
      "(1492117276632, 1492128076632] : False\n",
      "(1492128076632, 1492138876632] : False\n",
      "(1492138876632, 1492149676632] : False\n",
      "(1492149676632, 1492160476632] : False\n",
      "(1492160476632, 1492171276632] : False\n",
      "(1492171276632, 1492182076632] : False\n",
      "(1492182076632, 1492192876632] : False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492192876632, 1492203676632] : False\n",
      "(1492203676632, 1492214476632] : False\n",
      "(1492214476632, 1492225276632] : False\n",
      "(1492225276632, 1492236076632] : False\n",
      "(1492236076632, 1492246876632] : False\n",
      "(1492246876632, 1492257676632] : False\n",
      "(1492257676632, 1492268476632] : False\n",
      "(1492268476632, 1492279276632] : False\n",
      "(1492279276632, 1492290076632] : False\n",
      "(1492290076632, 1492300876632] : False\n",
      "(1492300876632, 1492311676632] : False\n",
      "(1492311676632, 1492322476632] : False\n",
      "(1492322476632, 1492333276632] : False\n",
      "(1492333276632, 1492344076632] : False\n",
      "(1492344076632, 1492354876632] : False\n",
      "(1492354876632, 1492365676632] : False\n",
      "(1492365676632, 1492376476632] : False\n",
      "(1492376476632, 1492387276632] : False\n",
      "(1492387276632, 1492398076632] : False\n",
      "(1492398076632, 1492408876632] : False\n",
      "(1492408876632, 1492419676632] : False\n",
      "(1492419676632, 1492430476632] : False\n",
      "(1492430476632, 1492441276632] : False\n",
      "(1492441276632, 1492452076632] : False\n",
      "(1492452076632, 1492462876632] : False\n",
      "(1492462876632, 1492473676632] : False\n",
      "(1492473676632, 1492484476632] : False\n",
      "(1492484476632, 1492495276632] : False\n",
      "(1492495276632, 1492506076632] : False\n",
      "(1492506076632, 1492516876632] : False\n",
      "(1492516876632, 1492527676632] : False\n"
     ]
    }
   ],
   "source": [
    "for i in grouped_data.keys():\n",
    "    #if (grouped_data[i].hasnull())\n",
    "    print(f'{i} : {grouped_data[i].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances which are empty: 167\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in grouped_data.keys():\n",
    "    if ( len(grouped_data[i]) == 0):\n",
    "        counter = counter +1;\n",
    "print(f\"Number of Instances which are empty: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 100 == 0 else x + 100 - x % 100\n",
    "#Convert to 3D arrays, input dict\n",
    "def make_array(dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    zero_arrays = []\n",
    "    for i in dic.keys():\n",
    "        if ( len(dic[i]) == 0):\n",
    "            zero_arrays.append(i);\n",
    "    for i in zero_arrays:\n",
    "        del dic[i]\n",
    "    for i in dic.keys():\n",
    "        x.append(np.array(dic[i].drop(['attackType'],axis = 1)).astype(np.float32))\n",
    "       # print(f'{i}')\n",
    "        y.append(dic[i]['attackType'].values[0])\n",
    "    print(len(y))\n",
    "    o = []\n",
    "    features = len(x[1][1])\n",
    "    #for i in x:\n",
    "     #   o.append(len(i))\n",
    "   # print(min(o))\n",
    "    o = num\n",
    "    o = roundup(o)\n",
    "    print(o)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        l = len(i)\n",
    "        i = list(i)\n",
    "        if(o > l):\n",
    "            l = o-l\n",
    "            for j in range(0, l, 1):\n",
    "                i.append([0] * features)\n",
    "        elif (o<l):\n",
    "            l = l-o\n",
    "            i = i[:-l]\n",
    "        #i = [k = np.array([k]) for l in i for k in l] # Makes array elements an array \n",
    "        x[index] = np.array(i).astype(np.float32)\n",
    "        index = index + 1\n",
    "    #x = [[i] for i in x]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "X,Y = make_array(grouped_data)\n",
    "del grouped_data\n",
    "gc.collect()\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 110, 1: 26, 3: 16, 4: 31}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_4D(arr):\n",
    "    x = []\n",
    "    for i in range(0, len(arr),1):\n",
    "        temp = []\n",
    "        for j in range(0,len(arr[i]),1):\n",
    "             temp.append([np.array([k]) for k in arr[i][j]])\n",
    "        x.append(np.array(temp).astype(np.float32))\n",
    "    return np.array(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = make_4D(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2, random_state=0,  stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X,Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 88, 1: 21, 3: 13, 4: 24}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 22, 1: 5, 3: 3, 4: 7}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in X_train:\n",
    "    print(f'{np.isnan(i).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 146 \n",
      " X:19800 \n",
      " Y:20 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "nsamples,nx, ny = X_train.shape\n",
    "print(f\"samples: {nsamples} \\n X:{nx} \\n Y:{ny} \\n \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19800"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((37,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128,64, 64],\n",
    "                                         learning_rate_rbm=0.00001,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=12,\n",
    "                                         n_iter_backprop=512,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del classifier\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\Desktop\\Project dISSERATAION\\dbn\\tensorflow\\models.py:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 25332.968750\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 25332.140625\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 25330.906250\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 25328.605469\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 25323.990234\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 25314.166016\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 25293.619141\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 25254.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 25183.914062\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 25077.185547\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 24926.800781\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 24727.859375\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 298.927795\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 298.881683\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 298.832458\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 298.783997\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 298.731659\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 298.682373\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 298.628906\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 298.572052\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 298.520447\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 298.466248\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 298.410492\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 298.352142\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 0.686568\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.686530\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.686491\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.686452\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.686413\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.686374\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.686335\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.686296\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.686258\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.686219\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.686180\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.686141\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.310071\n",
      ">> Epoch 1 finished \tANN training loss 1.248023\n",
      ">> Epoch 2 finished \tANN training loss 1.157869\n",
      ">> Epoch 3 finished \tANN training loss 0.834658\n",
      ">> Epoch 4 finished \tANN training loss 0.754924\n",
      ">> Epoch 5 finished \tANN training loss 0.655820\n",
      ">> Epoch 6 finished \tANN training loss 0.616247\n",
      ">> Epoch 7 finished \tANN training loss 0.594834\n",
      ">> Epoch 8 finished \tANN training loss 0.580536\n",
      ">> Epoch 9 finished \tANN training loss 0.570067\n",
      ">> Epoch 10 finished \tANN training loss 0.559510\n",
      ">> Epoch 11 finished \tANN training loss 0.551415\n",
      ">> Epoch 12 finished \tANN training loss 0.543389\n",
      ">> Epoch 13 finished \tANN training loss 0.534131\n",
      ">> Epoch 14 finished \tANN training loss 0.523192\n",
      ">> Epoch 15 finished \tANN training loss 0.509940\n",
      ">> Epoch 16 finished \tANN training loss 0.494235\n",
      ">> Epoch 17 finished \tANN training loss 0.477188\n",
      ">> Epoch 18 finished \tANN training loss 0.463547\n",
      ">> Epoch 19 finished \tANN training loss 0.455393\n",
      ">> Epoch 20 finished \tANN training loss 0.444119\n",
      ">> Epoch 21 finished \tANN training loss 0.431092\n",
      ">> Epoch 22 finished \tANN training loss 0.419339\n",
      ">> Epoch 23 finished \tANN training loss 0.407820\n",
      ">> Epoch 24 finished \tANN training loss 0.480717\n",
      ">> Epoch 25 finished \tANN training loss 0.413422\n",
      ">> Epoch 26 finished \tANN training loss 0.403881\n",
      ">> Epoch 27 finished \tANN training loss 0.365759\n",
      ">> Epoch 28 finished \tANN training loss 0.344318\n",
      ">> Epoch 29 finished \tANN training loss 0.320900\n",
      ">> Epoch 30 finished \tANN training loss 0.306404\n",
      ">> Epoch 31 finished \tANN training loss 0.309719\n",
      ">> Epoch 32 finished \tANN training loss 0.287741\n",
      ">> Epoch 33 finished \tANN training loss 0.281752\n",
      ">> Epoch 34 finished \tANN training loss 0.276742\n",
      ">> Epoch 35 finished \tANN training loss 0.271923\n",
      ">> Epoch 36 finished \tANN training loss 0.267649\n",
      ">> Epoch 37 finished \tANN training loss 0.263811\n",
      ">> Epoch 38 finished \tANN training loss 0.260019\n",
      ">> Epoch 39 finished \tANN training loss 0.256633\n",
      ">> Epoch 40 finished \tANN training loss 0.253747\n",
      ">> Epoch 41 finished \tANN training loss 0.250805\n",
      ">> Epoch 42 finished \tANN training loss 0.248126\n",
      ">> Epoch 43 finished \tANN training loss 0.245515\n",
      ">> Epoch 44 finished \tANN training loss 0.243322\n",
      ">> Epoch 45 finished \tANN training loss 0.240820\n",
      ">> Epoch 46 finished \tANN training loss 0.238751\n",
      ">> Epoch 47 finished \tANN training loss 0.236763\n",
      ">> Epoch 48 finished \tANN training loss 0.235008\n",
      ">> Epoch 49 finished \tANN training loss 0.233309\n",
      ">> Epoch 50 finished \tANN training loss 0.231629\n",
      ">> Epoch 51 finished \tANN training loss 0.230084\n",
      ">> Epoch 52 finished \tANN training loss 0.228642\n",
      ">> Epoch 53 finished \tANN training loss 0.227231\n",
      ">> Epoch 54 finished \tANN training loss 0.225741\n",
      ">> Epoch 55 finished \tANN training loss 0.224392\n",
      ">> Epoch 56 finished \tANN training loss 0.223026\n",
      ">> Epoch 57 finished \tANN training loss 0.221770\n",
      ">> Epoch 58 finished \tANN training loss 0.220558\n",
      ">> Epoch 59 finished \tANN training loss 0.219415\n",
      ">> Epoch 60 finished \tANN training loss 0.218295\n",
      ">> Epoch 61 finished \tANN training loss 0.217118\n",
      ">> Epoch 62 finished \tANN training loss 0.215951\n",
      ">> Epoch 63 finished \tANN training loss 0.214772\n",
      ">> Epoch 64 finished \tANN training loss 0.213579\n",
      ">> Epoch 65 finished \tANN training loss 0.212235\n",
      ">> Epoch 66 finished \tANN training loss 0.210766\n",
      ">> Epoch 67 finished \tANN training loss 0.209154\n",
      ">> Epoch 68 finished \tANN training loss 0.207020\n",
      ">> Epoch 69 finished \tANN training loss 0.204672\n",
      ">> Epoch 70 finished \tANN training loss 0.200982\n",
      ">> Epoch 71 finished \tANN training loss 0.196338\n",
      ">> Epoch 72 finished \tANN training loss 0.189858\n",
      ">> Epoch 73 finished \tANN training loss 0.180140\n",
      ">> Epoch 74 finished \tANN training loss 0.168556\n",
      ">> Epoch 75 finished \tANN training loss 0.156418\n",
      ">> Epoch 76 finished \tANN training loss 0.145851\n",
      ">> Epoch 77 finished \tANN training loss 0.136958\n",
      ">> Epoch 78 finished \tANN training loss 0.130376\n",
      ">> Epoch 79 finished \tANN training loss 0.124321\n",
      ">> Epoch 80 finished \tANN training loss 0.119150\n",
      ">> Epoch 81 finished \tANN training loss 0.114331\n",
      ">> Epoch 82 finished \tANN training loss 0.110538\n",
      ">> Epoch 83 finished \tANN training loss 0.106792\n",
      ">> Epoch 84 finished \tANN training loss 0.103429\n",
      ">> Epoch 85 finished \tANN training loss 0.100279\n",
      ">> Epoch 86 finished \tANN training loss 0.097316\n",
      ">> Epoch 87 finished \tANN training loss 0.094659\n",
      ">> Epoch 88 finished \tANN training loss 0.091920\n",
      ">> Epoch 89 finished \tANN training loss 0.089446\n",
      ">> Epoch 90 finished \tANN training loss 0.087230\n",
      ">> Epoch 91 finished \tANN training loss 0.085104\n",
      ">> Epoch 92 finished \tANN training loss 0.083096\n",
      ">> Epoch 93 finished \tANN training loss 0.081082\n",
      ">> Epoch 94 finished \tANN training loss 0.078971\n",
      ">> Epoch 95 finished \tANN training loss 0.077127\n",
      ">> Epoch 96 finished \tANN training loss 0.075492\n",
      ">> Epoch 97 finished \tANN training loss 0.073648\n",
      ">> Epoch 98 finished \tANN training loss 0.071995\n",
      ">> Epoch 99 finished \tANN training loss 0.070501\n",
      ">> Epoch 100 finished \tANN training loss 0.069074\n",
      ">> Epoch 101 finished \tANN training loss 0.067820\n",
      ">> Epoch 102 finished \tANN training loss 0.066379\n",
      ">> Epoch 103 finished \tANN training loss 0.065176\n",
      ">> Epoch 104 finished \tANN training loss 0.063905\n",
      ">> Epoch 105 finished \tANN training loss 0.062720\n",
      ">> Epoch 106 finished \tANN training loss 0.061642\n",
      ">> Epoch 107 finished \tANN training loss 0.060498\n",
      ">> Epoch 108 finished \tANN training loss 0.059434\n",
      ">> Epoch 109 finished \tANN training loss 0.058566\n",
      ">> Epoch 110 finished \tANN training loss 0.057644\n",
      ">> Epoch 111 finished \tANN training loss 0.056763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 112 finished \tANN training loss 0.055864\n",
      ">> Epoch 113 finished \tANN training loss 0.055002\n",
      ">> Epoch 114 finished \tANN training loss 0.054208\n",
      ">> Epoch 115 finished \tANN training loss 0.053338\n",
      ">> Epoch 116 finished \tANN training loss 0.052618\n",
      ">> Epoch 117 finished \tANN training loss 0.051878\n",
      ">> Epoch 118 finished \tANN training loss 0.051305\n",
      ">> Epoch 119 finished \tANN training loss 0.050551\n",
      ">> Epoch 120 finished \tANN training loss 0.049902\n",
      ">> Epoch 121 finished \tANN training loss 0.049293\n",
      ">> Epoch 122 finished \tANN training loss 0.048715\n",
      ">> Epoch 123 finished \tANN training loss 0.048110\n",
      ">> Epoch 124 finished \tANN training loss 0.047256\n",
      ">> Epoch 125 finished \tANN training loss 0.046128\n",
      ">> Epoch 126 finished \tANN training loss 0.044713\n",
      ">> Epoch 127 finished \tANN training loss 0.042942\n",
      ">> Epoch 128 finished \tANN training loss 0.041175\n",
      ">> Epoch 129 finished \tANN training loss 0.039386\n",
      ">> Epoch 130 finished \tANN training loss 0.037709\n",
      ">> Epoch 131 finished \tANN training loss 0.035199\n",
      ">> Epoch 132 finished \tANN training loss 0.033684\n",
      ">> Epoch 133 finished \tANN training loss 0.032231\n",
      ">> Epoch 134 finished \tANN training loss 0.030841\n",
      ">> Epoch 135 finished \tANN training loss 0.029442\n",
      ">> Epoch 136 finished \tANN training loss 0.028130\n",
      ">> Epoch 137 finished \tANN training loss 0.026401\n",
      ">> Epoch 138 finished \tANN training loss 0.025340\n",
      ">> Epoch 139 finished \tANN training loss 0.024366\n",
      ">> Epoch 140 finished \tANN training loss 0.023490\n",
      ">> Epoch 141 finished \tANN training loss 0.022297\n",
      ">> Epoch 142 finished \tANN training loss 0.021525\n",
      ">> Epoch 143 finished \tANN training loss 0.020832\n",
      ">> Epoch 144 finished \tANN training loss 0.020200\n",
      ">> Epoch 145 finished \tANN training loss 0.019565\n",
      ">> Epoch 146 finished \tANN training loss 0.018819\n",
      ">> Epoch 147 finished \tANN training loss 0.018227\n",
      ">> Epoch 148 finished \tANN training loss 0.017727\n",
      ">> Epoch 149 finished \tANN training loss 0.017229\n",
      ">> Epoch 150 finished \tANN training loss 0.016757\n",
      ">> Epoch 151 finished \tANN training loss 0.016323\n",
      ">> Epoch 152 finished \tANN training loss 0.015906\n",
      ">> Epoch 153 finished \tANN training loss 0.015489\n",
      ">> Epoch 154 finished \tANN training loss 0.015101\n",
      ">> Epoch 155 finished \tANN training loss 0.014719\n",
      ">> Epoch 156 finished \tANN training loss 0.014363\n",
      ">> Epoch 157 finished \tANN training loss 0.013978\n",
      ">> Epoch 158 finished \tANN training loss 0.013620\n",
      ">> Epoch 159 finished \tANN training loss 0.013286\n",
      ">> Epoch 160 finished \tANN training loss 0.012989\n",
      ">> Epoch 161 finished \tANN training loss 0.012614\n",
      ">> Epoch 162 finished \tANN training loss 0.012293\n",
      ">> Epoch 163 finished \tANN training loss 0.011994\n",
      ">> Epoch 164 finished \tANN training loss 0.011741\n",
      ">> Epoch 165 finished \tANN training loss 0.011484\n",
      ">> Epoch 166 finished \tANN training loss 0.011220\n",
      ">> Epoch 167 finished \tANN training loss 0.010976\n",
      ">> Epoch 168 finished \tANN training loss 0.010739\n",
      ">> Epoch 169 finished \tANN training loss 0.010498\n",
      ">> Epoch 170 finished \tANN training loss 0.010262\n",
      ">> Epoch 171 finished \tANN training loss 0.010035\n",
      ">> Epoch 172 finished \tANN training loss 0.009794\n",
      ">> Epoch 173 finished \tANN training loss 0.009578\n",
      ">> Epoch 174 finished \tANN training loss 0.009369\n",
      ">> Epoch 175 finished \tANN training loss 0.009156\n",
      ">> Epoch 176 finished \tANN training loss 0.008973\n",
      ">> Epoch 177 finished \tANN training loss 0.008787\n",
      ">> Epoch 178 finished \tANN training loss 0.008595\n",
      ">> Epoch 179 finished \tANN training loss 0.008417\n",
      ">> Epoch 180 finished \tANN training loss 0.008244\n",
      ">> Epoch 181 finished \tANN training loss 0.008073\n",
      ">> Epoch 182 finished \tANN training loss 0.007915\n",
      ">> Epoch 183 finished \tANN training loss 0.007750\n",
      ">> Epoch 184 finished \tANN training loss 0.007599\n",
      ">> Epoch 185 finished \tANN training loss 0.007434\n",
      ">> Epoch 186 finished \tANN training loss 0.007281\n",
      ">> Epoch 187 finished \tANN training loss 0.007138\n",
      ">> Epoch 188 finished \tANN training loss 0.006991\n",
      ">> Epoch 189 finished \tANN training loss 0.006838\n",
      ">> Epoch 190 finished \tANN training loss 0.006681\n",
      ">> Epoch 191 finished \tANN training loss 0.006542\n",
      ">> Epoch 192 finished \tANN training loss 0.006399\n",
      ">> Epoch 193 finished \tANN training loss 0.006265\n",
      ">> Epoch 194 finished \tANN training loss 0.006140\n",
      ">> Epoch 195 finished \tANN training loss 0.005998\n",
      ">> Epoch 196 finished \tANN training loss 0.005869\n",
      ">> Epoch 197 finished \tANN training loss 0.005731\n",
      ">> Epoch 198 finished \tANN training loss 0.005596\n",
      ">> Epoch 199 finished \tANN training loss 0.005476\n",
      ">> Epoch 200 finished \tANN training loss 0.005358\n",
      ">> Epoch 201 finished \tANN training loss 0.005236\n",
      ">> Epoch 202 finished \tANN training loss 0.005115\n",
      ">> Epoch 203 finished \tANN training loss 0.005003\n",
      ">> Epoch 204 finished \tANN training loss 0.004882\n",
      ">> Epoch 205 finished \tANN training loss 0.004764\n",
      ">> Epoch 206 finished \tANN training loss 0.004658\n",
      ">> Epoch 207 finished \tANN training loss 0.004550\n",
      ">> Epoch 208 finished \tANN training loss 0.004437\n",
      ">> Epoch 209 finished \tANN training loss 0.004332\n",
      ">> Epoch 210 finished \tANN training loss 0.004240\n",
      ">> Epoch 211 finished \tANN training loss 0.004148\n",
      ">> Epoch 212 finished \tANN training loss 0.004053\n",
      ">> Epoch 213 finished \tANN training loss 0.003960\n",
      ">> Epoch 214 finished \tANN training loss 0.003864\n",
      ">> Epoch 215 finished \tANN training loss 0.003775\n",
      ">> Epoch 216 finished \tANN training loss 0.003692\n",
      ">> Epoch 217 finished \tANN training loss 0.003610\n",
      ">> Epoch 218 finished \tANN training loss 0.003518\n",
      ">> Epoch 219 finished \tANN training loss 0.003422\n",
      ">> Epoch 220 finished \tANN training loss 0.003342\n",
      ">> Epoch 221 finished \tANN training loss 0.003263\n",
      ">> Epoch 222 finished \tANN training loss 0.003182\n",
      ">> Epoch 223 finished \tANN training loss 0.003099\n",
      ">> Epoch 224 finished \tANN training loss 0.003018\n",
      ">> Epoch 225 finished \tANN training loss 0.002946\n",
      ">> Epoch 226 finished \tANN training loss 0.002876\n",
      ">> Epoch 227 finished \tANN training loss 0.002804\n",
      ">> Epoch 228 finished \tANN training loss 0.002738\n",
      ">> Epoch 229 finished \tANN training loss 0.002676\n",
      ">> Epoch 230 finished \tANN training loss 0.002616\n",
      ">> Epoch 231 finished \tANN training loss 0.002557\n",
      ">> Epoch 232 finished \tANN training loss 0.002497\n",
      ">> Epoch 233 finished \tANN training loss 0.002435\n",
      ">> Epoch 234 finished \tANN training loss 0.002372\n",
      ">> Epoch 235 finished \tANN training loss 0.002318\n",
      ">> Epoch 236 finished \tANN training loss 0.002264\n",
      ">> Epoch 237 finished \tANN training loss 0.002209\n",
      ">> Epoch 238 finished \tANN training loss 0.002155\n",
      ">> Epoch 239 finished \tANN training loss 0.002108\n",
      ">> Epoch 240 finished \tANN training loss 0.002062\n",
      ">> Epoch 241 finished \tANN training loss 0.002010\n",
      ">> Epoch 242 finished \tANN training loss 0.001964\n",
      ">> Epoch 243 finished \tANN training loss 0.001920\n",
      ">> Epoch 244 finished \tANN training loss 0.001870\n",
      ">> Epoch 245 finished \tANN training loss 0.001826\n",
      ">> Epoch 246 finished \tANN training loss 0.001779\n",
      ">> Epoch 247 finished \tANN training loss 0.001738\n",
      ">> Epoch 248 finished \tANN training loss 0.001700\n",
      ">> Epoch 249 finished \tANN training loss 0.001657\n",
      ">> Epoch 250 finished \tANN training loss 0.001620\n",
      ">> Epoch 251 finished \tANN training loss 0.001584\n",
      ">> Epoch 252 finished \tANN training loss 0.001550\n",
      ">> Epoch 253 finished \tANN training loss 0.001513\n",
      ">> Epoch 254 finished \tANN training loss 0.001480\n",
      ">> Epoch 255 finished \tANN training loss 0.001449\n",
      ">> Epoch 256 finished \tANN training loss 0.001416\n",
      ">> Epoch 257 finished \tANN training loss 0.001384\n",
      ">> Epoch 258 finished \tANN training loss 0.001355\n",
      ">> Epoch 259 finished \tANN training loss 0.001326\n",
      ">> Epoch 260 finished \tANN training loss 0.001297\n",
      ">> Epoch 261 finished \tANN training loss 0.001272\n",
      ">> Epoch 262 finished \tANN training loss 0.001247\n",
      ">> Epoch 263 finished \tANN training loss 0.001222\n",
      ">> Epoch 264 finished \tANN training loss 0.001200\n",
      ">> Epoch 265 finished \tANN training loss 0.001179\n",
      ">> Epoch 266 finished \tANN training loss 0.001156\n",
      ">> Epoch 267 finished \tANN training loss 0.001133\n",
      ">> Epoch 268 finished \tANN training loss 0.001113\n",
      ">> Epoch 269 finished \tANN training loss 0.001093\n",
      ">> Epoch 270 finished \tANN training loss 0.001073\n",
      ">> Epoch 271 finished \tANN training loss 0.001053\n",
      ">> Epoch 272 finished \tANN training loss 0.001035\n",
      ">> Epoch 273 finished \tANN training loss 0.001017\n",
      ">> Epoch 274 finished \tANN training loss 0.000997\n",
      ">> Epoch 275 finished \tANN training loss 0.000978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 276 finished \tANN training loss 0.000962\n",
      ">> Epoch 277 finished \tANN training loss 0.000946\n",
      ">> Epoch 278 finished \tANN training loss 0.000929\n",
      ">> Epoch 279 finished \tANN training loss 0.000913\n",
      ">> Epoch 280 finished \tANN training loss 0.000898\n",
      ">> Epoch 281 finished \tANN training loss 0.000881\n",
      ">> Epoch 282 finished \tANN training loss 0.000866\n",
      ">> Epoch 283 finished \tANN training loss 0.000853\n",
      ">> Epoch 284 finished \tANN training loss 0.000840\n",
      ">> Epoch 285 finished \tANN training loss 0.000827\n",
      ">> Epoch 286 finished \tANN training loss 0.000815\n",
      ">> Epoch 287 finished \tANN training loss 0.000803\n",
      ">> Epoch 288 finished \tANN training loss 0.000791\n",
      ">> Epoch 289 finished \tANN training loss 0.000779\n",
      ">> Epoch 290 finished \tANN training loss 0.000767\n",
      ">> Epoch 291 finished \tANN training loss 0.000754\n",
      ">> Epoch 292 finished \tANN training loss 0.000743\n",
      ">> Epoch 293 finished \tANN training loss 0.000731\n",
      ">> Epoch 294 finished \tANN training loss 0.000719\n",
      ">> Epoch 295 finished \tANN training loss 0.000708\n",
      ">> Epoch 296 finished \tANN training loss 0.000698\n",
      ">> Epoch 297 finished \tANN training loss 0.000688\n",
      ">> Epoch 298 finished \tANN training loss 0.000679\n",
      ">> Epoch 299 finished \tANN training loss 0.000670\n",
      ">> Epoch 300 finished \tANN training loss 0.000661\n",
      ">> Epoch 301 finished \tANN training loss 0.000652\n",
      ">> Epoch 302 finished \tANN training loss 0.000643\n",
      ">> Epoch 303 finished \tANN training loss 0.000635\n",
      ">> Epoch 304 finished \tANN training loss 0.000627\n",
      ">> Epoch 305 finished \tANN training loss 0.000618\n",
      ">> Epoch 306 finished \tANN training loss 0.000611\n",
      ">> Epoch 307 finished \tANN training loss 0.000602\n",
      ">> Epoch 308 finished \tANN training loss 0.000594\n",
      ">> Epoch 309 finished \tANN training loss 0.000587\n",
      ">> Epoch 310 finished \tANN training loss 0.000580\n",
      ">> Epoch 311 finished \tANN training loss 0.000572\n",
      ">> Epoch 312 finished \tANN training loss 0.000564\n",
      ">> Epoch 313 finished \tANN training loss 0.000557\n",
      ">> Epoch 314 finished \tANN training loss 0.000550\n",
      ">> Epoch 315 finished \tANN training loss 0.000544\n",
      ">> Epoch 316 finished \tANN training loss 0.000536\n",
      ">> Epoch 317 finished \tANN training loss 0.000530\n",
      ">> Epoch 318 finished \tANN training loss 0.000524\n",
      ">> Epoch 319 finished \tANN training loss 0.000517\n",
      ">> Epoch 320 finished \tANN training loss 0.000511\n",
      ">> Epoch 321 finished \tANN training loss 0.000506\n",
      ">> Epoch 322 finished \tANN training loss 0.000500\n",
      ">> Epoch 323 finished \tANN training loss 0.000494\n",
      ">> Epoch 324 finished \tANN training loss 0.000488\n",
      ">> Epoch 325 finished \tANN training loss 0.000483\n",
      ">> Epoch 326 finished \tANN training loss 0.000478\n",
      ">> Epoch 327 finished \tANN training loss 0.000473\n",
      ">> Epoch 328 finished \tANN training loss 0.000468\n",
      ">> Epoch 329 finished \tANN training loss 0.000463\n",
      ">> Epoch 330 finished \tANN training loss 0.000458\n",
      ">> Epoch 331 finished \tANN training loss 0.000453\n",
      ">> Epoch 332 finished \tANN training loss 0.000449\n",
      ">> Epoch 333 finished \tANN training loss 0.000444\n",
      ">> Epoch 334 finished \tANN training loss 0.000440\n",
      ">> Epoch 335 finished \tANN training loss 0.000435\n",
      ">> Epoch 336 finished \tANN training loss 0.000431\n",
      ">> Epoch 337 finished \tANN training loss 0.000426\n",
      ">> Epoch 338 finished \tANN training loss 0.000422\n",
      ">> Epoch 339 finished \tANN training loss 0.000418\n",
      ">> Epoch 340 finished \tANN training loss 0.000414\n",
      ">> Epoch 341 finished \tANN training loss 0.000410\n",
      ">> Epoch 342 finished \tANN training loss 0.000407\n",
      ">> Epoch 343 finished \tANN training loss 0.000403\n",
      ">> Epoch 344 finished \tANN training loss 0.000399\n",
      ">> Epoch 345 finished \tANN training loss 0.000395\n",
      ">> Epoch 346 finished \tANN training loss 0.000392\n",
      ">> Epoch 347 finished \tANN training loss 0.000388\n",
      ">> Epoch 348 finished \tANN training loss 0.000385\n",
      ">> Epoch 349 finished \tANN training loss 0.000381\n",
      ">> Epoch 350 finished \tANN training loss 0.000378\n",
      ">> Epoch 351 finished \tANN training loss 0.000375\n",
      ">> Epoch 352 finished \tANN training loss 0.000371\n",
      ">> Epoch 353 finished \tANN training loss 0.000368\n",
      ">> Epoch 354 finished \tANN training loss 0.000365\n",
      ">> Epoch 355 finished \tANN training loss 0.000362\n",
      ">> Epoch 356 finished \tANN training loss 0.000359\n",
      ">> Epoch 357 finished \tANN training loss 0.000356\n",
      ">> Epoch 358 finished \tANN training loss 0.000353\n",
      ">> Epoch 359 finished \tANN training loss 0.000350\n",
      ">> Epoch 360 finished \tANN training loss 0.000347\n",
      ">> Epoch 361 finished \tANN training loss 0.000344\n",
      ">> Epoch 362 finished \tANN training loss 0.000341\n",
      ">> Epoch 363 finished \tANN training loss 0.000338\n",
      ">> Epoch 364 finished \tANN training loss 0.000336\n",
      ">> Epoch 365 finished \tANN training loss 0.000333\n",
      ">> Epoch 366 finished \tANN training loss 0.000330\n",
      ">> Epoch 367 finished \tANN training loss 0.000327\n",
      ">> Epoch 368 finished \tANN training loss 0.000324\n",
      ">> Epoch 369 finished \tANN training loss 0.000322\n",
      ">> Epoch 370 finished \tANN training loss 0.000319\n",
      ">> Epoch 371 finished \tANN training loss 0.000317\n",
      ">> Epoch 372 finished \tANN training loss 0.000315\n",
      ">> Epoch 373 finished \tANN training loss 0.000312\n",
      ">> Epoch 374 finished \tANN training loss 0.000310\n",
      ">> Epoch 375 finished \tANN training loss 0.000307\n",
      ">> Epoch 376 finished \tANN training loss 0.000305\n",
      ">> Epoch 377 finished \tANN training loss 0.000303\n",
      ">> Epoch 378 finished \tANN training loss 0.000300\n",
      ">> Epoch 379 finished \tANN training loss 0.000298\n",
      ">> Epoch 380 finished \tANN training loss 0.000296\n",
      ">> Epoch 381 finished \tANN training loss 0.000293\n",
      ">> Epoch 382 finished \tANN training loss 0.000291\n",
      ">> Epoch 383 finished \tANN training loss 0.000289\n",
      ">> Epoch 384 finished \tANN training loss 0.000287\n",
      ">> Epoch 385 finished \tANN training loss 0.000285\n",
      ">> Epoch 386 finished \tANN training loss 0.000283\n",
      ">> Epoch 387 finished \tANN training loss 0.000281\n",
      ">> Epoch 388 finished \tANN training loss 0.000279\n",
      ">> Epoch 389 finished \tANN training loss 0.000277\n",
      ">> Epoch 390 finished \tANN training loss 0.000275\n",
      ">> Epoch 391 finished \tANN training loss 0.000273\n",
      ">> Epoch 392 finished \tANN training loss 0.000272\n",
      ">> Epoch 393 finished \tANN training loss 0.000270\n",
      ">> Epoch 394 finished \tANN training loss 0.000268\n",
      ">> Epoch 395 finished \tANN training loss 0.000266\n",
      ">> Epoch 396 finished \tANN training loss 0.000264\n",
      ">> Epoch 397 finished \tANN training loss 0.000262\n",
      ">> Epoch 398 finished \tANN training loss 0.000260\n",
      ">> Epoch 399 finished \tANN training loss 0.000259\n",
      ">> Epoch 400 finished \tANN training loss 0.000257\n",
      ">> Epoch 401 finished \tANN training loss 0.000255\n",
      ">> Epoch 402 finished \tANN training loss 0.000254\n",
      ">> Epoch 403 finished \tANN training loss 0.000252\n",
      ">> Epoch 404 finished \tANN training loss 0.000250\n",
      ">> Epoch 405 finished \tANN training loss 0.000249\n",
      ">> Epoch 406 finished \tANN training loss 0.000247\n",
      ">> Epoch 407 finished \tANN training loss 0.000246\n",
      ">> Epoch 408 finished \tANN training loss 0.000244\n",
      ">> Epoch 409 finished \tANN training loss 0.000243\n",
      ">> Epoch 410 finished \tANN training loss 0.000241\n",
      ">> Epoch 411 finished \tANN training loss 0.000240\n",
      ">> Epoch 412 finished \tANN training loss 0.000238\n",
      ">> Epoch 413 finished \tANN training loss 0.000237\n",
      ">> Epoch 414 finished \tANN training loss 0.000235\n",
      ">> Epoch 415 finished \tANN training loss 0.000234\n",
      ">> Epoch 416 finished \tANN training loss 0.000232\n",
      ">> Epoch 417 finished \tANN training loss 0.000231\n",
      ">> Epoch 418 finished \tANN training loss 0.000230\n",
      ">> Epoch 419 finished \tANN training loss 0.000228\n",
      ">> Epoch 420 finished \tANN training loss 0.000227\n",
      ">> Epoch 421 finished \tANN training loss 0.000225\n",
      ">> Epoch 422 finished \tANN training loss 0.000224\n",
      ">> Epoch 423 finished \tANN training loss 0.000222\n",
      ">> Epoch 424 finished \tANN training loss 0.000221\n",
      ">> Epoch 425 finished \tANN training loss 0.000220\n",
      ">> Epoch 426 finished \tANN training loss 0.000219\n",
      ">> Epoch 427 finished \tANN training loss 0.000217\n",
      ">> Epoch 428 finished \tANN training loss 0.000216\n",
      ">> Epoch 429 finished \tANN training loss 0.000215\n",
      ">> Epoch 430 finished \tANN training loss 0.000214\n",
      ">> Epoch 431 finished \tANN training loss 0.000213\n",
      ">> Epoch 432 finished \tANN training loss 0.000211\n",
      ">> Epoch 433 finished \tANN training loss 0.000210\n",
      ">> Epoch 434 finished \tANN training loss 0.000209\n",
      ">> Epoch 435 finished \tANN training loss 0.000208\n",
      ">> Epoch 436 finished \tANN training loss 0.000207\n",
      ">> Epoch 437 finished \tANN training loss 0.000205\n",
      ">> Epoch 438 finished \tANN training loss 0.000204\n",
      ">> Epoch 439 finished \tANN training loss 0.000203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 440 finished \tANN training loss 0.000202\n",
      ">> Epoch 441 finished \tANN training loss 0.000201\n",
      ">> Epoch 442 finished \tANN training loss 0.000200\n",
      ">> Epoch 443 finished \tANN training loss 0.000199\n",
      ">> Epoch 444 finished \tANN training loss 0.000198\n",
      ">> Epoch 445 finished \tANN training loss 0.000197\n",
      ">> Epoch 446 finished \tANN training loss 0.000196\n",
      ">> Epoch 447 finished \tANN training loss 0.000194\n",
      ">> Epoch 448 finished \tANN training loss 0.000193\n",
      ">> Epoch 449 finished \tANN training loss 0.000192\n",
      ">> Epoch 450 finished \tANN training loss 0.000191\n",
      ">> Epoch 451 finished \tANN training loss 0.000190\n",
      ">> Epoch 452 finished \tANN training loss 0.000189\n",
      ">> Epoch 453 finished \tANN training loss 0.000188\n",
      ">> Epoch 454 finished \tANN training loss 0.000187\n",
      ">> Epoch 455 finished \tANN training loss 0.000186\n",
      ">> Epoch 456 finished \tANN training loss 0.000185\n",
      ">> Epoch 457 finished \tANN training loss 0.000184\n",
      ">> Epoch 458 finished \tANN training loss 0.000184\n",
      ">> Epoch 459 finished \tANN training loss 0.000183\n",
      ">> Epoch 460 finished \tANN training loss 0.000182\n",
      ">> Epoch 461 finished \tANN training loss 0.000181\n",
      ">> Epoch 462 finished \tANN training loss 0.000180\n",
      ">> Epoch 463 finished \tANN training loss 0.000179\n",
      ">> Epoch 464 finished \tANN training loss 0.000178\n",
      ">> Epoch 465 finished \tANN training loss 0.000177\n",
      ">> Epoch 466 finished \tANN training loss 0.000177\n",
      ">> Epoch 467 finished \tANN training loss 0.000176\n",
      ">> Epoch 468 finished \tANN training loss 0.000175\n",
      ">> Epoch 469 finished \tANN training loss 0.000174\n",
      ">> Epoch 470 finished \tANN training loss 0.000173\n",
      ">> Epoch 471 finished \tANN training loss 0.000173\n",
      ">> Epoch 472 finished \tANN training loss 0.000172\n",
      ">> Epoch 473 finished \tANN training loss 0.000171\n",
      ">> Epoch 474 finished \tANN training loss 0.000170\n",
      ">> Epoch 475 finished \tANN training loss 0.000169\n",
      ">> Epoch 476 finished \tANN training loss 0.000168\n",
      ">> Epoch 477 finished \tANN training loss 0.000167\n",
      ">> Epoch 478 finished \tANN training loss 0.000167\n",
      ">> Epoch 479 finished \tANN training loss 0.000166\n",
      ">> Epoch 480 finished \tANN training loss 0.000165\n",
      ">> Epoch 481 finished \tANN training loss 0.000164\n",
      ">> Epoch 482 finished \tANN training loss 0.000164\n",
      ">> Epoch 483 finished \tANN training loss 0.000163\n",
      ">> Epoch 484 finished \tANN training loss 0.000162\n",
      ">> Epoch 485 finished \tANN training loss 0.000161\n",
      ">> Epoch 486 finished \tANN training loss 0.000160\n",
      ">> Epoch 487 finished \tANN training loss 0.000160\n",
      ">> Epoch 488 finished \tANN training loss 0.000159\n",
      ">> Epoch 489 finished \tANN training loss 0.000158\n",
      ">> Epoch 490 finished \tANN training loss 0.000158\n",
      ">> Epoch 491 finished \tANN training loss 0.000157\n",
      ">> Epoch 492 finished \tANN training loss 0.000156\n",
      ">> Epoch 493 finished \tANN training loss 0.000155\n",
      ">> Epoch 494 finished \tANN training loss 0.000155\n",
      ">> Epoch 495 finished \tANN training loss 0.000154\n",
      ">> Epoch 496 finished \tANN training loss 0.000153\n",
      ">> Epoch 497 finished \tANN training loss 0.000153\n",
      ">> Epoch 498 finished \tANN training loss 0.000152\n",
      ">> Epoch 499 finished \tANN training loss 0.000151\n",
      ">> Epoch 500 finished \tANN training loss 0.000151\n",
      ">> Epoch 501 finished \tANN training loss 0.000150\n",
      ">> Epoch 502 finished \tANN training loss 0.000150\n",
      ">> Epoch 503 finished \tANN training loss 0.000149\n",
      ">> Epoch 504 finished \tANN training loss 0.000148\n",
      ">> Epoch 505 finished \tANN training loss 0.000148\n",
      ">> Epoch 506 finished \tANN training loss 0.000147\n",
      ">> Epoch 507 finished \tANN training loss 0.000147\n",
      ">> Epoch 508 finished \tANN training loss 0.000146\n",
      ">> Epoch 509 finished \tANN training loss 0.000146\n",
      ">> Epoch 510 finished \tANN training loss 0.000145\n",
      ">> Epoch 511 finished \tANN training loss 0.000144\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=1e-05,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 4, 3: 3},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 3: 3, 4: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=512,\n",
       "                            verbose=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_test)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "yhat_classes = yhat_probs# np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972973\n",
      "Precision: 0.989130\n",
      "Recall: 0.964286\n",
      "F1 score: 0.975214\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, yhat_classes,average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes, average='macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 0.952986\n",
      "[[22  0  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(Y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "#fprate, tprate, thresholds = roc_curve(Y_test, yhat_probs, average = 'macro')\n",
    "#print('ROC AUC: %f' % thresholds)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 5\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, yhat_classes, pos_label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[22  0  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0  6]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_test,yhat_classes, labels= [0, 1, 3, 4])\n",
    "print('Confusion matrix : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        22\n",
      "           1       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.97        37\n",
      "   macro avg       0.99      0.96      0.98        37\n",
      "weighted avg       0.97      0.97      0.97        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = classification_report(Y_test,yhat_classes, labels= [0, 1, 3,4])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('./models/Without IP address/DBN/DBN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
