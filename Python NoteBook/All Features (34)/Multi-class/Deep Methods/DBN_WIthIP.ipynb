{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "pd.set_option('display.max_columns', None)\n",
    "from dbn.tensorflow import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "## for machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer,StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "## for explainer\n",
    "#from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#b = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "a = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week1.csv', low_memory=False, encoding='cp1252')\n",
    "b = pd.read_csv('./CIDDS-001/traffic/OpenStack/CIDDS-001-internal-week2.csv', low_memory=False, encoding='cp1252')\n",
    "c =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "d =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "e =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('./CIDDS-001/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#c = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week3.csv', low_memory=False , encoding='cp1252')\n",
    "#d = pd.read_csv('drive/My Drive/Colab Notebooks/traffic/OpenStack/CIDDS-001-internal-week4.csv', low_memory=False, encoding='cp1252')\n",
    "#e =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week1.csv', low_memory=False, encoding='cp1252')\n",
    "#f =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week2.csv', low_memory=False, encoding='cp1252')\n",
    "#g =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week3.csv', low_memory=False, encoding='cp1252')\n",
    "#h =  pd.read_csv('drive/My Drive/Colab Notebooks/traffic/ExternalServer/CIDDS-001-external-week4.csv', low_memory=False, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10310733, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1795404, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b.shape)\n",
    "#a.drop(a[a['attackType'] == '---'].index, axis = 0, inplace= True) \n",
    "b.drop(b[b['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "c.drop(c[c['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "d.drop(d[d['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "#e.drop(e[e['attackType'] == '---'].index, axis = 0, inplace= True)  \n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_external = pd.concat([c,d,e], axis = 0)\n",
    "data_external.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to Increment attackID values\n",
    "data_external['attackID'] = data_external['attackID'].apply(lambda x: str(int(x) + 70) if x != '---' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(df):\n",
    "    if 'M' in df:\n",
    "        df = df.split('M')\n",
    "        df = df[0].strip()\n",
    "        df = float(df) * 1000000\n",
    "    elif 'B' in df:\n",
    "        df = df.split('B')\n",
    "        df = df[0].strip()\n",
    "        df =  float(df) * 1000000000\n",
    "    else: \n",
    "        df =float(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([a,b,data_external], axis = 0)\n",
    "data.reset_index(drop= True, inplace= True)\n",
    "data['Bytes'] = data['Bytes'].apply(lambda x: convert_bytes(x))\n",
    "columns = ['Src Pt', 'Dst Pt','Tos','Flows','Packets', 'Bytes']\n",
    "for i in columns:\n",
    "    data[i] = pd.to_numeric(data[i]);\n",
    "del columns\n",
    "del a,b,c,d,e, data_external\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Hexadecimal value to Binary\n",
    "def hex_to_binary(hexdata):\n",
    "    scale = 16 ## equals to hexadecimal\n",
    "    num_of_bits = 9\n",
    "    return bin(int(hexdata, scale))[2:].zfill(num_of_bits);\n",
    "#Converts TCP flags to Binary\n",
    "def to_Binary(x):\n",
    "    l = 0\n",
    "    x = '...' + x\n",
    "    x = list(x)\n",
    "    for i in x:\n",
    "        if (i=='.'):\n",
    "            x[l]= '0'\n",
    "        else:\n",
    "            x[l] = '1'\n",
    "        l = l +1\n",
    "    return ''.join(x)\n",
    "#Converts the 'Flags' column to 9 indiviual columns (manual oneshot encoding)\n",
    "def flag_convert(df):  \n",
    "   # df['Flags'] = df['Flags'].apply(lambda x: (list(x)))\n",
    "   # temp = df['Flags'].apply(lambda x: toBinary(x))\n",
    "    hex_values = list(df[(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    flag_values = list(df[~(df['Flags'].str.contains(\"0x\", na=False))]['Flags'].unique())\n",
    "    binary_values = {}\n",
    "    for i in hex_values:\n",
    "         binary_values[i] = (hex_to_binary(i))\n",
    "    for i in flag_values:\n",
    "         binary_values[i] = (to_Binary(i))\n",
    "    temp = df['Flags'].replace(binary_values)\n",
    "#temp = temp.apply(lambda x: pd.Series(x)) \n",
    "    temp = pd.DataFrame(temp.apply(list).tolist())\n",
    "#temp = pd.DataFrame(temp)\n",
    "#a = a.iloc[: , 1:]\n",
    "   # print(temp.head())\n",
    "    temp.columns = ['N','C','E','U' ,'A','P','R','S','F']\n",
    "    for i in temp.columns:\n",
    "        temp[i] = pd.to_numeric(temp[i]);\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a IP_pairs \n",
    "def make_pair(df):\n",
    "    ip_pair = df['Src IP Addr'] +'/' +df['Dst IP Addr']\n",
    "    source_ip = df['Src IP Addr'].unique().tolist()\n",
    "    destination_ip = df['Dst IP Addr'].unique().tolist()\n",
    "   # df = df.drop(columns = ['Src IP Addr', 'Dst IP Addr'])\n",
    "    df.insert(1, ' IP Pair', ip_pair)\n",
    "    return df\n",
    "\n",
    "def check_inverse(df):\n",
    "    list_pairs = df[' IP Pair'].unique()\n",
    "    tuple_pair = []\n",
    "    for i in list_pairs:\n",
    "        tuple_pair.append(tuple((i.split('/'))))\n",
    "    dic_store = {}\n",
    "    for i in tuple_pair:\n",
    "        if (i  not in dic_store.keys()) and (i[::-1] not in dic_store.keys()):\n",
    "            dic_store[i] = i[0] + '/' +i[1]\n",
    "    print(len(dic_store.keys()))\n",
    "    dic_final = {}\n",
    "    for i in dic_store.keys():\n",
    "        dic_final[i[0] + '/' +i[1]] = dic_store[i]\n",
    "        dic_final[i[1] + '/' +i[0]] = dic_store[i]\n",
    "    df[' IP Pair'] = df[' IP Pair'].map(dic_final)               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_IP(df):\n",
    "    columns = ['sourceIP_feature 1', 'sourceIP_feature 2', 'sourceIP_feature 3', 'sourceIP_feature 4', 'destIP_feature 1',\n",
    "              'destIP_feature 2', 'destIP_feature 3', 'destIP_feature 4']\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    columns = data.select_dtypes(include=numerics).columns\n",
    "    normalized = df[columns]\n",
    "    print(columns)\n",
    "    transformed = MinMaxScaler().fit(normalized).transform(normalized)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    j = 0\n",
    "    col = {}\n",
    "    for i in columns:\n",
    "        col[j] = i\n",
    "        j=j+1\n",
    "    transformed = transformed.rename(columns = col)\n",
    "    transformed = transformed.reset_index()\n",
    "    for i in columns:\n",
    "        df[i] = transformed[i].to_numpy()\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    #df.astype({'attackType': 'str'})\n",
    "    df['attackType'] = label_encoder.fit_transform(df['attackType'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    \n",
    "    df['sourceIP_feature 1'] = label_encoder.fit_transform(df['sourceIP_feature 1'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['sourceIP_feature 2'] = label_encoder.fit_transform(df['sourceIP_feature 2'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['sourceIP_feature 3'] = label_encoder.fit_transform(df['sourceIP_feature 3'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['sourceIP_feature 4'] = label_encoder.fit_transform(df['sourceIP_feature 4'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['destIP_feature 1'] = label_encoder.fit_transform(df['destIP_feature 1'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['destIP_feature 2'] = label_encoder.fit_transform(df['destIP_feature 2'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['destIP_feature 3'] = label_encoder.fit_transform(df['destIP_feature 3'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['destIP_feature 4'] = label_encoder.fit_transform(df['destIP_feature 4'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    df['Proto'] = label_encoder.fit_transform(df['Proto'])\n",
    "    print(list(label_encoder.classes_))\n",
    "    print(list(label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    onehot_encoder1 = OneHotEncoder()\n",
    "    onehot_encoder1.fit(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = onehot_encoder1.transform(df.Proto.to_numpy().reshape(-1, 1))\n",
    "    proto = pd.DataFrame.sparse.from_spmatrix(proto)\n",
    "    proto.astype('int32')\n",
    "    proto.columns = label_encoder.classes_\n",
    "   # print(proto.head(1))\n",
    "    df = pd.concat([df, proto], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(columns = ['Date first seen', ' IP Pair', 'Flows', 'class', 'attackID','Flags',\n",
    "                              'attackDescription', 'Src IP Addr', 'Dst IP Addr','Proto'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplit IP address into features, 7 features\n",
    "def split_to_net(IP_address):\n",
    "    IP_list = IP_address.split(\".\")\n",
    "    needed_len = 7\n",
    "    needed_len = needed_len - len(IP_list)\n",
    "    for i in range(0,needed_len,1):\n",
    "        IP_list.append('0')\n",
    "    return IP_list\n",
    "#replace unknown IP address, and convert to columns\n",
    "def IP_split(df): \n",
    "    replace = {\"ATTACKER1\":\"0.0.0.0\",\n",
    "           \"ATTACKER2\":\"0.0.0.0\",\n",
    "           \"ATTACKER3\":\"0.0.0.0\",\n",
    "           \"EXT_SERVER\": \"0.0.0.0.1\",\n",
    "          \"OPENSTACK_NET\": \"0.0.0.0.0.1\",\n",
    "          \"DNS\": \"0.0.0.0.0.0.1\"}\n",
    "    df = df.replace({\"Src IP Addr\": replace, \"Dst IP Addr\": replace}, value=None)\n",
    "    temp_source = df[\"Src IP Addr\"].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "    temp_des = df['Dst IP Addr'].apply(lambda x: \"0.0.0.0.0.0.0\" if ('_') in x else x)\n",
    "   # sourceIP = list(df[\"Src IP Addr\"].unique())\n",
    "   # destIP = list(df[\"Dst IP Addr\"].unique())\n",
    "   # sourceIP_values = {}\n",
    "   # desIP_values = {}\n",
    "   # for i in sourceIP:\n",
    "   #      sourceIP_values[i] = (split_to_net(i))\n",
    "   # for i in destIP:\n",
    "   #      desIP_values[i] = (split_to_net(i))\n",
    "    #print(sourceIP_values)\n",
    "   # print(desIP_values)\n",
    "#for Source IP\n",
    "    temp_source = temp_source.apply(lambda x: split_to_net(x) )\n",
    "    temp_source = pd.DataFrame(temp_source.apply(list).tolist())\n",
    "    temp_source.columns = ['sourceIP_feature 1','sourceIP_feature 2','sourceIP_feature 3','sourceIP_feature 4' ,\n",
    "                    'sourceEXT_SERVER','sourceOPENSTACK_NET','sourceDNS']\n",
    "    for i in temp_source.columns:\n",
    "        temp_source[i] = pd.to_numeric(temp_source[i]);\n",
    "    temp_source = temp_source.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_source], axis = 1)\n",
    "    #for Destination IP\n",
    "    temp_des = temp_des.apply(lambda x: split_to_net(x) )\n",
    "    temp_des = pd.DataFrame(temp_des.apply(list).tolist())\n",
    "    temp_des.columns = ['destIP_feature 1','destIP_feature 2','destIP_feature 3','destIP_feature 4' ,\n",
    "                    'destEXT_SERVER','destOPENSTACK_NET','destDNS']\n",
    "   # for i in temp_des.columns:\n",
    "       # temp_des[i] = pd.to_numeric(temp_des[i]);\n",
    "    temp_des = temp_des.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = pd.concat([df, temp_des], axis = 1)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59362\n"
     ]
    }
   ],
   "source": [
    "data = make_pair(data)\n",
    "data = check_inverse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = IP_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Duration', 'Src Pt', 'Dst Pt', 'Packets', 'Bytes', 'Flows', 'Tos',\n",
      "       'sourceIP_feature 1', 'sourceIP_feature 2', 'sourceIP_feature 3',\n",
      "       'sourceIP_feature 4', 'sourceEXT_SERVER', 'sourceOPENSTACK_NET',\n",
      "       'sourceDNS'],\n",
      "      dtype='object')\n",
      "['---', 'bruteForce', 'dos', 'pingScan', 'portScan']\n",
      "[0, 1, 2, 3, 4]\n",
      "[0.0, 1.0]\n",
      "[0, 1]\n",
      "[0.0, 1.0]\n",
      "[0, 1]\n",
      "[0.0, 0.003952569169960474, 0.01976284584980237, 0.3162055335968379, 0.3952569169960474, 0.7905138339920948, 0.8300395256916996, 0.8695652173913043, 0.9999999999999999]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[0.0, 0.004, 0.008, 0.012, 0.016, 0.02, 0.024, 0.028, 0.032, 0.036000000000000004, 0.04, 0.044, 0.048, 0.052000000000000005, 0.056, 0.06, 0.064, 0.4, 0.432, 0.456, 0.612, 0.884, 0.96, 1.0]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "['0', '192', '255']\n",
      "[0, 1, 2]\n",
      "['0', '168', '255']\n",
      "[0, 1, 2]\n",
      "['0', '1', '100', '200', '210', '220', '253', '255']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "['0', '1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '14', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '20', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '21', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '22', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '24', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\n",
      "['GRE  ', 'ICMP ', 'IGMP ', 'TCP  ', 'UDP  ']\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sourceIP_feature 1', 'sourceIP_feature 2', 'sourceIP_feature 3', 'sourceIP_feature 4', 'destIP_feature 1', 'destIP_feature 2', 'destIP_feature 3', 'destIP_feature 4']\n"
     ]
    }
   ],
   "source": [
    "data = normalize(data)\n",
    "data =  one_shot(data) \n",
    "data = normalize_IP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def unix_time(df):\n",
    "  #  df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x + ':00' if len(x) != 19 else x)\n",
    "   # df[' Timestamp'] = df[' Timestamp'].apply(lambda x: x[0 : 5 : ] + x[7 : :] if len(x) != 19 else x[0 : 7 : ] + x[9 : :])\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "    df['Date first seen'] = df['Date first seen'].apply(lambda x: x.timestamp()*1000)\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_profile(grouped):\n",
    "    grouped['---'] = unix_time(grouped['---'])\n",
    "    start_time = int(grouped['---'].head(1)['Date first seen'].values[0])\n",
    "    end_time = int(grouped['---'].tail(1)['Date first seen'].values[0])\n",
    "#date_bins = pd.IntervalIndex.from_tuples(\n",
    "#        [(i, i+3600000) for i in range(start_time, end_time, 3600000)],\n",
    "#        closed=\"left\")\n",
    "#date_labels = [f\"{i}\" for i in range(1, len(date_bins)+1, 1)]\n",
    "    normal_data = dict(tuple( grouped['---'].groupby( pd.cut(\n",
    "            grouped['---']['Date first seen'],\n",
    "               np.arange(start_time, end_time, 3*3600000)))))\n",
    "    del grouped['---']\n",
    "    num = []\n",
    "    for i in grouped_data.keys():\n",
    "          num.append(len(grouped_data[i]))\n",
    "    print(min(num))\n",
    "    num = max(num)\n",
    "    print(num)\n",
    "    print(len(grouped.keys()))\n",
    "    grouped = {**grouped, **normal_data}\n",
    "    print(len(grouped.keys()))\n",
    "    return grouped, num;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data= dict(tuple(data.groupby(['attackID'])))\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---: 7195669 : 0\n",
      "Attack ID: 25; Lenght of Attack: 201; Attack Type: 1\n",
      "Attack ID: 27; Lenght of Attack: 680; Attack Type: 1\n",
      "Attack ID: 30; Lenght of Attack: 46; Attack Type: 1\n",
      "Attack ID: 32; Lenght of Attack: 335; Attack Type: 1\n",
      "Attack ID: 39; Lenght of Attack: 364; Attack Type: 1\n",
      "Attack ID: 54; Lenght of Attack: 183; Attack Type: 1\n",
      "Attack ID: 55; Lenght of Attack: 757; Attack Type: 1\n",
      "Attack ID: 56; Lenght of Attack: 427; Attack Type: 1\n",
      "Attack ID: 61; Lenght of Attack: 705; Attack Type: 1\n",
      "Attack ID: 62; Lenght of Attack: 574; Attack Type: 1\n",
      "Attack ID: 64; Lenght of Attack: 480; Attack Type: 1\n",
      "Attack ID: 70; Lenght of Attack: 240; Attack Type: 1\n",
      "Attack ID: 73; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 74; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 75; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 76; Lenght of Attack: 168; Attack Type: 1\n",
      "Attack ID: 78; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 79; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 81; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 83; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 84; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 87; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 88; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 90; Lenght of Attack: 200; Attack Type: 1\n",
      "Attack ID: 91; Lenght of Attack: 40; Attack Type: 1\n",
      "Attack ID: 92; Lenght of Attack: 40; Attack Type: 1\n",
      "dos : 18\n",
      "Attack ID: 16; Lenght of Attack: 261003; Attack Type: 2\n",
      "Attack ID: 18; Lenght of Attack: 295302; Attack Type: 2\n",
      "Attack ID: 23; Lenght of Attack: 72788; Attack Type: 2\n",
      "Attack ID: 26; Lenght of Attack: 74471; Attack Type: 2\n",
      "Attack ID: 28; Lenght of Attack: 36306; Attack Type: 2\n",
      "Attack ID: 3; Lenght of Attack: 37118; Attack Type: 2\n",
      "Attack ID: 31; Lenght of Attack: 144845; Attack Type: 2\n",
      "Attack ID: 4; Lenght of Attack: 72063; Attack Type: 2\n",
      "Attack ID: 42; Lenght of Attack: 184040; Attack Type: 2\n",
      "Attack ID: 44; Lenght of Attack: 261169; Attack Type: 2\n",
      "Attack ID: 45; Lenght of Attack: 224960; Attack Type: 2\n",
      "Attack ID: 46; Lenght of Attack: 111720; Attack Type: 2\n",
      "Attack ID: 53; Lenght of Attack: 516299; Attack Type: 2\n",
      "Attack ID: 59; Lenght of Attack: 110484; Attack Type: 2\n",
      "Attack ID: 6; Lenght of Attack: 37134; Attack Type: 2\n",
      "Attack ID: 60; Lenght of Attack: 333627; Attack Type: 2\n",
      "Attack ID: 63; Lenght of Attack: 148641; Attack Type: 2\n",
      "Attack ID: 9; Lenght of Attack: 37057; Attack Type: 2\n",
      "pingScan: 16\n",
      "Attack ID: 10; Lenght of Attack: 311; Attack Type: 3\n",
      "Attack ID: 13; Lenght of Attack: 513; Attack Type: 3\n",
      "Attack ID: 15; Lenght of Attack: 64; Attack Type: 3\n",
      "Attack ID: 22; Lenght of Attack: 295; Attack Type: 3\n",
      "Attack ID: 24; Lenght of Attack: 466; Attack Type: 3\n",
      "Attack ID: 33; Lenght of Attack: 307; Attack Type: 3\n",
      "Attack ID: 35; Lenght of Attack: 263; Attack Type: 3\n",
      "Attack ID: 36; Lenght of Attack: 494; Attack Type: 3\n",
      "Attack ID: 38; Lenght of Attack: 267; Attack Type: 3\n",
      "Attack ID: 41; Lenght of Attack: 379; Attack Type: 3\n",
      "Attack ID: 52; Lenght of Attack: 607; Attack Type: 3\n",
      "Attack ID: 57; Lenght of Attack: 522; Attack Type: 3\n",
      "Attack ID: 58; Lenght of Attack: 510; Attack Type: 3\n",
      "Attack ID: 65; Lenght of Attack: 373; Attack Type: 3\n",
      "Attack ID: 66; Lenght of Attack: 359; Attack Type: 3\n",
      "Attack ID: 69; Lenght of Attack: 360; Attack Type: 3\n",
      "portScan : 32\n",
      "Attack ID: 1; Lenght of Attack: 7657; Attack Type: 4\n",
      "Attack ID: 11; Lenght of Attack: 17401; Attack Type: 4\n",
      "Attack ID: 12; Lenght of Attack: 11526; Attack Type: 4\n",
      "Attack ID: 14; Lenght of Attack: 13807; Attack Type: 4\n",
      "Attack ID: 17; Lenght of Attack: 13338; Attack Type: 4\n",
      "Attack ID: 19; Lenght of Attack: 11672; Attack Type: 4\n",
      "Attack ID: 2; Lenght of Attack: 1927; Attack Type: 4\n",
      "Attack ID: 20; Lenght of Attack: 11748; Attack Type: 4\n",
      "Attack ID: 21; Lenght of Attack: 5113; Attack Type: 4\n",
      "Attack ID: 29; Lenght of Attack: 19732; Attack Type: 4\n",
      "Attack ID: 34; Lenght of Attack: 12909; Attack Type: 4\n",
      "Attack ID: 37; Lenght of Attack: 26114; Attack Type: 4\n",
      "Attack ID: 40; Lenght of Attack: 11609; Attack Type: 4\n",
      "Attack ID: 43; Lenght of Attack: 2143; Attack Type: 4\n",
      "Attack ID: 47; Lenght of Attack: 13420; Attack Type: 4\n",
      "Attack ID: 48; Lenght of Attack: 13600; Attack Type: 4\n",
      "Attack ID: 49; Lenght of Attack: 17629; Attack Type: 4\n",
      "Attack ID: 5; Lenght of Attack: 4948; Attack Type: 4\n",
      "Attack ID: 50; Lenght of Attack: 4589; Attack Type: 4\n",
      "Attack ID: 51; Lenght of Attack: 11968; Attack Type: 4\n",
      "Attack ID: 67; Lenght of Attack: 13426; Attack Type: 4\n",
      "Attack ID: 68; Lenght of Attack: 5632; Attack Type: 4\n",
      "Attack ID: 7; Lenght of Attack: 9586; Attack Type: 4\n",
      "Attack ID: 71; Lenght of Attack: 2008; Attack Type: 4\n",
      "Attack ID: 72; Lenght of Attack: 2002; Attack Type: 4\n",
      "Attack ID: 77; Lenght of Attack: 6410; Attack Type: 4\n",
      "Attack ID: 8; Lenght of Attack: 4424; Attack Type: 4\n",
      "Attack ID: 80; Lenght of Attack: 1991; Attack Type: 4\n",
      "Attack ID: 82; Lenght of Attack: 1370; Attack Type: 4\n",
      "Attack ID: 85; Lenght of Attack: 1984; Attack Type: 4\n",
      "Attack ID: 86; Lenght of Attack: 2002; Attack Type: 4\n",
      "Attack ID: 89; Lenght of Attack: 952; Attack Type: 4\n"
     ]
    }
   ],
   "source": [
    "no_1 = []\n",
    "no_2 = []\n",
    "no_3 = []\n",
    "no_4 = []\n",
    "for i in grouped_data.keys():\n",
    "   \n",
    "    if grouped_data[i]['attackType'].unique()[0] == 0:\n",
    "        print(f\"{i}: {len(grouped_data[i])} : {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 1:\n",
    "              no_1.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 2:\n",
    "              no_2.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 3:\n",
    "              no_3.append(i)\n",
    "    if grouped_data[i]['attackType'].unique()[0] == 4:\n",
    "              no_4.append(i)\n",
    "for i in no_1:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"dos : {len(no_2)}\")\n",
    "for i in no_2:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"pingScan: {len(no_3)}\")\n",
    "for i in no_3:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n",
    "print(f\"portScan : {len(no_4)}\")\n",
    "for i in no_4:\n",
    "     print(f\"Attack ID: {i}; Lenght of Attack: {len(grouped_data[i])}; Attack Type: {grouped_data[i]['attackType'].unique()[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del no_1\n",
    "del no_2\n",
    "del no_3\n",
    "del no_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_largeInstances(dic, length):\n",
    "    remove_ID = []\n",
    "    for i in dic.keys():\n",
    "        if (i != '---'):\n",
    "            if(len(dic[i]) >= length):\n",
    "                remove_ID.append(i)\n",
    "    print(len(remove_ID))\n",
    "    removed_attacks = {}\n",
    "    for i in remove_ID:\n",
    "        removed_attacks[i] = dic[i]\n",
    "        del dic[i]\n",
    "    return dic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "grouped_data = del_largeInstances(grouped_data, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "19732\n",
      "73\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "#grouped_data, num = normal_profile(grouped_data)\n",
    "grouped_data1= {}\n",
    "for i in grouped_data.keys():\n",
    "    grouped_data[i] = flag_convert(grouped_data[i])\n",
    "   # grouped_data[i] =  drop_columns(grouped_data[i])\n",
    "grouped_data, num = normal_profile(grouped_data)\n",
    "for i in grouped_data.keys():\n",
    "   # grouped_data[i] = flag_convert(grouped_data[i])\n",
    "    grouped_data[i] =  drop_columns(grouped_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : False\n",
      "10 : False\n",
      "11 : False\n",
      "12 : False\n",
      "13 : False\n",
      "14 : False\n",
      "15 : False\n",
      "17 : False\n",
      "19 : False\n",
      "2 : False\n",
      "20 : False\n",
      "21 : False\n",
      "22 : False\n",
      "24 : False\n",
      "25 : False\n",
      "27 : False\n",
      "29 : False\n",
      "30 : False\n",
      "32 : False\n",
      "33 : False\n",
      "34 : False\n",
      "35 : False\n",
      "36 : False\n",
      "38 : False\n",
      "39 : False\n",
      "40 : False\n",
      "41 : False\n",
      "43 : False\n",
      "47 : False\n",
      "48 : False\n",
      "49 : False\n",
      "5 : False\n",
      "50 : False\n",
      "51 : False\n",
      "52 : False\n",
      "54 : False\n",
      "55 : False\n",
      "56 : False\n",
      "57 : False\n",
      "58 : False\n",
      "61 : False\n",
      "62 : False\n",
      "64 : False\n",
      "65 : False\n",
      "66 : False\n",
      "67 : False\n",
      "68 : False\n",
      "69 : False\n",
      "7 : False\n",
      "70 : False\n",
      "71 : False\n",
      "72 : False\n",
      "73 : False\n",
      "74 : False\n",
      "75 : False\n",
      "76 : False\n",
      "77 : False\n",
      "78 : False\n",
      "79 : False\n",
      "8 : False\n",
      "80 : False\n",
      "81 : False\n",
      "82 : False\n",
      "83 : False\n",
      "84 : False\n",
      "85 : False\n",
      "86 : False\n",
      "87 : False\n",
      "88 : False\n",
      "89 : False\n",
      "90 : False\n",
      "91 : False\n",
      "92 : False\n",
      "(1489536076632, 1489546876632] : False\n",
      "(1489546876632, 1489557676632] : False\n",
      "(1489557676632, 1489568476632] : False\n",
      "(1489568476632, 1489579276632] : False\n",
      "(1489579276632, 1489590076632] : False\n",
      "(1489590076632, 1489600876632] : False\n",
      "(1489600876632, 1489611676632] : False\n",
      "(1489611676632, 1489622476632] : False\n",
      "(1489622476632, 1489633276632] : False\n",
      "(1489633276632, 1489644076632] : False\n",
      "(1489644076632, 1489654876632] : False\n",
      "(1489654876632, 1489665676632] : False\n",
      "(1489665676632, 1489676476632] : False\n",
      "(1489676476632, 1489687276632] : False\n",
      "(1489687276632, 1489698076632] : False\n",
      "(1489698076632, 1489708876632] : False\n",
      "(1489708876632, 1489719676632] : False\n",
      "(1489719676632, 1489730476632] : False\n",
      "(1489730476632, 1489741276632] : False\n",
      "(1489741276632, 1489752076632] : False\n",
      "(1489752076632, 1489762876632] : False\n",
      "(1489762876632, 1489773676632] : False\n",
      "(1489773676632, 1489784476632] : False\n",
      "(1489784476632, 1489795276632] : False\n",
      "(1489795276632, 1489806076632] : False\n",
      "(1489806076632, 1489816876632] : False\n",
      "(1489816876632, 1489827676632] : False\n",
      "(1489827676632, 1489838476632] : False\n",
      "(1489838476632, 1489849276632] : False\n",
      "(1489849276632, 1489860076632] : False\n",
      "(1489860076632, 1489870876632] : False\n",
      "(1489870876632, 1489881676632] : False\n",
      "(1489881676632, 1489892476632] : False\n",
      "(1489892476632, 1489903276632] : False\n",
      "(1489903276632, 1489914076632] : False\n",
      "(1489914076632, 1489924876632] : False\n",
      "(1489924876632, 1489935676632] : False\n",
      "(1489935676632, 1489946476632] : False\n",
      "(1489946476632, 1489957276632] : False\n",
      "(1489957276632, 1489968076632] : False\n",
      "(1489968076632, 1489978876632] : False\n",
      "(1489978876632, 1489989676632] : False\n",
      "(1489989676632, 1490000476632] : False\n",
      "(1490000476632, 1490011276632] : False\n",
      "(1490011276632, 1490022076632] : False\n",
      "(1490022076632, 1490032876632] : False\n",
      "(1490032876632, 1490043676632] : False\n",
      "(1490043676632, 1490054476632] : False\n",
      "(1490054476632, 1490065276632] : False\n",
      "(1490065276632, 1490076076632] : False\n",
      "(1490076076632, 1490086876632] : False\n",
      "(1490086876632, 1490097676632] : False\n",
      "(1490097676632, 1490108476632] : False\n",
      "(1490108476632, 1490119276632] : False\n",
      "(1490119276632, 1490130076632] : False\n",
      "(1490130076632, 1490140876632] : False\n",
      "(1490140876632, 1490151676632] : False\n",
      "(1490151676632, 1490162476632] : False\n",
      "(1490162476632, 1490173276632] : False\n",
      "(1490173276632, 1490184076632] : False\n",
      "(1490184076632, 1490194876632] : False\n",
      "(1490194876632, 1490205676632] : False\n",
      "(1490205676632, 1490216476632] : False\n",
      "(1490216476632, 1490227276632] : False\n",
      "(1490227276632, 1490238076632] : False\n",
      "(1490238076632, 1490248876632] : False\n",
      "(1490248876632, 1490259676632] : False\n",
      "(1490259676632, 1490270476632] : False\n",
      "(1490270476632, 1490281276632] : False\n",
      "(1490281276632, 1490292076632] : False\n",
      "(1490292076632, 1490302876632] : False\n",
      "(1490302876632, 1490313676632] : False\n",
      "(1490313676632, 1490324476632] : False\n",
      "(1490324476632, 1490335276632] : False\n",
      "(1490335276632, 1490346076632] : False\n",
      "(1490346076632, 1490356876632] : False\n",
      "(1490356876632, 1490367676632] : False\n",
      "(1490367676632, 1490378476632] : False\n",
      "(1490378476632, 1490389276632] : False\n",
      "(1490389276632, 1490400076632] : False\n",
      "(1490400076632, 1490410876632] : False\n",
      "(1490410876632, 1490421676632] : False\n",
      "(1490421676632, 1490432476632] : False\n",
      "(1490432476632, 1490443276632] : False\n",
      "(1490443276632, 1490454076632] : False\n",
      "(1490454076632, 1490464876632] : False\n",
      "(1490464876632, 1490475676632] : False\n",
      "(1490475676632, 1490486476632] : False\n",
      "(1490486476632, 1490497276632] : False\n",
      "(1490497276632, 1490508076632] : False\n",
      "(1490508076632, 1490518876632] : False\n",
      "(1490518876632, 1490529676632] : False\n",
      "(1490529676632, 1490540476632] : False\n",
      "(1490540476632, 1490551276632] : False\n",
      "(1490551276632, 1490562076632] : False\n",
      "(1490562076632, 1490572876632] : False\n",
      "(1490572876632, 1490583676632] : False\n",
      "(1490583676632, 1490594476632] : False\n",
      "(1490594476632, 1490605276632] : False\n",
      "(1490605276632, 1490616076632] : False\n",
      "(1490616076632, 1490626876632] : False\n",
      "(1490626876632, 1490637676632] : False\n",
      "(1490637676632, 1490648476632] : False\n",
      "(1490648476632, 1490659276632] : False\n",
      "(1490659276632, 1490670076632] : False\n",
      "(1490670076632, 1490680876632] : False\n",
      "(1490680876632, 1490691676632] : False\n",
      "(1490691676632, 1490702476632] : False\n",
      "(1490702476632, 1490713276632] : False\n",
      "(1490713276632, 1490724076632] : False\n",
      "(1490724076632, 1490734876632] : False\n",
      "(1490734876632, 1490745676632] : False\n",
      "(1490745676632, 1490756476632] : False\n",
      "(1490756476632, 1490767276632] : False\n",
      "(1490767276632, 1490778076632] : False\n",
      "(1490778076632, 1490788876632] : False\n",
      "(1490788876632, 1490799676632] : False\n",
      "(1490799676632, 1490810476632] : False\n",
      "(1490810476632, 1490821276632] : False\n",
      "(1490821276632, 1490832076632] : False\n",
      "(1490832076632, 1490842876632] : False\n",
      "(1490842876632, 1490853676632] : False\n",
      "(1490853676632, 1490864476632] : False\n",
      "(1490864476632, 1490875276632] : False\n",
      "(1490875276632, 1490886076632] : False\n",
      "(1490886076632, 1490896876632] : False\n",
      "(1490896876632, 1490907676632] : False\n",
      "(1490907676632, 1490918476632] : False\n",
      "(1490918476632, 1490929276632] : False\n",
      "(1490929276632, 1490940076632] : False\n",
      "(1490940076632, 1490950876632] : False\n",
      "(1490950876632, 1490961676632] : False\n",
      "(1490961676632, 1490972476632] : False\n",
      "(1490972476632, 1490983276632] : False\n",
      "(1490983276632, 1490994076632] : False\n",
      "(1490994076632, 1491004876632] : False\n",
      "(1491004876632, 1491015676632] : False\n",
      "(1491015676632, 1491026476632] : False\n",
      "(1491026476632, 1491037276632] : False\n",
      "(1491037276632, 1491048076632] : False\n",
      "(1491048076632, 1491058876632] : False\n",
      "(1491058876632, 1491069676632] : False\n",
      "(1491069676632, 1491080476632] : False\n",
      "(1491080476632, 1491091276632] : False\n",
      "(1491091276632, 1491102076632] : False\n",
      "(1491102076632, 1491112876632] : False\n",
      "(1491112876632, 1491123676632] : False\n",
      "(1491123676632, 1491134476632] : False\n",
      "(1491134476632, 1491145276632] : False\n",
      "(1491145276632, 1491156076632] : False\n",
      "(1491156076632, 1491166876632] : False\n",
      "(1491166876632, 1491177676632] : False\n",
      "(1491177676632, 1491188476632] : False\n",
      "(1491188476632, 1491199276632] : False\n",
      "(1491199276632, 1491210076632] : False\n",
      "(1491210076632, 1491220876632] : False\n",
      "(1491220876632, 1491231676632] : False\n",
      "(1491231676632, 1491242476632] : False\n",
      "(1491242476632, 1491253276632] : False\n",
      "(1491253276632, 1491264076632] : False\n",
      "(1491264076632, 1491274876632] : False\n",
      "(1491274876632, 1491285676632] : False\n",
      "(1491285676632, 1491296476632] : False\n",
      "(1491296476632, 1491307276632] : False\n",
      "(1491307276632, 1491318076632] : False\n",
      "(1491318076632, 1491328876632] : False\n",
      "(1491328876632, 1491339676632] : False\n",
      "(1491339676632, 1491350476632] : False\n",
      "(1491350476632, 1491361276632] : False\n",
      "(1491361276632, 1491372076632] : False\n",
      "(1491372076632, 1491382876632] : False\n",
      "(1491382876632, 1491393676632] : False\n",
      "(1491393676632, 1491404476632] : False\n",
      "(1491404476632, 1491415276632] : False\n",
      "(1491415276632, 1491426076632] : False\n",
      "(1491426076632, 1491436876632] : False\n",
      "(1491436876632, 1491447676632] : False\n",
      "(1491447676632, 1491458476632] : False\n",
      "(1491458476632, 1491469276632] : False\n",
      "(1491469276632, 1491480076632] : False\n",
      "(1491480076632, 1491490876632] : False\n",
      "(1491490876632, 1491501676632] : False\n",
      "(1491501676632, 1491512476632] : False\n",
      "(1491512476632, 1491523276632] : False\n",
      "(1491523276632, 1491534076632] : False\n",
      "(1491534076632, 1491544876632] : False\n",
      "(1491544876632, 1491555676632] : False\n",
      "(1491555676632, 1491566476632] : False\n",
      "(1491566476632, 1491577276632] : False\n",
      "(1491577276632, 1491588076632] : False\n",
      "(1491588076632, 1491598876632] : False\n",
      "(1491598876632, 1491609676632] : False\n",
      "(1491609676632, 1491620476632] : False\n",
      "(1491620476632, 1491631276632] : False\n",
      "(1491631276632, 1491642076632] : False\n",
      "(1491642076632, 1491652876632] : False\n",
      "(1491652876632, 1491663676632] : False\n",
      "(1491663676632, 1491674476632] : False\n",
      "(1491674476632, 1491685276632] : False\n",
      "(1491685276632, 1491696076632] : False\n",
      "(1491696076632, 1491706876632] : False\n",
      "(1491706876632, 1491717676632] : False\n",
      "(1491717676632, 1491728476632] : False\n",
      "(1491728476632, 1491739276632] : False\n",
      "(1491739276632, 1491750076632] : False\n",
      "(1491750076632, 1491760876632] : False\n",
      "(1491760876632, 1491771676632] : False\n",
      "(1491771676632, 1491782476632] : False\n",
      "(1491782476632, 1491793276632] : False\n",
      "(1491793276632, 1491804076632] : False\n",
      "(1491804076632, 1491814876632] : False\n",
      "(1491814876632, 1491825676632] : False\n",
      "(1491825676632, 1491836476632] : False\n",
      "(1491836476632, 1491847276632] : False\n",
      "(1491847276632, 1491858076632] : False\n",
      "(1491858076632, 1491868876632] : False\n",
      "(1491868876632, 1491879676632] : False\n",
      "(1491879676632, 1491890476632] : False\n",
      "(1491890476632, 1491901276632] : False\n",
      "(1491901276632, 1491912076632] : False\n",
      "(1491912076632, 1491922876632] : False\n",
      "(1491922876632, 1491933676632] : False\n",
      "(1491933676632, 1491944476632] : False\n",
      "(1491944476632, 1491955276632] : False\n",
      "(1491955276632, 1491966076632] : False\n",
      "(1491966076632, 1491976876632] : False\n",
      "(1491976876632, 1491987676632] : False\n",
      "(1491987676632, 1491998476632] : False\n",
      "(1491998476632, 1492009276632] : False\n",
      "(1492009276632, 1492020076632] : False\n",
      "(1492020076632, 1492030876632] : False\n",
      "(1492030876632, 1492041676632] : False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492041676632, 1492052476632] : False\n",
      "(1492052476632, 1492063276632] : False\n",
      "(1492063276632, 1492074076632] : False\n",
      "(1492074076632, 1492084876632] : False\n",
      "(1492084876632, 1492095676632] : False\n",
      "(1492095676632, 1492106476632] : False\n",
      "(1492106476632, 1492117276632] : False\n",
      "(1492117276632, 1492128076632] : False\n",
      "(1492128076632, 1492138876632] : False\n",
      "(1492138876632, 1492149676632] : False\n",
      "(1492149676632, 1492160476632] : False\n",
      "(1492160476632, 1492171276632] : False\n",
      "(1492171276632, 1492182076632] : False\n",
      "(1492182076632, 1492192876632] : False\n",
      "(1492192876632, 1492203676632] : False\n",
      "(1492203676632, 1492214476632] : False\n",
      "(1492214476632, 1492225276632] : False\n",
      "(1492225276632, 1492236076632] : False\n",
      "(1492236076632, 1492246876632] : False\n",
      "(1492246876632, 1492257676632] : False\n",
      "(1492257676632, 1492268476632] : False\n",
      "(1492268476632, 1492279276632] : False\n",
      "(1492279276632, 1492290076632] : False\n",
      "(1492290076632, 1492300876632] : False\n",
      "(1492300876632, 1492311676632] : False\n",
      "(1492311676632, 1492322476632] : False\n",
      "(1492322476632, 1492333276632] : False\n",
      "(1492333276632, 1492344076632] : False\n",
      "(1492344076632, 1492354876632] : False\n",
      "(1492354876632, 1492365676632] : False\n",
      "(1492365676632, 1492376476632] : False\n",
      "(1492376476632, 1492387276632] : False\n",
      "(1492387276632, 1492398076632] : False\n",
      "(1492398076632, 1492408876632] : False\n",
      "(1492408876632, 1492419676632] : False\n",
      "(1492419676632, 1492430476632] : False\n",
      "(1492430476632, 1492441276632] : False\n",
      "(1492441276632, 1492452076632] : False\n",
      "(1492452076632, 1492462876632] : False\n",
      "(1492462876632, 1492473676632] : False\n",
      "(1492473676632, 1492484476632] : False\n",
      "(1492484476632, 1492495276632] : False\n",
      "(1492495276632, 1492506076632] : False\n",
      "(1492506076632, 1492516876632] : False\n",
      "(1492516876632, 1492527676632] : False\n"
     ]
    }
   ],
   "source": [
    "for i in grouped_data.keys():\n",
    "    #if (grouped_data[i].hasnull())\n",
    "    print(f'{i} : {grouped_data[i].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances which are empty: 167\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in grouped_data.keys():\n",
    "    if ( len(grouped_data[i]) == 0):\n",
    "        counter = counter +1;\n",
    "print(f\"Number of Instances which are empty: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return x if x % 100 == 0 else x + 100 - x % 100\n",
    "#Convert to 3D arrays, input dict\n",
    "def make_array(dic):\n",
    "    x = []\n",
    "    y = []\n",
    "    zero_arrays = []\n",
    "    for i in dic.keys():\n",
    "        if ( len(dic[i]) == 0):\n",
    "            zero_arrays.append(i);\n",
    "    for i in zero_arrays:\n",
    "        del dic[i]\n",
    "    for i in dic.keys():\n",
    "        x.append(np.array(dic[i].drop(['attackType'],axis = 1)).astype(np.float32))\n",
    "       # print(f'{i}')\n",
    "        y.append(dic[i]['attackType'].values[0])\n",
    "    print(len(y))\n",
    "    o = []\n",
    "    features = len(x[1][1])\n",
    "    #for i in x:\n",
    "     #   o.append(len(i))\n",
    "   # print(min(o))\n",
    "    o = num\n",
    "    o = roundup(o)\n",
    "    print(o)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        l = len(i)\n",
    "        i = list(i)\n",
    "        if(o > l):\n",
    "            l = o-l\n",
    "            for j in range(0, l, 1):\n",
    "                i.append([0] * features)\n",
    "        elif (o<l):\n",
    "            l = l-o\n",
    "            i = i[:-l]\n",
    "        #i = [k = np.array([k]) for l in i for k in l] # Makes array elements an array \n",
    "        x[index] = np.array(i).astype(np.float32)\n",
    "        index = index + 1\n",
    "    #x = [[i] for i in x]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "19800\n"
     ]
    }
   ],
   "source": [
    "X,Y = make_array(grouped_data)\n",
    "del grouped_data\n",
    "gc.collect()\n",
    "Y = np.array(Y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 110, 1: 26, 3: 16, 4: 31}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_4D(arr):\n",
    "    x = []\n",
    "    for i in range(0, len(arr),1):\n",
    "        temp = []\n",
    "        for j in range(0,len(arr[i]),1):\n",
    "             temp.append([np.array([k]) for k in arr[i][j]])\n",
    "        x.append(np.array(temp).astype(np.float32))\n",
    "    return np.array(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = make_4D(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y , test_size=0.2, random_state=0,  stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X,Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 88, 1: 21, 3: 13, 4: 24}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 22, 1: 5, 3: 3, 4: 7}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(Y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in X_train:\n",
    "    print(f'{np.isnan(i).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 146 \n",
      " X:19800 \n",
      " Y:34 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "nsamples,nx, ny = X_train.shape\n",
    "print(f\"samples: {nsamples} \\n X:{nx} \\n Y:{ny} \\n \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19800"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((37,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128,64, 64],\n",
    "                                         learning_rate_rbm=0.00001,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=12,\n",
    "                                         n_iter_backprop=512,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3887"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del classifier\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 46351.882812\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 46348.246094\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46334.957031\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46293.437500\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 46186.550781\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 45970.343750\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 45614.191406\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 45101.679688\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 44422.617188\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 43578.062500\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 42575.007812\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 41424.621094\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2611.053955\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2608.028320\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 2604.531250\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2600.456787\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2595.725098\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2590.256348\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2584.161377\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 2577.229004\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 2569.507324\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 2560.831787\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 2551.228271\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 2540.693604\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 40.423767\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 40.420357\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 40.416969\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 40.413628\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40.410351\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 40.407104\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 40.403828\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 40.400448\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 40.397102\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 40.393814\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 40.390396\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 40.387005\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.132750\n",
      ">> Epoch 1 finished \tANN training loss 0.798049\n",
      ">> Epoch 2 finished \tANN training loss 0.807148\n",
      ">> Epoch 3 finished \tANN training loss 0.611881\n",
      ">> Epoch 4 finished \tANN training loss 0.612379\n",
      ">> Epoch 5 finished \tANN training loss 0.564581\n",
      ">> Epoch 6 finished \tANN training loss 0.543780\n",
      ">> Epoch 7 finished \tANN training loss 0.531014\n",
      ">> Epoch 8 finished \tANN training loss 0.506545\n",
      ">> Epoch 9 finished \tANN training loss 0.487066\n",
      ">> Epoch 10 finished \tANN training loss 0.479511\n",
      ">> Epoch 11 finished \tANN training loss 0.454617\n",
      ">> Epoch 12 finished \tANN training loss 0.434675\n",
      ">> Epoch 13 finished \tANN training loss 0.412146\n",
      ">> Epoch 14 finished \tANN training loss 0.382226\n",
      ">> Epoch 15 finished \tANN training loss 0.369256\n",
      ">> Epoch 16 finished \tANN training loss 0.319297\n",
      ">> Epoch 17 finished \tANN training loss 0.309543\n",
      ">> Epoch 18 finished \tANN training loss 0.298832\n",
      ">> Epoch 19 finished \tANN training loss 0.292607\n",
      ">> Epoch 20 finished \tANN training loss 0.286177\n",
      ">> Epoch 21 finished \tANN training loss 0.279975\n",
      ">> Epoch 22 finished \tANN training loss 0.274704\n",
      ">> Epoch 23 finished \tANN training loss 0.270561\n",
      ">> Epoch 24 finished \tANN training loss 0.266343\n",
      ">> Epoch 25 finished \tANN training loss 0.262647\n",
      ">> Epoch 26 finished \tANN training loss 0.259333\n",
      ">> Epoch 27 finished \tANN training loss 0.256414\n",
      ">> Epoch 28 finished \tANN training loss 0.253790\n",
      ">> Epoch 29 finished \tANN training loss 0.251141\n",
      ">> Epoch 30 finished \tANN training loss 0.248677\n",
      ">> Epoch 31 finished \tANN training loss 0.246283\n",
      ">> Epoch 32 finished \tANN training loss 0.243990\n",
      ">> Epoch 33 finished \tANN training loss 0.242028\n",
      ">> Epoch 34 finished \tANN training loss 0.239883\n",
      ">> Epoch 35 finished \tANN training loss 0.238080\n",
      ">> Epoch 36 finished \tANN training loss 0.236343\n",
      ">> Epoch 37 finished \tANN training loss 0.234635\n",
      ">> Epoch 38 finished \tANN training loss 0.233097\n",
      ">> Epoch 39 finished \tANN training loss 0.231563\n",
      ">> Epoch 40 finished \tANN training loss 0.230167\n",
      ">> Epoch 41 finished \tANN training loss 0.228777\n",
      ">> Epoch 42 finished \tANN training loss 0.227617\n",
      ">> Epoch 43 finished \tANN training loss 0.226549\n",
      ">> Epoch 44 finished \tANN training loss 0.225454\n",
      ">> Epoch 45 finished \tANN training loss 0.224407\n",
      ">> Epoch 46 finished \tANN training loss 0.223389\n",
      ">> Epoch 47 finished \tANN training loss 0.222546\n",
      ">> Epoch 48 finished \tANN training loss 0.221578\n",
      ">> Epoch 49 finished \tANN training loss 0.220582\n",
      ">> Epoch 50 finished \tANN training loss 0.219682\n",
      ">> Epoch 51 finished \tANN training loss 0.218811\n",
      ">> Epoch 52 finished \tANN training loss 0.218005\n",
      ">> Epoch 53 finished \tANN training loss 0.217165\n",
      ">> Epoch 54 finished \tANN training loss 0.216400\n",
      ">> Epoch 55 finished \tANN training loss 0.215660\n",
      ">> Epoch 56 finished \tANN training loss 0.214963\n",
      ">> Epoch 57 finished \tANN training loss 0.214281\n",
      ">> Epoch 58 finished \tANN training loss 0.213612\n",
      ">> Epoch 59 finished \tANN training loss 0.212995\n",
      ">> Epoch 60 finished \tANN training loss 0.212324\n",
      ">> Epoch 61 finished \tANN training loss 0.211720\n",
      ">> Epoch 62 finished \tANN training loss 0.211095\n",
      ">> Epoch 63 finished \tANN training loss 0.210492\n",
      ">> Epoch 64 finished \tANN training loss 0.209910\n",
      ">> Epoch 65 finished \tANN training loss 0.209379\n",
      ">> Epoch 66 finished \tANN training loss 0.208887\n",
      ">> Epoch 67 finished \tANN training loss 0.208352\n",
      ">> Epoch 68 finished \tANN training loss 0.207839\n",
      ">> Epoch 69 finished \tANN training loss 0.207347\n",
      ">> Epoch 70 finished \tANN training loss 0.206860\n",
      ">> Epoch 71 finished \tANN training loss 0.206426\n",
      ">> Epoch 72 finished \tANN training loss 0.205980\n",
      ">> Epoch 73 finished \tANN training loss 0.205557\n",
      ">> Epoch 74 finished \tANN training loss 0.205135\n",
      ">> Epoch 75 finished \tANN training loss 0.204717\n",
      ">> Epoch 76 finished \tANN training loss 0.204333\n",
      ">> Epoch 77 finished \tANN training loss 0.203953\n",
      ">> Epoch 78 finished \tANN training loss 0.203563\n",
      ">> Epoch 79 finished \tANN training loss 0.203146\n",
      ">> Epoch 80 finished \tANN training loss 0.202795\n",
      ">> Epoch 81 finished \tANN training loss 0.202433\n",
      ">> Epoch 82 finished \tANN training loss 0.202049\n",
      ">> Epoch 83 finished \tANN training loss 0.201689\n",
      ">> Epoch 84 finished \tANN training loss 0.201308\n",
      ">> Epoch 85 finished \tANN training loss 0.200919\n",
      ">> Epoch 86 finished \tANN training loss 0.200543\n",
      ">> Epoch 87 finished \tANN training loss 0.200176\n",
      ">> Epoch 88 finished \tANN training loss 0.199840\n",
      ">> Epoch 89 finished \tANN training loss 0.199518\n",
      ">> Epoch 90 finished \tANN training loss 0.199139\n",
      ">> Epoch 91 finished \tANN training loss 0.198737\n",
      ">> Epoch 92 finished \tANN training loss 0.198335\n",
      ">> Epoch 93 finished \tANN training loss 0.197882\n",
      ">> Epoch 94 finished \tANN training loss 0.197444\n",
      ">> Epoch 95 finished \tANN training loss 0.196909\n",
      ">> Epoch 96 finished \tANN training loss 0.196264\n",
      ">> Epoch 97 finished \tANN training loss 0.195442\n",
      ">> Epoch 98 finished \tANN training loss 0.194401\n",
      ">> Epoch 99 finished \tANN training loss 0.192983\n",
      ">> Epoch 100 finished \tANN training loss 0.190963\n",
      ">> Epoch 101 finished \tANN training loss 0.188071\n",
      ">> Epoch 102 finished \tANN training loss 0.183373\n",
      ">> Epoch 103 finished \tANN training loss 0.177487\n",
      ">> Epoch 104 finished \tANN training loss 0.168363\n",
      ">> Epoch 105 finished \tANN training loss 0.158245\n",
      ">> Epoch 106 finished \tANN training loss 0.147632\n",
      ">> Epoch 107 finished \tANN training loss 0.138441\n",
      ">> Epoch 108 finished \tANN training loss 0.131932\n",
      ">> Epoch 109 finished \tANN training loss 0.124439\n",
      ">> Epoch 110 finished \tANN training loss 0.118643\n",
      ">> Epoch 111 finished \tANN training loss 0.113402\n",
      ">> Epoch 112 finished \tANN training loss 0.109501\n",
      ">> Epoch 113 finished \tANN training loss 0.104502\n",
      ">> Epoch 114 finished \tANN training loss 0.100864\n",
      ">> Epoch 115 finished \tANN training loss 0.097137\n",
      ">> Epoch 116 finished \tANN training loss 0.093537\n",
      ">> Epoch 117 finished \tANN training loss 0.090345\n",
      ">> Epoch 118 finished \tANN training loss 0.087030\n",
      ">> Epoch 119 finished \tANN training loss 0.084428\n",
      ">> Epoch 120 finished \tANN training loss 0.081524\n",
      ">> Epoch 121 finished \tANN training loss 0.078999\n",
      ">> Epoch 122 finished \tANN training loss 0.076594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 123 finished \tANN training loss 0.074089\n",
      ">> Epoch 124 finished \tANN training loss 0.071425\n",
      ">> Epoch 125 finished \tANN training loss 0.068946\n",
      ">> Epoch 126 finished \tANN training loss 0.065832\n",
      ">> Epoch 127 finished \tANN training loss 0.063044\n",
      ">> Epoch 128 finished \tANN training loss 0.059874\n",
      ">> Epoch 129 finished \tANN training loss 0.056547\n",
      ">> Epoch 130 finished \tANN training loss 0.053031\n",
      ">> Epoch 131 finished \tANN training loss 0.048891\n",
      ">> Epoch 132 finished \tANN training loss 0.045117\n",
      ">> Epoch 133 finished \tANN training loss 0.041649\n",
      ">> Epoch 134 finished \tANN training loss 0.038291\n",
      ">> Epoch 135 finished \tANN training loss 0.035108\n",
      ">> Epoch 136 finished \tANN training loss 0.032616\n",
      ">> Epoch 137 finished \tANN training loss 0.030954\n",
      ">> Epoch 138 finished \tANN training loss 0.029480\n",
      ">> Epoch 139 finished \tANN training loss 0.028343\n",
      ">> Epoch 140 finished \tANN training loss 0.027307\n",
      ">> Epoch 141 finished \tANN training loss 0.026490\n",
      ">> Epoch 142 finished \tANN training loss 0.025652\n",
      ">> Epoch 143 finished \tANN training loss 0.024844\n",
      ">> Epoch 144 finished \tANN training loss 0.024034\n",
      ">> Epoch 145 finished \tANN training loss 0.023222\n",
      ">> Epoch 146 finished \tANN training loss 0.022453\n",
      ">> Epoch 147 finished \tANN training loss 0.021670\n",
      ">> Epoch 148 finished \tANN training loss 0.020528\n",
      ">> Epoch 149 finished \tANN training loss 0.019799\n",
      ">> Epoch 150 finished \tANN training loss 0.019092\n",
      ">> Epoch 151 finished \tANN training loss 0.018387\n",
      ">> Epoch 152 finished \tANN training loss 0.017682\n",
      ">> Epoch 153 finished \tANN training loss 0.016988\n",
      ">> Epoch 154 finished \tANN training loss 0.016293\n",
      ">> Epoch 155 finished \tANN training loss 0.015602\n",
      ">> Epoch 156 finished \tANN training loss 0.014913\n",
      ">> Epoch 157 finished \tANN training loss 0.014240\n",
      ">> Epoch 158 finished \tANN training loss 0.013574\n",
      ">> Epoch 159 finished \tANN training loss 0.012911\n",
      ">> Epoch 160 finished \tANN training loss 0.012249\n",
      ">> Epoch 161 finished \tANN training loss 0.011602\n",
      ">> Epoch 162 finished \tANN training loss 0.010966\n",
      ">> Epoch 163 finished \tANN training loss 0.010346\n",
      ">> Epoch 164 finished \tANN training loss 0.009747\n",
      ">> Epoch 165 finished \tANN training loss 0.009166\n",
      ">> Epoch 166 finished \tANN training loss 0.008212\n",
      ">> Epoch 167 finished \tANN training loss 0.007707\n",
      ">> Epoch 168 finished \tANN training loss 0.007232\n",
      ">> Epoch 169 finished \tANN training loss 0.006790\n",
      ">> Epoch 170 finished \tANN training loss 0.006380\n",
      ">> Epoch 171 finished \tANN training loss 0.005994\n",
      ">> Epoch 172 finished \tANN training loss 0.005639\n",
      ">> Epoch 173 finished \tANN training loss 0.005313\n",
      ">> Epoch 174 finished \tANN training loss 0.005014\n",
      ">> Epoch 175 finished \tANN training loss 0.004737\n",
      ">> Epoch 176 finished \tANN training loss 0.004482\n",
      ">> Epoch 177 finished \tANN training loss 0.004246\n",
      ">> Epoch 178 finished \tANN training loss 0.004035\n",
      ">> Epoch 179 finished \tANN training loss 0.003834\n",
      ">> Epoch 180 finished \tANN training loss 0.003650\n",
      ">> Epoch 181 finished \tANN training loss 0.003480\n",
      ">> Epoch 182 finished \tANN training loss 0.003324\n",
      ">> Epoch 183 finished \tANN training loss 0.003091\n",
      ">> Epoch 184 finished \tANN training loss 0.002961\n",
      ">> Epoch 185 finished \tANN training loss 0.002832\n",
      ">> Epoch 186 finished \tANN training loss 0.002719\n",
      ">> Epoch 187 finished \tANN training loss 0.002611\n",
      ">> Epoch 188 finished \tANN training loss 0.002457\n",
      ">> Epoch 189 finished \tANN training loss 0.002370\n",
      ">> Epoch 190 finished \tANN training loss 0.002236\n",
      ">> Epoch 191 finished \tANN training loss 0.002162\n",
      ">> Epoch 192 finished \tANN training loss 0.002091\n",
      ">> Epoch 193 finished \tANN training loss 0.002022\n",
      ">> Epoch 194 finished \tANN training loss 0.001925\n",
      ">> Epoch 195 finished \tANN training loss 0.001869\n",
      ">> Epoch 196 finished \tANN training loss 0.001816\n",
      ">> Epoch 197 finished \tANN training loss 0.001766\n",
      ">> Epoch 198 finished \tANN training loss 0.001716\n",
      ">> Epoch 199 finished \tANN training loss 0.001670\n",
      ">> Epoch 200 finished \tANN training loss 0.001625\n",
      ">> Epoch 201 finished \tANN training loss 0.001584\n",
      ">> Epoch 202 finished \tANN training loss 0.001544\n",
      ">> Epoch 203 finished \tANN training loss 0.001507\n",
      ">> Epoch 204 finished \tANN training loss 0.001469\n",
      ">> Epoch 205 finished \tANN training loss 0.001434\n",
      ">> Epoch 206 finished \tANN training loss 0.001401\n",
      ">> Epoch 207 finished \tANN training loss 0.001369\n",
      ">> Epoch 208 finished \tANN training loss 0.001337\n",
      ">> Epoch 209 finished \tANN training loss 0.001308\n",
      ">> Epoch 210 finished \tANN training loss 0.001278\n",
      ">> Epoch 211 finished \tANN training loss 0.001251\n",
      ">> Epoch 212 finished \tANN training loss 0.001225\n",
      ">> Epoch 213 finished \tANN training loss 0.001187\n",
      ">> Epoch 214 finished \tANN training loss 0.001164\n",
      ">> Epoch 215 finished \tANN training loss 0.001140\n",
      ">> Epoch 216 finished \tANN training loss 0.001118\n",
      ">> Epoch 217 finished \tANN training loss 0.001086\n",
      ">> Epoch 218 finished \tANN training loss 0.001064\n",
      ">> Epoch 219 finished \tANN training loss 0.001044\n",
      ">> Epoch 220 finished \tANN training loss 0.001025\n",
      ">> Epoch 221 finished \tANN training loss 0.001006\n",
      ">> Epoch 222 finished \tANN training loss 0.000988\n",
      ">> Epoch 223 finished \tANN training loss 0.000970\n",
      ">> Epoch 224 finished \tANN training loss 0.000953\n",
      ">> Epoch 225 finished \tANN training loss 0.000927\n",
      ">> Epoch 226 finished \tANN training loss 0.000904\n",
      ">> Epoch 227 finished \tANN training loss 0.000889\n",
      ">> Epoch 228 finished \tANN training loss 0.000875\n",
      ">> Epoch 229 finished \tANN training loss 0.000855\n",
      ">> Epoch 230 finished \tANN training loss 0.000841\n",
      ">> Epoch 231 finished \tANN training loss 0.000827\n",
      ">> Epoch 232 finished \tANN training loss 0.000815\n",
      ">> Epoch 233 finished \tANN training loss 0.000802\n",
      ">> Epoch 234 finished \tANN training loss 0.000791\n",
      ">> Epoch 235 finished \tANN training loss 0.000780\n",
      ">> Epoch 236 finished \tANN training loss 0.000769\n",
      ">> Epoch 237 finished \tANN training loss 0.000758\n",
      ">> Epoch 238 finished \tANN training loss 0.000747\n",
      ">> Epoch 239 finished \tANN training loss 0.000732\n",
      ">> Epoch 240 finished \tANN training loss 0.000716\n",
      ">> Epoch 241 finished \tANN training loss 0.000707\n",
      ">> Epoch 242 finished \tANN training loss 0.000697\n",
      ">> Epoch 243 finished \tANN training loss 0.000688\n",
      ">> Epoch 244 finished \tANN training loss 0.000678\n",
      ">> Epoch 245 finished \tANN training loss 0.000669\n",
      ">> Epoch 246 finished \tANN training loss 0.000661\n",
      ">> Epoch 247 finished \tANN training loss 0.000652\n",
      ">> Epoch 248 finished \tANN training loss 0.000644\n",
      ">> Epoch 249 finished \tANN training loss 0.000636\n",
      ">> Epoch 250 finished \tANN training loss 0.000628\n",
      ">> Epoch 251 finished \tANN training loss 0.000620\n",
      ">> Epoch 252 finished \tANN training loss 0.000613\n",
      ">> Epoch 253 finished \tANN training loss 0.000605\n",
      ">> Epoch 254 finished \tANN training loss 0.000594\n",
      ">> Epoch 255 finished \tANN training loss 0.000587\n",
      ">> Epoch 256 finished \tANN training loss 0.000580\n",
      ">> Epoch 257 finished \tANN training loss 0.000573\n",
      ">> Epoch 258 finished \tANN training loss 0.000567\n",
      ">> Epoch 259 finished \tANN training loss 0.000560\n",
      ">> Epoch 260 finished \tANN training loss 0.000554\n",
      ">> Epoch 261 finished \tANN training loss 0.000548\n",
      ">> Epoch 262 finished \tANN training loss 0.000542\n",
      ">> Epoch 263 finished \tANN training loss 0.000533\n",
      ">> Epoch 264 finished \tANN training loss 0.000526\n",
      ">> Epoch 265 finished \tANN training loss 0.000521\n",
      ">> Epoch 266 finished \tANN training loss 0.000515\n",
      ">> Epoch 267 finished \tANN training loss 0.000510\n",
      ">> Epoch 268 finished \tANN training loss 0.000505\n",
      ">> Epoch 269 finished \tANN training loss 0.000499\n",
      ">> Epoch 270 finished \tANN training loss 0.000494\n",
      ">> Epoch 271 finished \tANN training loss 0.000489\n",
      ">> Epoch 272 finished \tANN training loss 0.000484\n",
      ">> Epoch 273 finished \tANN training loss 0.000479\n",
      ">> Epoch 274 finished \tANN training loss 0.000472\n",
      ">> Epoch 275 finished \tANN training loss 0.000467\n",
      ">> Epoch 276 finished \tANN training loss 0.000463\n",
      ">> Epoch 277 finished \tANN training loss 0.000458\n",
      ">> Epoch 278 finished \tANN training loss 0.000452\n",
      ">> Epoch 279 finished \tANN training loss 0.000447\n",
      ">> Epoch 280 finished \tANN training loss 0.000443\n",
      ">> Epoch 281 finished \tANN training loss 0.000439\n",
      ">> Epoch 282 finished \tANN training loss 0.000435\n",
      ">> Epoch 283 finished \tANN training loss 0.000431\n",
      ">> Epoch 284 finished \tANN training loss 0.000427\n",
      ">> Epoch 285 finished \tANN training loss 0.000423\n",
      ">> Epoch 286 finished \tANN training loss 0.000419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 287 finished \tANN training loss 0.000415\n",
      ">> Epoch 288 finished \tANN training loss 0.000412\n",
      ">> Epoch 289 finished \tANN training loss 0.000408\n",
      ">> Epoch 290 finished \tANN training loss 0.000403\n",
      ">> Epoch 291 finished \tANN training loss 0.000399\n",
      ">> Epoch 292 finished \tANN training loss 0.000396\n",
      ">> Epoch 293 finished \tANN training loss 0.000391\n",
      ">> Epoch 294 finished \tANN training loss 0.000388\n",
      ">> Epoch 295 finished \tANN training loss 0.000384\n",
      ">> Epoch 296 finished \tANN training loss 0.000380\n",
      ">> Epoch 297 finished \tANN training loss 0.000376\n",
      ">> Epoch 298 finished \tANN training loss 0.000373\n",
      ">> Epoch 299 finished \tANN training loss 0.000369\n",
      ">> Epoch 300 finished \tANN training loss 0.000366\n",
      ">> Epoch 301 finished \tANN training loss 0.000363\n",
      ">> Epoch 302 finished \tANN training loss 0.000360\n",
      ">> Epoch 303 finished \tANN training loss 0.000357\n",
      ">> Epoch 304 finished \tANN training loss 0.000354\n",
      ">> Epoch 305 finished \tANN training loss 0.000351\n",
      ">> Epoch 306 finished \tANN training loss 0.000349\n",
      ">> Epoch 307 finished \tANN training loss 0.000346\n",
      ">> Epoch 308 finished \tANN training loss 0.000343\n",
      ">> Epoch 309 finished \tANN training loss 0.000339\n",
      ">> Epoch 310 finished \tANN training loss 0.000336\n",
      ">> Epoch 311 finished \tANN training loss 0.000334\n",
      ">> Epoch 312 finished \tANN training loss 0.000332\n",
      ">> Epoch 313 finished \tANN training loss 0.000328\n",
      ">> Epoch 314 finished \tANN training loss 0.000326\n",
      ">> Epoch 315 finished \tANN training loss 0.000322\n",
      ">> Epoch 316 finished \tANN training loss 0.000320\n",
      ">> Epoch 317 finished \tANN training loss 0.000317\n",
      ">> Epoch 318 finished \tANN training loss 0.000315\n",
      ">> Epoch 319 finished \tANN training loss 0.000313\n",
      ">> Epoch 320 finished \tANN training loss 0.000311\n",
      ">> Epoch 321 finished \tANN training loss 0.000308\n",
      ">> Epoch 322 finished \tANN training loss 0.000306\n",
      ">> Epoch 323 finished \tANN training loss 0.000304\n",
      ">> Epoch 324 finished \tANN training loss 0.000302\n",
      ">> Epoch 325 finished \tANN training loss 0.000299\n",
      ">> Epoch 326 finished \tANN training loss 0.000297\n",
      ">> Epoch 327 finished \tANN training loss 0.000295\n",
      ">> Epoch 328 finished \tANN training loss 0.000293\n",
      ">> Epoch 329 finished \tANN training loss 0.000291\n",
      ">> Epoch 330 finished \tANN training loss 0.000289\n",
      ">> Epoch 331 finished \tANN training loss 0.000287\n",
      ">> Epoch 332 finished \tANN training loss 0.000285\n",
      ">> Epoch 333 finished \tANN training loss 0.000283\n",
      ">> Epoch 334 finished \tANN training loss 0.000281\n",
      ">> Epoch 335 finished \tANN training loss 0.000279\n",
      ">> Epoch 336 finished \tANN training loss 0.000277\n",
      ">> Epoch 337 finished \tANN training loss 0.000275\n",
      ">> Epoch 338 finished \tANN training loss 0.000273\n",
      ">> Epoch 339 finished \tANN training loss 0.000272\n",
      ">> Epoch 340 finished \tANN training loss 0.000270\n",
      ">> Epoch 341 finished \tANN training loss 0.000268\n",
      ">> Epoch 342 finished \tANN training loss 0.000266\n",
      ">> Epoch 343 finished \tANN training loss 0.000265\n",
      ">> Epoch 344 finished \tANN training loss 0.000263\n",
      ">> Epoch 345 finished \tANN training loss 0.000262\n",
      ">> Epoch 346 finished \tANN training loss 0.000260\n",
      ">> Epoch 347 finished \tANN training loss 0.000258\n",
      ">> Epoch 348 finished \tANN training loss 0.000257\n",
      ">> Epoch 349 finished \tANN training loss 0.000255\n",
      ">> Epoch 350 finished \tANN training loss 0.000254\n",
      ">> Epoch 351 finished \tANN training loss 0.000252\n",
      ">> Epoch 352 finished \tANN training loss 0.000250\n",
      ">> Epoch 353 finished \tANN training loss 0.000248\n",
      ">> Epoch 354 finished \tANN training loss 0.000247\n",
      ">> Epoch 355 finished \tANN training loss 0.000245\n",
      ">> Epoch 356 finished \tANN training loss 0.000244\n",
      ">> Epoch 357 finished \tANN training loss 0.000243\n",
      ">> Epoch 358 finished \tANN training loss 0.000241\n",
      ">> Epoch 359 finished \tANN training loss 0.000239\n",
      ">> Epoch 360 finished \tANN training loss 0.000238\n",
      ">> Epoch 361 finished \tANN training loss 0.000237\n",
      ">> Epoch 362 finished \tANN training loss 0.000235\n",
      ">> Epoch 363 finished \tANN training loss 0.000233\n",
      ">> Epoch 364 finished \tANN training loss 0.000231\n",
      ">> Epoch 365 finished \tANN training loss 0.000230\n",
      ">> Epoch 366 finished \tANN training loss 0.000229\n",
      ">> Epoch 367 finished \tANN training loss 0.000227\n",
      ">> Epoch 368 finished \tANN training loss 0.000226\n",
      ">> Epoch 369 finished \tANN training loss 0.000225\n",
      ">> Epoch 370 finished \tANN training loss 0.000224\n",
      ">> Epoch 371 finished \tANN training loss 0.000223\n",
      ">> Epoch 372 finished \tANN training loss 0.000221\n",
      ">> Epoch 373 finished \tANN training loss 0.000220\n",
      ">> Epoch 374 finished \tANN training loss 0.000218\n",
      ">> Epoch 375 finished \tANN training loss 0.000217\n",
      ">> Epoch 376 finished \tANN training loss 0.000216\n",
      ">> Epoch 377 finished \tANN training loss 0.000215\n",
      ">> Epoch 378 finished \tANN training loss 0.000214\n",
      ">> Epoch 379 finished \tANN training loss 0.000213\n",
      ">> Epoch 380 finished \tANN training loss 0.000212\n",
      ">> Epoch 381 finished \tANN training loss 0.000211\n",
      ">> Epoch 382 finished \tANN training loss 0.000210\n",
      ">> Epoch 383 finished \tANN training loss 0.000208\n",
      ">> Epoch 384 finished \tANN training loss 0.000207\n",
      ">> Epoch 385 finished \tANN training loss 0.000206\n",
      ">> Epoch 386 finished \tANN training loss 0.000205\n",
      ">> Epoch 387 finished \tANN training loss 0.000204\n",
      ">> Epoch 388 finished \tANN training loss 0.000203\n",
      ">> Epoch 389 finished \tANN training loss 0.000202\n",
      ">> Epoch 390 finished \tANN training loss 0.000201\n",
      ">> Epoch 391 finished \tANN training loss 0.000199\n",
      ">> Epoch 392 finished \tANN training loss 0.000198\n",
      ">> Epoch 393 finished \tANN training loss 0.000197\n",
      ">> Epoch 394 finished \tANN training loss 0.000195\n",
      ">> Epoch 395 finished \tANN training loss 0.000194\n",
      ">> Epoch 396 finished \tANN training loss 0.000193\n",
      ">> Epoch 397 finished \tANN training loss 0.000192\n",
      ">> Epoch 398 finished \tANN training loss 0.000191\n",
      ">> Epoch 399 finished \tANN training loss 0.000190\n",
      ">> Epoch 400 finished \tANN training loss 0.000190\n",
      ">> Epoch 401 finished \tANN training loss 0.000189\n",
      ">> Epoch 402 finished \tANN training loss 0.000187\n",
      ">> Epoch 403 finished \tANN training loss 0.000187\n",
      ">> Epoch 404 finished \tANN training loss 0.000186\n",
      ">> Epoch 405 finished \tANN training loss 0.000185\n",
      ">> Epoch 406 finished \tANN training loss 0.000184\n",
      ">> Epoch 407 finished \tANN training loss 0.000183\n",
      ">> Epoch 408 finished \tANN training loss 0.000182\n",
      ">> Epoch 409 finished \tANN training loss 0.000181\n",
      ">> Epoch 410 finished \tANN training loss 0.000180\n",
      ">> Epoch 411 finished \tANN training loss 0.000179\n",
      ">> Epoch 412 finished \tANN training loss 0.000179\n",
      ">> Epoch 413 finished \tANN training loss 0.000178\n",
      ">> Epoch 414 finished \tANN training loss 0.000177\n",
      ">> Epoch 415 finished \tANN training loss 0.000176\n",
      ">> Epoch 416 finished \tANN training loss 0.000175\n",
      ">> Epoch 417 finished \tANN training loss 0.000174\n",
      ">> Epoch 418 finished \tANN training loss 0.000173\n",
      ">> Epoch 419 finished \tANN training loss 0.000173\n",
      ">> Epoch 420 finished \tANN training loss 0.000172\n",
      ">> Epoch 421 finished \tANN training loss 0.000171\n",
      ">> Epoch 422 finished \tANN training loss 0.000170\n",
      ">> Epoch 423 finished \tANN training loss 0.000170\n",
      ">> Epoch 424 finished \tANN training loss 0.000169\n",
      ">> Epoch 425 finished \tANN training loss 0.000168\n",
      ">> Epoch 426 finished \tANN training loss 0.000167\n",
      ">> Epoch 427 finished \tANN training loss 0.000166\n",
      ">> Epoch 428 finished \tANN training loss 0.000165\n",
      ">> Epoch 429 finished \tANN training loss 0.000165\n",
      ">> Epoch 430 finished \tANN training loss 0.000164\n",
      ">> Epoch 431 finished \tANN training loss 0.000163\n",
      ">> Epoch 432 finished \tANN training loss 0.000162\n",
      ">> Epoch 433 finished \tANN training loss 0.000161\n",
      ">> Epoch 434 finished \tANN training loss 0.000161\n",
      ">> Epoch 435 finished \tANN training loss 0.000160\n",
      ">> Epoch 436 finished \tANN training loss 0.000159\n",
      ">> Epoch 437 finished \tANN training loss 0.000159\n",
      ">> Epoch 438 finished \tANN training loss 0.000158\n",
      ">> Epoch 439 finished \tANN training loss 0.000157\n",
      ">> Epoch 440 finished \tANN training loss 0.000157\n",
      ">> Epoch 441 finished \tANN training loss 0.000156\n",
      ">> Epoch 442 finished \tANN training loss 0.000155\n",
      ">> Epoch 443 finished \tANN training loss 0.000155\n",
      ">> Epoch 444 finished \tANN training loss 0.000154\n",
      ">> Epoch 445 finished \tANN training loss 0.000153\n",
      ">> Epoch 446 finished \tANN training loss 0.000153\n",
      ">> Epoch 447 finished \tANN training loss 0.000152\n",
      ">> Epoch 448 finished \tANN training loss 0.000152\n",
      ">> Epoch 449 finished \tANN training loss 0.000151\n",
      ">> Epoch 450 finished \tANN training loss 0.000150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 451 finished \tANN training loss 0.000150\n",
      ">> Epoch 452 finished \tANN training loss 0.000149\n",
      ">> Epoch 453 finished \tANN training loss 0.000149\n",
      ">> Epoch 454 finished \tANN training loss 0.000148\n",
      ">> Epoch 455 finished \tANN training loss 0.000147\n",
      ">> Epoch 456 finished \tANN training loss 0.000147\n",
      ">> Epoch 457 finished \tANN training loss 0.000146\n",
      ">> Epoch 458 finished \tANN training loss 0.000146\n",
      ">> Epoch 459 finished \tANN training loss 0.000145\n",
      ">> Epoch 460 finished \tANN training loss 0.000145\n",
      ">> Epoch 461 finished \tANN training loss 0.000144\n",
      ">> Epoch 462 finished \tANN training loss 0.000143\n",
      ">> Epoch 463 finished \tANN training loss 0.000143\n",
      ">> Epoch 464 finished \tANN training loss 0.000142\n",
      ">> Epoch 465 finished \tANN training loss 0.000142\n",
      ">> Epoch 466 finished \tANN training loss 0.000141\n",
      ">> Epoch 467 finished \tANN training loss 0.000141\n",
      ">> Epoch 468 finished \tANN training loss 0.000140\n",
      ">> Epoch 469 finished \tANN training loss 0.000139\n",
      ">> Epoch 470 finished \tANN training loss 0.000139\n",
      ">> Epoch 471 finished \tANN training loss 0.000138\n",
      ">> Epoch 472 finished \tANN training loss 0.000137\n",
      ">> Epoch 473 finished \tANN training loss 0.000137\n",
      ">> Epoch 474 finished \tANN training loss 0.000136\n",
      ">> Epoch 475 finished \tANN training loss 0.000136\n",
      ">> Epoch 476 finished \tANN training loss 0.000135\n",
      ">> Epoch 477 finished \tANN training loss 0.000135\n",
      ">> Epoch 478 finished \tANN training loss 0.000134\n",
      ">> Epoch 479 finished \tANN training loss 0.000134\n",
      ">> Epoch 480 finished \tANN training loss 0.000133\n",
      ">> Epoch 481 finished \tANN training loss 0.000133\n",
      ">> Epoch 482 finished \tANN training loss 0.000132\n",
      ">> Epoch 483 finished \tANN training loss 0.000132\n",
      ">> Epoch 484 finished \tANN training loss 0.000131\n",
      ">> Epoch 485 finished \tANN training loss 0.000131\n",
      ">> Epoch 486 finished \tANN training loss 0.000130\n",
      ">> Epoch 487 finished \tANN training loss 0.000130\n",
      ">> Epoch 488 finished \tANN training loss 0.000130\n",
      ">> Epoch 489 finished \tANN training loss 0.000129\n",
      ">> Epoch 490 finished \tANN training loss 0.000129\n",
      ">> Epoch 491 finished \tANN training loss 0.000128\n",
      ">> Epoch 492 finished \tANN training loss 0.000128\n",
      ">> Epoch 493 finished \tANN training loss 0.000127\n",
      ">> Epoch 494 finished \tANN training loss 0.000127\n",
      ">> Epoch 495 finished \tANN training loss 0.000126\n",
      ">> Epoch 496 finished \tANN training loss 0.000126\n",
      ">> Epoch 497 finished \tANN training loss 0.000126\n",
      ">> Epoch 498 finished \tANN training loss 0.000125\n",
      ">> Epoch 499 finished \tANN training loss 0.000125\n",
      ">> Epoch 500 finished \tANN training loss 0.000124\n",
      ">> Epoch 501 finished \tANN training loss 0.000124\n",
      ">> Epoch 502 finished \tANN training loss 0.000123\n",
      ">> Epoch 503 finished \tANN training loss 0.000123\n",
      ">> Epoch 504 finished \tANN training loss 0.000122\n",
      ">> Epoch 505 finished \tANN training loss 0.000122\n",
      ">> Epoch 506 finished \tANN training loss 0.000121\n",
      ">> Epoch 507 finished \tANN training loss 0.000121\n",
      ">> Epoch 508 finished \tANN training loss 0.000121\n",
      ">> Epoch 509 finished \tANN training loss 0.000120\n",
      ">> Epoch 510 finished \tANN training loss 0.000120\n",
      ">> Epoch 511 finished \tANN training loss 0.000119\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedDBNClassification(batch_size=32, dropout_p=1e-05,\n",
       "                            idx_to_label_map={0: 0, 1: 1, 2: 4, 3: 3},\n",
       "                            l2_regularization=1.0,\n",
       "                            label_to_idx_map={0: 0, 1: 1, 3: 3, 4: 2},\n",
       "                            learning_rate=0.1, n_iter_backprop=512,\n",
       "                            verbose=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = classifier.predict(X_test)\n",
    "# predict crisp classes for test set\n",
    "#yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "yhat_classes = yhat_probs# np.argmax(yhat_probs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n",
      "Precision: 1.000000\n",
      "Recall: 1.000000\n",
      "F1 score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(Y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(Y_test, yhat_classes, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(Y_test, yhat_classes,average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(Y_test, yhat_classes, average='macro')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 1.000000\n",
      "[[22  0  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(Y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "# ROC AUC\n",
    "#fprate, tprate, thresholds = roc_curve(Y_test, yhat_probs, average = 'macro')\n",
    "#print('ROC AUC: %f' % thresholds)\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(Y_test, yhat_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "\n",
    "n_class = 5\n",
    "\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, yhat_classes, pos_label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting    \n",
    "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
    "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
    "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
    "plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\n",
    "plt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\n",
    "plt.title('Multiclass ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('Multiclass ROC',dpi=300); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[22  0  0  0]\n",
      " [ 0  5  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  0  0  7]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_test,yhat_classes, labels= [0, 1, 3, 4])\n",
    "print('Confusion matrix : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         3\n",
      "           4       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = classification_report(Y_test,yhat_classes, labels= [0, 1, 3, 4])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('./models/IP address/DBN/DBN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
